{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec633a20",
   "metadata": {},
   "source": [
    "# CAFA-6 — Kaggle notebook setup\n",
    "\n",
    "\n",
    "\n",
    "Everything lives in this notebook:\n",
    "\n",
    "- Config (paths + artefacts)\n",
    "\n",
    "- Sanity checks (inputs present, quick stats)\n",
    "\n",
    "- Minimal diagnostics plots (so we catch path/data issues early)\n",
    "\n",
    "\n",
    "\n",
    "Assumption (Kaggle): your dataset folder under `/kaggle/input/.../` contains:\n",
    "\n",
    "- `Train/` and `Test/`\n",
    "\n",
    "- `IA.tsv` and `sample_submission.tsv`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8338233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. SETUP, CONFIG & DIAGNOSTICS\n",
    "# ==========================================\n",
    "# HARDWARE: CPU (Standard)\n",
    "# ==========================================\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ------------------------------------------\n",
    "# Environment Detection & Paths\n",
    "# ------------------------------------------\n",
    "IS_KAGGLE = os.getenv('KAGGLE_KERNEL_RUN_TYPE') is not None or Path('/kaggle').exists()\n",
    "\n",
    "if IS_KAGGLE:\n",
    "    print(\"Environment: Kaggle Detected\")\n",
    "    INPUT_ROOT = Path('/kaggle/input')\n",
    "    WORKING_ROOT = Path('/kaggle/working')\n",
    "    \n",
    "    # Standard Kaggle Input Listing\n",
    "    for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "        for filename in filenames:\n",
    "            print(os.path.join(dirname, filename))\n",
    "else:\n",
    "    print(\"Environment: Local Detected\")\n",
    "    # Robust Project Root Detection\n",
    "    CURRENT_DIR = Path.cwd()\n",
    "    if CURRENT_DIR.name == 'notebooks':\n",
    "        PROJECT_ROOT = CURRENT_DIR.parent\n",
    "    else:\n",
    "        PROJECT_ROOT = CURRENT_DIR\n",
    "        \n",
    "    INPUT_ROOT = PROJECT_ROOT\n",
    "    WORKING_ROOT = PROJECT_ROOT / 'artefacts_local'\n",
    "    WORKING_ROOT.mkdir(exist_ok=True)\n",
    "\n",
    "# Artefacts Directory\n",
    "ARTEFACTS_DIR = WORKING_ROOT / 'artefacts'\n",
    "ARTEFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "(ARTEFACTS_DIR / 'parsed').mkdir(parents=True, exist_ok=True)\n",
    "(ARTEFACTS_DIR / 'features').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ------------------------------------------\n",
    "# Dataset Discovery\n",
    "# ------------------------------------------\n",
    "DATASET_SLUG = 'cafa-6-protein-function-prediction'\n",
    "\n",
    "def find_dataset_root(input_root: Path, dataset_slug: str) -> Path:\n",
    "    # 1. Check for Kaggle slug\n",
    "    candidate = input_root / dataset_slug\n",
    "    if candidate.exists():\n",
    "        return candidate\n",
    "    \n",
    "    # 2. Check if we are already in the root (Local)\n",
    "    if (input_root / 'Train').exists():\n",
    "        return input_root\n",
    "\n",
    "    # 3. Fallback search\n",
    "    candidates = [p for p in input_root.iterdir() if p.is_dir()]\n",
    "    def score(p: Path) -> int:\n",
    "        return int((p / 'Train').exists()) + int((p / 'Test').exists())\n",
    "    candidates = sorted(candidates, key=score, reverse=True)\n",
    "    if candidates and score(candidates[0]) > 0:\n",
    "        return candidates[0]\n",
    "        \n",
    "    raise FileNotFoundError(f\"Dataset not found in {input_root}\")\n",
    "\n",
    "DATASET_ROOT = find_dataset_root(INPUT_ROOT, DATASET_SLUG)\n",
    "print(f\"DATASET_ROOT: {DATASET_ROOT}\")\n",
    "\n",
    "# Define Paths\n",
    "PATH_IA = DATASET_ROOT / 'IA.tsv'\n",
    "PATH_SAMPLE_SUB = DATASET_ROOT / 'sample_submission.tsv'\n",
    "PATH_TRAIN_FASTA = DATASET_ROOT / 'Train' / 'train_sequences.fasta'\n",
    "PATH_TRAIN_TERMS = DATASET_ROOT / 'Train' / 'train_terms.tsv'\n",
    "PATH_TRAIN_TAXON = DATASET_ROOT / 'Train' / 'train_taxonomy.tsv'\n",
    "PATH_GO_OBO = DATASET_ROOT / 'Train' / 'go-basic.obo'\n",
    "PATH_TEST_FASTA = DATASET_ROOT / 'Test' / 'testsuperset.fasta'\n",
    "PATH_TEST_TAXON = DATASET_ROOT / 'Test' / 'testsuperset-taxon-list.tsv'\n",
    "\n",
    "# ------------------------------------------\n",
    "# Sanity Checks\n",
    "# ------------------------------------------\n",
    "required = {\n",
    "    'IA.tsv': PATH_IA,\n",
    "    'Train/train_sequences.fasta': PATH_TRAIN_FASTA,\n",
    "    'Train/train_terms.tsv': PATH_TRAIN_TERMS,\n",
    "    'Train/go-basic.obo': PATH_GO_OBO,\n",
    "}\n",
    "missing = {k: v for k, v in required.items() if not v.exists()}\n",
    "if missing:\n",
    "    raise FileNotFoundError(f\"Missing files: {missing}\")\n",
    "print(\"All required inputs found.\")\n",
    "\n",
    "# ------------------------------------------\n",
    "# Initial Diagnostics (Sequence Lengths)\n",
    "# ------------------------------------------\n",
    "%matplotlib inline\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "\n",
    "def read_fasta_lengths(path: Path, max_records=20000):\n",
    "    lengths = []\n",
    "    current = 0\n",
    "    n = 0\n",
    "    with path.open('r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith('>'):\n",
    "                if n > 0: lengths.append(current)\n",
    "                n += 1\n",
    "                current = 0\n",
    "                if max_records and n > max_records: break\n",
    "            else:\n",
    "                current += len(line)\n",
    "        if n > 0: lengths.append(current)\n",
    "    return np.array(lengths)\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.hist(read_fasta_lengths(PATH_TRAIN_FASTA), bins=50, alpha=0.5, label='Train')\n",
    "if PATH_TEST_FASTA.exists():\n",
    "    plt.hist(read_fasta_lengths(PATH_TEST_FASTA), bins=50, alpha=0.5, label='Test')\n",
    "plt.title('Sequence Length Distribution (First 20k)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe900118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. PHASE 1: DATA STRUCTURING & HIERARCHY\n",
    "# ==========================================\n",
    "# HARDWARE: CPU (Standard)\n",
    "# ==========================================\n",
    "\n",
    "# ------------------------------------------\n",
    "# A. Parse FASTA to Feather\n",
    "# ------------------------------------------\n",
    "def parse_fasta(path: Path) -> pd.DataFrame:\n",
    "    ids, seqs = [], []\n",
    "    cur_id, cur_seq = None, []\n",
    "    with path.open('r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith('>'):\n",
    "                if cur_id:\n",
    "                    ids.append(cur_id)\n",
    "                    seqs.append(''.join(cur_seq))\n",
    "                cur_id = line[1:].split()[0]\n",
    "                cur_seq = []\n",
    "            else:\n",
    "                cur_seq.append(line)\n",
    "        if cur_id:\n",
    "            ids.append(cur_id)\n",
    "            seqs.append(''.join(cur_seq))\n",
    "    return pd.DataFrame({'id': ids, 'sequence': seqs})\n",
    "\n",
    "print(\"Parsing FASTA...\")\n",
    "parse_fasta(PATH_TRAIN_FASTA).to_feather(ARTEFACTS_DIR / 'parsed' / 'train_seq.feather')\n",
    "if PATH_TEST_FASTA.exists():\n",
    "    parse_fasta(PATH_TEST_FASTA).to_feather(ARTEFACTS_DIR / 'parsed' / 'test_seq.feather')\n",
    "print(\"FASTA parsed and saved to artefacts.\")\n",
    "\n",
    "# ------------------------------------------\n",
    "# B. Parse OBO & Terms\n",
    "# ------------------------------------------\n",
    "def parse_obo(path: Path):\n",
    "    parents = {}\n",
    "    namespaces = {}\n",
    "    cur_id, cur_ns = None, None\n",
    "    with path.open('r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line == '[Term]':\n",
    "                if cur_id and cur_ns: namespaces[cur_id] = cur_ns\n",
    "                cur_id, cur_ns = None, None\n",
    "            elif line.startswith('id: GO:'):\n",
    "                cur_id = line.split('id: ', 1)[1]\n",
    "            elif line.startswith('namespace:'):\n",
    "                cur_ns = line.split('namespace: ', 1)[1]\n",
    "            elif line.startswith('is_a:') and cur_id:\n",
    "                parent = line.split('is_a: ', 1)[1].split(' ! ')[0]\n",
    "                if cur_id not in parents: parents[cur_id] = set()\n",
    "                parents[cur_id].add(parent)\n",
    "        if cur_id and cur_ns: namespaces[cur_id] = cur_ns\n",
    "    return parents, namespaces\n",
    "\n",
    "print(\"Parsing OBO...\")\n",
    "go_parents, go_namespaces = parse_obo(PATH_GO_OBO)\n",
    "print(f\"GO Graph: {len(go_parents)} nodes with parents, {len(go_namespaces)} terms with namespace.\")\n",
    "\n",
    "# ------------------------------------------\n",
    "# C. Process Terms & Priors\n",
    "# ------------------------------------------\n",
    "terms = pd.read_csv(PATH_TRAIN_TERMS, sep='\\t')\n",
    "col_term = terms.columns[1]\n",
    "terms['aspect'] = terms[col_term].map(lambda x: go_namespaces.get(x, 'UNK'))\n",
    "\n",
    "# Plot Aspects\n",
    "plt.figure(figsize=(6, 3))\n",
    "terms['aspect'].value_counts().plot(kind='bar', title='Annotations by Namespace')\n",
    "plt.show()\n",
    "\n",
    "# Save Priors\n",
    "priors = (terms[col_term].value_counts() / terms.iloc[:,0].nunique()).reset_index()\n",
    "priors.columns = ['term', 'prior']\n",
    "if PATH_IA.exists():\n",
    "    ia = pd.read_csv(PATH_IA, sep='\\t', names=['term', 'ia'])\n",
    "    priors = priors.merge(ia, on='term', how='left').fillna(0)\n",
    "priors.to_parquet(ARTEFACTS_DIR / 'parsed' / 'term_priors.parquet')\n",
    "print(\"Terms processed and priors saved.\")\n",
    "\n",
    "# ------------------------------------------\n",
    "# D. Process Taxonomy\n",
    "# ------------------------------------------\n",
    "print(\"Processing Taxonomy...\")\n",
    "# Train Taxonomy\n",
    "tax_train = pd.read_csv(PATH_TRAIN_TAXON, sep='\\t', header=None, names=['id', 'taxon_id'])\n",
    "tax_train['taxon_id'] = tax_train['taxon_id'].astype(int)\n",
    "tax_train.to_feather(ARTEFACTS_DIR / 'parsed' / 'train_taxa.feather')\n",
    "\n",
    "# Test Taxonomy (Extract from FASTA headers)\n",
    "if PATH_TEST_FASTA.exists():\n",
    "    ids, taxons = [], []\n",
    "    with PATH_TEST_FASTA.open('r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith('>'):\n",
    "                parts = line[1:].split()\n",
    "                ids.append(parts[0])\n",
    "                # Assume second part is taxon if present\n",
    "                if len(parts) > 1:\n",
    "                    try:\n",
    "                        taxons.append(int(parts[1]))\n",
    "                    except ValueError:\n",
    "                        taxons.append(0)\n",
    "                else:\n",
    "                    taxons.append(0)\n",
    "    tax_test = pd.DataFrame({'id': ids, 'taxon_id': taxons})\n",
    "    tax_test.to_feather(ARTEFACTS_DIR / 'parsed' / 'test_taxa.feather')\n",
    "    print(f\"Taxonomy processed. Train: {len(tax_train)}, Test: {len(tax_test)}\")\n",
    "else:\n",
    "    print(f\"Taxonomy processed. Train: {len(tax_train)}\")\n",
    "\n",
    "# ------------------------------------------\n",
    "# E. Save Targets & Term List\n",
    "# ------------------------------------------\n",
    "print(\"Saving Targets & Term List...\")\n",
    "# Save full terms list (long format)\n",
    "terms.to_parquet(ARTEFACTS_DIR / 'parsed' / 'train_terms.parquet')\n",
    "\n",
    "# Save unique term list with counts\n",
    "term_counts = terms['term'].value_counts().reset_index()\n",
    "term_counts.columns = ['term', 'count']\n",
    "term_counts.to_parquet(ARTEFACTS_DIR / 'parsed' / 'term_counts.parquet')\n",
    "print(\"Targets saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7a5147",
   "metadata": {},
   "source": [
    "## How the local GOA notebook fits (recommended)\n",
    "This notebook’s **External Data & Evidence Codes** step is disk-heavy and is best done locally.\n",
    "\n",
    "Use the local precompute notebook: `notebooks/CAFA6_Local_GOA_Precompute.ipynb`\n",
    "- It creates compact, compressed artefacts filtered to CAFA protein IDs:\n",
    "  - `goa_filtered_iea.tsv.gz` (IEA-only; safest default)\n",
    "  - `goa_filtered_all.tsv.gz` (all evidence codes; richer signal)\n",
    "- Upload one or both files to Kaggle as a Dataset (or keep locally if running locally).\n",
    "\n",
    "Then, here (on Kaggle):\n",
    "- Prefer **using the precomputed `.tsv.gz`** rather than downloading/parsing the full GOA dump.\n",
    "- The notebook cell below is updated to look for these artefacts first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d3d27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 PHASE 1 (Step 3): EXTERNAL DATA & EVIDENCE CODES (ARTEFACT-FIRST)\n",
    "# ================================================================\n",
    "# Goal: use precomputed, CAFA-filtered GOA artefacts and avoid Kaggle disk blow-ups.\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Toggle to enable external features\n",
    "PROCESS_EXTERNAL = True\n",
    "\n",
    "# Recommended (Kaggle): set this to your dataset folder, e.g. Path('/kaggle/input/goa-filtered-all-tsv-gz')\n",
    "# If left empty on Kaggle, we scan `/kaggle/input` for `goa_filtered_*.tsv(.gz)` only.\n",
    "GOA_ARTEFACT_DIRS = [\n",
    "    # Path('/kaggle/input/<dataset-folder>'),\n",
    "]\n",
    "\n",
    "if PROCESS_EXTERNAL:\n",
    "    EXT_DIR = ARTEFACTS_DIR / 'external'\n",
    "    EXT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "    def _existing(dirs: list[Path]) -> list[Path]:\n",
    "        return [Path(d) for d in dirs if d is not None and str(d).strip() and Path(d).exists()]\n",
    "\n",
    "    def _discover(dirs: list[Path]) -> list[Path]:\n",
    "        # keep discovery tight so we don't accidentally pick dataset TSVs (train_terms, etc.)\n",
    "        patterns = ('**/goa_filtered_*.tsv.gz', '**/goa_filtered_*.tsv')\n",
    "        found: list[Path] = []\n",
    "        for d in dirs:\n",
    "            # expected names (fast path)\n",
    "            found += [\n",
    "                d / 'goa_filtered_all.tsv.gz', d / 'goa_filtered_iea.tsv.gz',\n",
    "                d / 'goa_filtered_all.tsv', d / 'goa_filtered_iea.tsv',\n",
    "            ]\n",
    "            for pat in patterns:\n",
    "                found += sorted(Path(d).glob(pat))\n",
    "        # de-dupe + keep only existing files\n",
    "        out: list[Path] = []\n",
    "        seen = set()\n",
    "        for p in found:\n",
    "            p = Path(p)\n",
    "            if p in seen:\n",
    "                continue\n",
    "            seen.add(p)\n",
    "            if p.exists() and p.is_file():\n",
    "                out.append(p)\n",
    "        return out\n",
    "\n",
    "    def _pick_best(paths: list[Path]) -> Path:\n",
    "        def score(p: Path) -> tuple[int, int]:\n",
    "            name = p.name.lower()\n",
    "            ext_rank = 0 if name.endswith('.tsv.gz') else 1\n",
    "            if 'goa_filtered_all' in name:\n",
    "                return (0, ext_rank)\n",
    "            if 'goa_filtered_iea' in name:\n",
    "                return (1, ext_rank)\n",
    "            if 'all' in name:\n",
    "                return (2, ext_rank)\n",
    "            if 'iea' in name:\n",
    "                return (3, ext_rank)\n",
    "            return (4, ext_rank)\n",
    "        return sorted(paths, key=score)[0]\n",
    "\n",
    "    roots = _existing(GOA_ARTEFACT_DIRS)\n",
    "    if not roots and Path('/kaggle/input').exists():\n",
    "        print('GOA_ARTEFACT_DIRS empty; scanning /kaggle/input for artefacts...')\n",
    "        roots = [Path('/kaggle/input')]\n",
    "    elif not roots:\n",
    "        roots = [EXT_DIR, Path('artefacts_local/artefacts/external')]\n",
    "\n",
    "    goa_paths = _discover(roots)\n",
    "    if not goa_paths:\n",
    "        print('No precomputed GOA artefacts found.')\n",
    "        print('Fix: upload `goa_filtered_all.tsv(.gz)` and/or `goa_filtered_iea.tsv(.gz)` as a Kaggle Dataset,')\n",
    "        print('then set GOA_ARTEFACT_DIRS = [Path(\"/kaggle/input/<dataset-folder>\")]')\n",
    "        PROCESS_EXTERNAL = False\n",
    "    else:\n",
    "        print('Found GOA artefacts (first 10):')\n",
    "        for p in goa_paths[:10]:\n",
    "            print(' -', p)\n",
    "        if len(goa_paths) > 10:\n",
    "            print(f' ... (+{len(goa_paths) - 10} more)')\n",
    "        GOA_FEATURE_PATH = _pick_best(goa_paths)\n",
    "        print('Using:', GOA_FEATURE_PATH)\n",
    "        print('Format expected: EntryID<TAB>term<TAB>evidence (header included)')\n",
    "else:\n",
    "    print('Skipping External Data (PROCESS_EXTERNAL=False).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b734c023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 PHASE 1 (Step 4): HIERARCHY PROPAGATION FOR EXTERNAL GOA (NO-KAGGLE / IEA)\n",
    "# =======================================================================\n",
    "# Produces the intended artefacts from `NB LM/Phases.md`:\n",
    "# - `prop_train_no_kaggle.tsv.gz`\n",
    "# - `prop_test_no_kaggle.tsv.gz`\n",
    "# These are propagated external labels (IEA only) restricted to the top-K train terms.\n",
    "\n",
    "import gzip\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "EXTERNAL_TOP_K = 1500\n",
    "EXTERNAL_PRIOR_SCORE = 1.0  # binary prior (later we down-weight when injecting into models)\n",
    "\n",
    "if PROCESS_EXTERNAL and 'GOA_FEATURE_PATH' in locals():\n",
    "    # Ensure we have GO parents (hierarchy)\n",
    "    if 'go_parents' not in locals():\n",
    "        def parse_obo(path: Path):\n",
    "            parents = {}\n",
    "            namespaces = {}\n",
    "            cur_id, cur_ns = None, None\n",
    "            with path.open('r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    if line == '[Term]':\n",
    "                        if cur_id and cur_ns:\n",
    "                            namespaces[cur_id] = cur_ns\n",
    "                        cur_id, cur_ns = None, None\n",
    "                    elif line.startswith('id: GO:'):\n",
    "                        cur_id = line.split('id: ', 1)[1]\n",
    "                    elif line.startswith('namespace:'):\n",
    "                        cur_ns = line.split('namespace: ', 1)[1]\n",
    "                    elif line.startswith('is_a:') and cur_id:\n",
    "                        parent = line.split('is_a: ', 1)[1].split(' ! ')[0]\n",
    "                        parents.setdefault(cur_id, set()).add(parent)\n",
    "                if cur_id and cur_ns:\n",
    "                    namespaces[cur_id] = cur_ns\n",
    "            return parents, namespaces\n",
    "        go_parents, go_namespaces = parse_obo(PATH_GO_OBO)\n",
    "\n",
    "    # Ensure train/test IDs are available\n",
    "    train_seq_path = ARTEFACTS_DIR / 'parsed' / 'train_seq.feather'\n",
    "    test_seq_path = ARTEFACTS_DIR / 'parsed' / 'test_seq.feather'\n",
    "    if not train_seq_path.exists():\n",
    "        raise FileNotFoundError('Missing train_seq.feather. Run Phase 1 Step 2 (FASTA parse) first.')\n",
    "    if not test_seq_path.exists():\n",
    "        raise FileNotFoundError('Missing test_seq.feather. Run Phase 1 Step 2 (FASTA parse) first.')\n",
    "    train_ids = set(pd.read_feather(train_seq_path)['id'].astype(str).tolist())\n",
    "    test_ids = set(pd.read_feather(test_seq_path)['id'].astype(str).tolist())\n",
    "\n",
    "    # Get top-K train terms (defines the external feature space)\n",
    "    train_terms_path = ARTEFACTS_DIR / 'parsed' / 'train_terms.parquet'\n",
    "    if not train_terms_path.exists():\n",
    "        raise FileNotFoundError('Missing train_terms.parquet. Run Phase 1 Step 2 (targets parse) first.')\n",
    "    train_terms = pd.read_parquet(train_terms_path)\n",
    "    top_terms = train_terms['term'].value_counts().head(EXTERNAL_TOP_K).index.tolist()\n",
    "    top_terms_set = set(top_terms)\n",
    "    print(f'External propagation restricted to top {len(top_terms)} train terms.')\n",
    "\n",
    "    EXT_DIR = ARTEFACTS_DIR / 'external'\n",
    "    EXT_DIR.mkdir(exist_ok=True)\n",
    "    out_train = EXT_DIR / 'prop_train_no_kaggle.tsv.gz'\n",
    "    out_test = EXT_DIR / 'prop_test_no_kaggle.tsv.gz'\n",
    "\n",
    "    # Ancestor closure with memoisation\n",
    "    _anc_cache: dict[str, set[str]] = {}\n",
    "    def ancestors(term: str) -> set[str]:\n",
    "        if term in _anc_cache:\n",
    "            return _anc_cache[term]\n",
    "        seen = set([term])\n",
    "        stack = [term]\n",
    "        while stack:\n",
    "            t = stack.pop()\n",
    "            for p in go_parents.get(t, ()):\n",
    "                if p not in seen:\n",
    "                    seen.add(p)\n",
    "                    stack.append(p)\n",
    "        _anc_cache[term] = seen\n",
    "        return seen\n",
    "\n",
    "    # Stream GOA rows and write propagated edges\n",
    "    cols = ['EntryID', 'term', 'evidence']\n",
    "    print('Streaming GOA artefact:', GOA_FEATURE_PATH)\n",
    "    print('Writing:', out_train)\n",
    "    print('Writing:', out_test)\n",
    "\n",
    "    n_train = 0\n",
    "    n_test = 0\n",
    "    # Write headers\n",
    "    with gzip.open(out_train, 'wt', encoding='utf-8') as ftr, gzip.open(out_test, 'wt', encoding='utf-8') as fte:\n",
    "        ftr.write('EntryID\\tterm\\tscore\\n')\n",
    "        fte.write('EntryID\\tterm\\tscore\\n')\n",
    "\n",
    "        for chunk in pd.read_csv(\n",
    "            GOA_FEATURE_PATH,\n",
    "            sep='\\t',\n",
    "            dtype=str,\n",
    "            usecols=lambda c: c in cols,\n",
    "            chunksize=500_000,\n",
    "        ):\n",
    "            # Normalise columns if needed\n",
    "            missing = [c for c in cols if c not in chunk.columns]\n",
    "            if missing:\n",
    "                raise ValueError(f'GOA artefact missing columns: {missing}. Found: {list(chunk.columns)}')\n",
    "            chunk = chunk[cols]\n",
    "            chunk = chunk.dropna()\n",
    "\n",
    "            # No-Kaggle = IEA only\n",
    "            chunk = chunk[chunk['evidence'] == 'IEA']\n",
    "            if chunk.empty:\n",
    "                continue\n",
    "\n",
    "            # De-dupe within chunk (cuts write volume)\n",
    "            chunk = chunk.drop_duplicates(subset=['EntryID', 'term'])\n",
    "\n",
    "            # Propagate -> write only terms in top_terms_set\n",
    "            for entry_id, term in zip(chunk['EntryID'].tolist(), chunk['term'].tolist()):\n",
    "                if entry_id in train_ids:\n",
    "                    target = ftr\n",
    "                elif entry_id in test_ids:\n",
    "                    target = fte\n",
    "                else:\n",
    "                    continue\n",
    "                keep = ancestors(term) & top_terms_set\n",
    "                if not keep:\n",
    "                    continue\n",
    "                for t in keep:\n",
    "                    target.write(f'{entry_id}\\t{t}\\t{EXTERNAL_PRIOR_SCORE}\\n')\n",
    "                if target is ftr:\n",
    "                    n_train += len(keep)\n",
    "                else:\n",
    "                    n_test += len(keep)\n",
    "\n",
    "    print(f'Wrote propagated IEA edges: train={n_train:,} test={n_test:,}')\n",
    "    print('Outputs are intentionally sparse priors (score=1.0) and will be down-weighted when injected.')\n",
    "else:\n",
    "    print('Skipping external propagation (PROCESS_EXTERNAL=False or GOA_FEATURE_PATH missing).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e395534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3a. PHASE 1: EMBEDDINGS GENERATION (T5 only)\n",
    "# ============================================\n",
    "# HARDWARE: GPU recommended\n",
    "# ============================================\n",
    "\n",
    "# Split from ESM2 so you can run each independently on Kaggle.\n",
    "\n",
    "COMPUTE_T5 = True  # <--- enable/disable T5 run\n",
    "\n",
    "if COMPUTE_T5:\n",
    "    import os\n",
    "    import gc\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import torch\n",
    "    from transformers import T5Tokenizer, T5EncoderModel\n",
    "    from tqdm.auto import tqdm\n",
    "    import contextlib\n",
    "\n",
    "    # Optimise CUDA memory allocation\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "    # Fix Protobuf 'GetPrototype' error\n",
    "    os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    amp_ctx = torch.amp.autocast('cuda') if device.type == 'cuda' else contextlib.nullcontext()\n",
    "\n",
    "    def get_t5_model():\n",
    "        print(\"Loading T5 Model...\")\n",
    "        tokenizer = T5Tokenizer.from_pretrained(\n",
    "            \"Rostlab/prot_t5_xl_half_uniref50-enc\", do_lower_case=False, legacy=True\n",
    "        )\n",
    "        model = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_half_uniref50-enc\").to(device)\n",
    "        model.eval()\n",
    "        return tokenizer, model\n",
    "\n",
    "    def generate_embeddings_t5(model, tokenizer, sequences, batch_size=4, max_len=1024):\n",
    "        seq_lens = [len(s) for s in sequences]\n",
    "        sort_idx = np.argsort(seq_lens)[::-1]\n",
    "        sorted_seqs = [sequences[i] for i in sort_idx]\n",
    "\n",
    "        embeddings_list = []\n",
    "        for i in tqdm(range(0, len(sorted_seqs), batch_size), desc=\"Embedding T5 (Smart Batch)\"):\n",
    "            batch_seqs = sorted_seqs[i : i + batch_size]\n",
    "            batch_seqs = [seq.replace('U','X').replace('Z','X').replace('O','X').replace('B','X') for seq in batch_seqs]\n",
    "            batch_seqs = [\" \".join(list(seq)) for seq in batch_seqs]\n",
    "\n",
    "            ids = tokenizer.batch_encode_plus(\n",
    "                batch_seqs, add_special_tokens=True, padding=\"longest\",\n",
    "                truncation=True, max_length=max_len, return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                with amp_ctx:\n",
    "                    embedding_repr = model(input_ids=ids['input_ids'], attention_mask=ids['attention_mask'])\n",
    "\n",
    "            emb = embedding_repr.last_hidden_state.float().detach().cpu().numpy()\n",
    "            mask = ids['attention_mask'].detach().cpu().numpy()\n",
    "            for j in range(len(batch_seqs)):\n",
    "                seq_len = int(mask[j].sum())\n",
    "                valid_emb = emb[j, :seq_len]\n",
    "                embeddings_list.append(valid_emb.mean(axis=0))\n",
    "\n",
    "            del ids, embedding_repr, emb, mask\n",
    "            if device.type == 'cuda':\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        sorted_embeddings = np.vstack(embeddings_list)\n",
    "        original_order_embeddings = np.zeros_like(sorted_embeddings)\n",
    "        original_order_embeddings[sort_idx] = sorted_embeddings\n",
    "        return original_order_embeddings\n",
    "\n",
    "    print(\"Loading sequences for T5 embedding...\")\n",
    "    train_df = pd.read_feather(ARTEFACTS_DIR / 'parsed' / 'train_seq.feather')\n",
    "    test_df = None\n",
    "    if (ARTEFACTS_DIR / 'parsed' / 'test_seq.feather').exists():\n",
    "        test_df = pd.read_feather(ARTEFACTS_DIR / 'parsed' / 'test_seq.feather')\n",
    "\n",
    "    tokenizer, model = get_t5_model()\n",
    "    print(f\"Generating Train Embeddings T5 ({len(train_df)})...\")\n",
    "    train_emb = generate_embeddings_t5(model, tokenizer, train_df['sequence'].tolist())\n",
    "    np.save(ARTEFACTS_DIR / 'features' / 'train_embeds_t5.npy', train_emb)\n",
    "    del train_emb\n",
    "    gc.collect()\n",
    "\n",
    "    if test_df is not None:\n",
    "        print(f\"Generating Test Embeddings T5 ({len(test_df)})...\")\n",
    "        test_emb = generate_embeddings_t5(model, tokenizer, test_df['sequence'].tolist())\n",
    "        np.save(ARTEFACTS_DIR / 'features' / 'test_embeds_t5.npy', test_emb)\n",
    "        del test_emb\n",
    "\n",
    "    del model, tokenizer, train_df, test_df\n",
    "    gc.collect()\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "    print(\"T5 embeddings generated.\")\n",
    "else:\n",
    "    print(\"Skipping T5 embedding generation (COMPUTE_T5=False).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e81fcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3b. PHASE 1: EMBEDDINGS GENERATION (ESM2 only)\n",
    "# ==============================================\n",
    "# HARDWARE: GPU recommended\n",
    "# ==============================================\n",
    "\n",
    "# Split from T5 so you can run each independently on Kaggle.\n",
    "\n",
    "COMPUTE_ESM2 = True  # <--- enable/disable ESM2 run\n",
    "\n",
    "if COMPUTE_ESM2:\n",
    "    import os\n",
    "    import gc\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import torch\n",
    "    from transformers import EsmTokenizer, EsmModel\n",
    "    from tqdm.auto import tqdm\n",
    "    import contextlib\n",
    "\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "    os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    amp_ctx = torch.amp.autocast('cuda') if device.type == 'cuda' else contextlib.nullcontext()\n",
    "\n",
    "    def get_esm2_model():\n",
    "        print(\"Loading ESM2 650M Model...\")\n",
    "        model_name = \"facebook/esm2_t33_650M_UR50D\"\n",
    "        tokenizer = EsmTokenizer.from_pretrained(model_name)\n",
    "        model = EsmModel.from_pretrained(model_name).to(device)\n",
    "        model.eval()\n",
    "        return tokenizer, model\n",
    "\n",
    "    def generate_embeddings_esm2(model, tokenizer, sequences, batch_size=16, max_len=1024):\n",
    "        seq_lens = [len(s) for s in sequences]\n",
    "        sort_idx = np.argsort(seq_lens)[::-1]\n",
    "        sorted_seqs = [sequences[i] for i in sort_idx]\n",
    "\n",
    "        embeddings_list = []\n",
    "        for i in tqdm(range(0, len(sorted_seqs), batch_size), desc=\"Embedding ESM2 (Smart Batch)\"):\n",
    "            batch_seqs = sorted_seqs[i : i + batch_size]\n",
    "            ids = tokenizer.batch_encode_plus(\n",
    "                batch_seqs, add_special_tokens=True, padding=\"longest\",\n",
    "                truncation=True, max_length=max_len, return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                with amp_ctx:\n",
    "                    output = model(input_ids=ids['input_ids'], attention_mask=ids['attention_mask'])\n",
    "\n",
    "            emb = output.last_hidden_state.float().detach().cpu().numpy()\n",
    "            mask = ids['attention_mask'].detach().cpu().numpy()\n",
    "            for j in range(len(batch_seqs)):\n",
    "                seq_len = int(mask[j].sum())\n",
    "                valid_emb = emb[j, :seq_len]\n",
    "                embeddings_list.append(valid_emb.mean(axis=0))\n",
    "\n",
    "            del ids, output, emb, mask\n",
    "            if device.type == 'cuda':\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        sorted_embeddings = np.vstack(embeddings_list)\n",
    "        original_order_embeddings = np.zeros_like(sorted_embeddings)\n",
    "        original_order_embeddings[sort_idx] = sorted_embeddings\n",
    "        return original_order_embeddings\n",
    "\n",
    "    print(\"Loading sequences for ESM2 embedding...\")\n",
    "    train_df = pd.read_feather(ARTEFACTS_DIR / 'parsed' / 'train_seq.feather')\n",
    "    test_df = None\n",
    "    if (ARTEFACTS_DIR / 'parsed' / 'test_seq.feather').exists():\n",
    "        test_df = pd.read_feather(ARTEFACTS_DIR / 'parsed' / 'test_seq.feather')\n",
    "\n",
    "    tokenizer, model = get_esm2_model()\n",
    "    print(f\"Generating Train Embeddings ESM2 ({len(train_df)})...\")\n",
    "    train_emb = generate_embeddings_esm2(model, tokenizer, train_df['sequence'].tolist())\n",
    "    np.save(ARTEFACTS_DIR / 'features' / 'train_embeds_esm2.npy', train_emb)\n",
    "    del train_emb\n",
    "    gc.collect()\n",
    "\n",
    "    if test_df is not None:\n",
    "        print(f\"Generating Test Embeddings ESM2 ({len(test_df)})...\")\n",
    "        test_emb = generate_embeddings_esm2(model, tokenizer, test_df['sequence'].tolist())\n",
    "        np.save(ARTEFACTS_DIR / 'features' / 'test_embeds_esm2.npy', test_emb)\n",
    "        del test_emb\n",
    "\n",
    "    del model, tokenizer, train_df, test_df\n",
    "    gc.collect()\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "    print(\"ESM2 embeddings generated.\")\n",
    "else:\n",
    "    print(\"Skipping ESM2 embedding generation (COMPUTE_ESM2=False).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c74353",
   "metadata": {},
   "source": [
    "## Multimodal inputs (auditor note)\n",
    "This notebook is intentionally **Kaggle-session safe**: only **T5** and **ESM2-650M** embedding generation is included as runnable code.\n",
    "\n",
    "However, **Phase 2 is already wired** to load and use additional modalities *if you provide them as `.npy` artefacts* in `artefacts/.../features/`.\n",
    "This matches the Rank-1 “extreme multimodal” pattern: separate feature extraction (often done offline) + unified training/stacking.\n",
    "\n",
    "### Optional embedding artefacts (drop-in)\n",
    "Place these files under `ARTEFACTS_DIR / 'features'` (same row-order as `parsed/train_seq.feather` / `parsed/test_seq.feather`):\n",
    "- `train_embeds_esm2_3b.npy` + `test_embeds_esm2_3b.npy` (expected ~2560D)\n",
    "- `train_embeds_ankh.npy` + `test_embeds_ankh.npy` (expected ~1536D)\n",
    "- `train_embeds_text.npy` + `test_embeds_text.npy` (expected ~10279D)\n",
    "\n",
    "### Taxa feature\n",
    "Taxonomy is parsed in Phase 1 to:\n",
    "- `parsed/train_taxa.feather`\n",
    "- `parsed/test_taxa.feather`\n",
    "Phase 2 one-hot encodes this and concatenates it as a strong contextual prior.\n",
    "\n",
    "### Where they are consumed\n",
    "- **Cell 9 (Phase 2)**: auto-loads any of the above that exist and trains LR/GBDT on a flat concatenation; the DNN uses a multi-tower architecture over the separate modalities.\n",
    "- **Cell 11 (Phase 3b)**: stacker consumes Level-1 OOF/test predictions and trains 3 specialised GCNs (BP/MF/CC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5f698d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. PHASE 2: LEVEL-1 MODELS (DIVERSE ENSEMBLE)\n",
    "# =============================================\n",
    "# HARDWARE: GPU (32GB+ recommended for full run)\n",
    "# =============================================\n",
    "\n",
    "# We train a diverse set of models:\n",
    "# 1. Logistic Regression (Baseline)\n",
    "# 2. Py-Boost (GBDT) - Requires 'py-boost' package\n",
    "# 3. DNN Ensemble (Deep Learning)\n",
    "\n",
    "TRAIN_LEVEL1 = True\n",
    "\n",
    "if TRAIN_LEVEL1:\n",
    "    import joblib\n",
    "    from sklearn.model_selection import KFold\n",
    "    from sklearn.metrics import f1_score\n",
    "\n",
    "    # -----------------------------\n",
    "    # Load targets + ids\n",
    "    # -----------------------------\n",
    "    print(\"Loading targets...\")\n",
    "    train_terms = pd.read_parquet(ARTEFACTS_DIR / 'parsed' / 'train_terms.parquet')\n",
    "    train_ids = pd.read_feather(ARTEFACTS_DIR / 'parsed' / 'train_seq.feather')['id'].astype(str)\n",
    "    test_ids = pd.read_feather(ARTEFACTS_DIR / 'parsed' / 'test_seq.feather')['id'].astype(str)\n",
    "\n",
    "    # Target Matrix Construction (Top K Terms)\n",
    "    TOP_K = 1500\n",
    "    top_terms = train_terms['term'].value_counts().head(TOP_K).index.tolist()\n",
    "\n",
    "    train_terms_top = train_terms[train_terms['term'].isin(top_terms)]\n",
    "    Y_df = train_terms_top.pivot_table(index='EntryID', columns='term', aggfunc='size', fill_value=0)\n",
    "    Y_df = Y_df.reindex(train_ids, fill_value=0)\n",
    "    Y = Y_df.values.astype(np.float32)\n",
    "\n",
    "    print(f\"Targets: Y={Y.shape}\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Feature loading (multimodal)\n",
    "    # -----------------------------\n",
    "    print(\"Loading multimodal features...\")\n",
    "    FEAT_DIR = ARTEFACTS_DIR / 'features'\n",
    "\n",
    "    def _load_pair(stem):\n",
    "        tr = FEAT_DIR / f'train_embeds_{stem}.npy'\n",
    "        te = FEAT_DIR / f'test_embeds_{stem}.npy'\n",
    "        if tr.exists() and te.exists():\n",
    "            return np.load(tr).astype(np.float32), np.load(te).astype(np.float32)\n",
    "        return None, None\n",
    "\n",
    "    features_train = {}\n",
    "    features_test = {}\n",
    "\n",
    "    # Sequence embeddings (present today: T5 + ESM2-650M; optional: ESM2-3B / Ankh / Text)\n",
    "    for stem, key in [\n",
    "        ('t5', 't5'),\n",
    "        ('esm2', 'esm2_650m'),\n",
    "        ('esm2_3b', 'esm2_3b'),\n",
    "        ('ankh', 'ankh'),\n",
    "        ('text', 'text'),\n",
    "    ]:\n",
    "        a_tr, a_te = _load_pair(stem)\n",
    "        if a_tr is not None:\n",
    "            features_train[key] = a_tr\n",
    "            features_test[key] = a_te\n",
    "\n",
    "    # Taxonomy (encode as one-hot / bag-of-taxa)\n",
    "    taxa_train_path = ARTEFACTS_DIR / 'parsed' / 'train_taxa.feather'\n",
    "    taxa_test_path = ARTEFACTS_DIR / 'parsed' / 'test_taxa.feather'\n",
    "    if taxa_train_path.exists() and taxa_test_path.exists():\n",
    "        from sklearn.preprocessing import OneHotEncoder\n",
    "        tax_tr = pd.read_feather(taxa_train_path).astype({'id': str})\n",
    "        tax_te = pd.read_feather(taxa_test_path).astype({'id': str})\n",
    "\n",
    "        tax_tr = tax_tr.set_index('id').reindex(train_ids, fill_value=0).reset_index()\n",
    "        tax_te = tax_te.set_index('id').reindex(test_ids, fill_value=0).reset_index()\n",
    "\n",
    "        enc = OneHotEncoder(handle_unknown='ignore', sparse_output=False, dtype=np.float32)\n",
    "        enc.fit(pd.concat([tax_tr[['taxon_id']], tax_te[['taxon_id']]], axis=0))\n",
    "        X_tax_tr = enc.transform(tax_tr[['taxon_id']]).astype(np.float32)\n",
    "        X_tax_te = enc.transform(tax_te[['taxon_id']]).astype(np.float32)\n",
    "\n",
    "        features_train['taxa'] = X_tax_tr\n",
    "        features_test['taxa'] = X_tax_te\n",
    "        print(f\"Taxa features: train={X_tax_tr.shape} test={X_tax_te.shape}\")\n",
    "    else:\n",
    "        print(\"Taxa features not found; skipping (expected parsed/train_taxa.feather + parsed/test_taxa.feather).\")\n",
    "\n",
    "    # Sanity checks\n",
    "    n_train = len(train_ids)\n",
    "    n_test = len(test_ids)\n",
    "    for k, v in features_train.items():\n",
    "        if v.shape[0] != n_train:\n",
    "            raise ValueError(f\"Feature {k} train rows mismatch: {v.shape[0]} vs {n_train}\")\n",
    "    for k, v in features_test.items():\n",
    "        if v.shape[0] != n_test:\n",
    "            raise ValueError(f\"Feature {k} test rows mismatch: {v.shape[0]} vs {n_test}\")\n",
    "\n",
    "    if 't5' not in features_train:\n",
    "        raise FileNotFoundError(\"Missing required T5 embeddings: features/train_embeds_t5.npy and features/test_embeds_t5.npy\")\n",
    "\n",
    "    # Flat concatenation for classical models (LR/GBDT)\n",
    "    FLAT_KEYS = [k for k in ['t5', 'esm2_650m', 'esm2_3b', 'ankh', 'taxa', 'text'] if k in features_train]\n",
    "    X = np.hstack([features_train[k] for k in FLAT_KEYS]).astype(np.float32)\n",
    "    X_test = np.hstack([features_test[k] for k in FLAT_KEYS]).astype(np.float32)\n",
    "    print(f\"Flat X keys={FLAT_KEYS}\")\n",
    "    print(f\"Flat shapes: X={X.shape}, X_test={X_test.shape}\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # CAFA-like IA-weighted diagnostic F1 (vectorised)\n",
    "    # -----------------------------\n",
    "    if 'ia' in locals():\n",
    "        ia_df = ia[['term', 'ia']].copy()\n",
    "    elif PATH_IA.exists():\n",
    "        ia_df = pd.read_csv(PATH_IA, sep='\\t', names=['term', 'ia'])\n",
    "    else:\n",
    "        ia_df = pd.DataFrame({'term': [], 'ia': []})\n",
    "\n",
    "    ia_map = dict(zip(ia_df['term'], ia_df['ia']))\n",
    "\n",
    "    def _ia_weight(term):\n",
    "        v = ia_map.get(term, 0.0)\n",
    "        if pd.isna(v):\n",
    "            return 0.0\n",
    "        return float(v)\n",
    "\n",
    "    weights = np.array([_ia_weight(t) for t in top_terms], dtype=np.float32)\n",
    "\n",
    "    ns_to_aspect = {\n",
    "        'molecular_function': 'MF',\n",
    "        'biological_process': 'BP',\n",
    "        'cellular_component': 'CC',\n",
    "    }\n",
    "    if 'go_namespaces' in locals():\n",
    "        term_aspects = np.array([ns_to_aspect.get(go_namespaces.get(t, ''), 'UNK') for t in top_terms])\n",
    "    else:\n",
    "        term_aspects = np.array(['UNK'] * len(top_terms))\n",
    "\n",
    "    def ia_weighted_f1(y_true, y_score, thr=0.3):\n",
    "        y_true = (y_true > 0).astype(np.int8)\n",
    "        y_pred = (y_score >= thr).astype(np.int8)\n",
    "\n",
    "        tp = (y_pred & y_true).sum(axis=0).astype(np.float64)\n",
    "        pred = y_pred.sum(axis=0).astype(np.float64)\n",
    "        true = y_true.sum(axis=0).astype(np.float64)\n",
    "\n",
    "        def _score(mask=None):\n",
    "            w = weights if mask is None else (weights * mask)\n",
    "            w_tp = float((w * tp).sum())\n",
    "            w_pred = float((w * pred).sum())\n",
    "            w_true = float((w * true).sum())\n",
    "            p = (w_tp / w_pred) if w_pred > 0 else 0.0\n",
    "            r = (w_tp / w_true) if w_true > 0 else 0.0\n",
    "            return (2 * p * r / (p + r)) if (p + r) > 0 else 0.0\n",
    "\n",
    "        out = {'ALL': _score(None)}\n",
    "        for asp in ['MF', 'BP', 'CC']:\n",
    "            mask = (term_aspects == asp).astype(np.float32)\n",
    "            out[asp] = _score(mask)\n",
    "        return out\n",
    "\n",
    "    # ------------------------------------------\n",
    "    # A. Logistic Regression (Baseline)\n",
    "    # ------------------------------------------\n",
    "    print(\"\\n--- Training Logistic Regression ---\")\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    oof_preds_logreg = np.zeros(Y.shape, dtype=np.float32)\n",
    "    test_preds_logreg = np.zeros((len(test_ids), Y.shape[1]), dtype=np.float32)\n",
    "\n",
    "    for fold, (idx_tr, idx_val) in enumerate(kf.split(X)):\n",
    "        print(f\"LogReg Fold {fold+1}/5\")\n",
    "        X_tr, X_val = X[idx_tr], X[idx_val]\n",
    "        Y_tr, Y_val = Y[idx_tr], Y[idx_val]\n",
    "\n",
    "        clf_logreg = OneVsRestClassifier(\n",
    "            LogisticRegression(max_iter=500, solver='sag', n_jobs=1, C=1.0)\n",
    "        )\n",
    "        clf_logreg.n_jobs = -1\n",
    "        clf_logreg.fit(X_tr, Y_tr)\n",
    "\n",
    "        val_probs = clf_logreg.predict_proba(X_val)\n",
    "        oof_preds_logreg[idx_val] = val_probs\n",
    "\n",
    "        test_preds_logreg += clf_logreg.predict_proba(X_test) / kf.get_n_splits()\n",
    "\n",
    "        val_preds = (val_probs > 0.3).astype(int)\n",
    "        f1 = f1_score(Y_val, val_preds, average='micro')\n",
    "        ia_f1 = ia_weighted_f1(Y_val, val_probs, thr=0.3)\n",
    "        print(f\"  >> Fold {fold+1} micro-F1@0.30: {f1:.4f}\")\n",
    "        print(f\"  >> Fold {fold+1} IA-F1@0.30: ALL={ia_f1['ALL']:.4f} MF={ia_f1['MF']:.4f} BP={ia_f1['BP']:.4f} CC={ia_f1['CC']:.4f}\")\n",
    "\n",
    "        joblib.dump(clf_logreg, ARTEFACTS_DIR / 'features' / f'level1_logreg_fold{fold}.pkl')\n",
    "\n",
    "    np.save(ARTEFACTS_DIR / 'features' / 'oof_logreg.npy', oof_preds_logreg)\n",
    "    np.save(ARTEFACTS_DIR / 'features' / 'test_pred_logreg.npy', test_preds_logreg)\n",
    "    print(\"LogReg OOF + test preds saved.\")\n",
    "\n",
    "    # ------------------------------------------\n",
    "    # B. Py-Boost (GBDT)\n",
    "    # ------------------------------------------\n",
    "    try:\n",
    "        from py_boost import GradientBoosting\n",
    "        HAS_PYBOOST = True\n",
    "    except ImportError:\n",
    "        print(\"\\n[WARNING] Py-Boost not installed. Skipping GBDT.\")\n",
    "        HAS_PYBOOST = False\n",
    "\n",
    "    if HAS_PYBOOST:\n",
    "        print(\"\\n--- Training Py-Boost GBDT ---\")\n",
    "        oof_preds_gbdt = np.zeros(Y.shape, dtype=np.float32)\n",
    "        test_preds_gbdt = np.zeros((len(test_ids), Y.shape[1]), dtype=np.float32)\n",
    "\n",
    "        for fold, (idx_tr, idx_val) in enumerate(kf.split(X)):\n",
    "            print(f\"GBDT Fold {fold+1}/5\")\n",
    "            X_tr, X_val = X[idx_tr], X[idx_val]\n",
    "            Y_tr, Y_val = Y[idx_tr], Y[idx_val]\n",
    "\n",
    "            model = GradientBoosting(\n",
    "                loss='bce',\n",
    "                ntrees=1000,\n",
    "                lr=0.05,\n",
    "                max_depth=6,\n",
    "                verbose=100,\n",
    "                es=50,\n",
    "            )\n",
    "\n",
    "            model.fit(X_tr, Y_tr, eval_sets=[{'X': X_val, 'y': Y_val}])\n",
    "\n",
    "            val_probs = model.predict(X_val)\n",
    "            oof_preds_gbdt[idx_val] = val_probs\n",
    "\n",
    "            test_preds_gbdt += model.predict(X_test) / kf.get_n_splits()\n",
    "\n",
    "            val_preds = (val_probs > 0.3).astype(int)\n",
    "            f1 = f1_score(Y_val, val_preds, average='micro')\n",
    "            ia_f1 = ia_weighted_f1(Y_val, val_probs, thr=0.3)\n",
    "            print(f\"  >> Fold {fold+1} micro-F1@0.30: {f1:.4f}\")\n",
    "            print(f\"  >> Fold {fold+1} IA-F1@0.30: ALL={ia_f1['ALL']:.4f} MF={ia_f1['MF']:.4f} BP={ia_f1['BP']:.4f} CC={ia_f1['CC']:.4f}\")\n",
    "\n",
    "            model.save(str(ARTEFACTS_DIR / 'features' / f'level1_gbdt_fold{fold}.json'))\n",
    "\n",
    "        np.save(ARTEFACTS_DIR / 'features' / 'oof_gbdt.npy', oof_preds_gbdt)\n",
    "        np.save(ARTEFACTS_DIR / 'features' / 'test_pred_gbdt.npy', test_preds_gbdt)\n",
    "        print(\"GBDT OOF + test preds saved.\")\n",
    "\n",
    "    # ------------------------------------------\n",
    "    # C. DNN Ensemble (PyTorch, IA-weighted, multi-input + multi-state)\n",
    "    # ------------------------------------------\n",
    "    print(\"\\n--- Training DNN Ensemble (IA-weighted, multimodal, multi-state) ---\")\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Build a stable per-label IA weight vector for the current TOP_K targets\n",
    "    ia_w = weights.copy()\n",
    "    ia_w = np.where(np.isfinite(ia_w) & (ia_w > 0), ia_w, 1.0).astype(np.float32)\n",
    "    ia_w = ia_w / float(np.mean(ia_w))\n",
    "    ia_w = np.clip(ia_w, 0.5, 5.0)\n",
    "    ia_w_t = torch.tensor(ia_w, dtype=torch.float32, device=device).view(1, -1)\n",
    "\n",
    "    # Optional: include other model predictions as an input stream (PB OOFs analogue)\n",
    "    USE_BASE_OOFS_IN_DNN = True\n",
    "    if USE_BASE_OOFS_IN_DNN and (ARTEFACTS_DIR / 'features' / 'oof_logreg.npy').exists():\n",
    "        oof_stream = [np.load(ARTEFACTS_DIR / 'features' / 'oof_logreg.npy').astype(np.float32)]\n",
    "        test_stream = [np.load(ARTEFACTS_DIR / 'features' / 'test_pred_logreg.npy').astype(np.float32)]\n",
    "        if (ARTEFACTS_DIR / 'features' / 'oof_gbdt.npy').exists():\n",
    "            oof_stream.append(np.load(ARTEFACTS_DIR / 'features' / 'oof_gbdt.npy').astype(np.float32))\n",
    "            test_stream.append(np.load(ARTEFACTS_DIR / 'features' / 'test_pred_gbdt.npy').astype(np.float32))\n",
    "        base_oof = np.hstack(oof_stream)\n",
    "        base_test = np.hstack(test_stream)\n",
    "        features_train['base_oof'] = base_oof\n",
    "        features_test['base_oof'] = base_test\n",
    "        print(f\"Base OOF stream: train={base_oof.shape} test={base_test.shape}\")\n",
    "\n",
    "    # Select modality keys for the DNN (towers)\n",
    "    DNN_KEYS = [k for k in ['t5', 'esm2_650m', 'esm2_3b', 'ankh', 'taxa', 'text', 'base_oof'] if k in features_train]\n",
    "    print(f\"DNN modality keys={DNN_KEYS}\")\n",
    "\n",
    "    class Tower(nn.Module):\n",
    "        def __init__(self, in_dim, out_dim=512, dropout=0.1):\n",
    "            super().__init__()\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(in_dim, 1024),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(1024, out_dim),\n",
    "                nn.ReLU(),\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.net(x)\n",
    "\n",
    "    class ColossalMultiModalDNN(nn.Module):\n",
    "        def __init__(self, dims: dict, output_dim: int):\n",
    "            super().__init__()\n",
    "            self.keys = list(dims.keys())\n",
    "            self.towers = nn.ModuleDict({k: Tower(dims[k]) for k in self.keys})\n",
    "            fused_dim = 512 * len(self.keys)\n",
    "            self.head = nn.Sequential(\n",
    "                nn.Linear(fused_dim, 2048),\n",
    "                nn.BatchNorm1d(2048),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3),\n",
    "                nn.Linear(2048, 1024),\n",
    "                nn.BatchNorm1d(1024),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(1024, output_dim),\n",
    "            )\n",
    "\n",
    "        def forward(self, batch: dict):\n",
    "            hs = [self.towers[k](batch[k]) for k in self.keys]\n",
    "            h = torch.cat(hs, dim=1)\n",
    "            return self.head(h)\n",
    "\n",
    "    # Prepare torch tensors per modality\n",
    "    train_t = {k: torch.tensor(features_train[k], dtype=torch.float32, device=device) for k in DNN_KEYS}\n",
    "    test_t = {k: torch.tensor(features_test[k], dtype=torch.float32, device=device) for k in DNN_KEYS}\n",
    "\n",
    "    def _batch_dict(tensors: dict, idx):\n",
    "        return {k: v[idx] for k, v in tensors.items()}\n",
    "\n",
    "    # Multi-state ensembling\n",
    "    DNN_SEEDS = [42, 43, 44, 45, 46]\n",
    "    DNN_EPOCHS = 10\n",
    "    BATCH_SIZE = 256\n",
    "\n",
    "    oof_sum = np.zeros(Y.shape, dtype=np.float32)\n",
    "    test_sum = np.zeros((len(test_ids), Y.shape[1]), dtype=np.float32)\n",
    "    n_states = len(DNN_SEEDS)\n",
    "\n",
    "    for state_i, seed in enumerate(DNN_SEEDS, 1):\n",
    "        print(f\"\\n[DNN] Random state {state_i}/{n_states}: seed={seed}\")\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        kf_state = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "        oof_state = np.zeros(Y.shape, dtype=np.float32)\n",
    "        test_state = np.zeros((len(test_ids), Y.shape[1]), dtype=np.float32)\n",
    "\n",
    "        dims = {k: int(features_train[k].shape[1]) for k in DNN_KEYS}\n",
    "\n",
    "        for fold, (idx_tr, idx_val) in enumerate(kf_state.split(train_ids)):\n",
    "            print(f\"DNN Fold {fold+1}/5\")\n",
    "            model = ColossalMultiModalDNN(dims=dims, output_dim=Y.shape[1]).to(device)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "            Y_full_t = torch.tensor(Y, dtype=torch.float32, device=device)\n",
    "            n_samples = len(idx_tr)\n",
    "\n",
    "            model.train()\n",
    "            idx_tr_t = torch.tensor(idx_tr, dtype=torch.long, device=device)\n",
    "            for _epoch in range(DNN_EPOCHS):\n",
    "                perm = idx_tr_t[torch.randperm(n_samples, device=device)]\n",
    "                for i in range(0, n_samples, BATCH_SIZE):\n",
    "                    b = perm[i:i + BATCH_SIZE]\n",
    "                    optimizer.zero_grad()\n",
    "                    logits = model(_batch_dict(train_t, b))\n",
    "                    yb = Y_full_t[b]\n",
    "                    loss_el = F.binary_cross_entropy_with_logits(logits, yb, reduction='none')\n",
    "                    loss = (loss_el * ia_w_t).mean()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                idx_val_t = torch.tensor(idx_val, dtype=torch.long, device=device)\n",
    "                val_probs = torch.sigmoid(model(_batch_dict(train_t, idx_val_t))).cpu().numpy()\n",
    "                oof_state[idx_val] = val_probs\n",
    "\n",
    "                # test prediction (average over folds)\n",
    "                test_probs = torch.sigmoid(model(test_t)).cpu().numpy()\n",
    "                test_state += test_probs / kf_state.get_n_splits()\n",
    "\n",
    "            val_preds = (val_probs > 0.3).astype(int)\n",
    "            f1 = f1_score(Y[idx_val], val_preds, average='micro')\n",
    "            ia_f1 = ia_weighted_f1(Y[idx_val], val_probs, thr=0.3)\n",
    "            print(f\"  >> Fold {fold+1} micro-F1@0.30: {f1:.4f}\")\n",
    "            print(f\"  >> Fold {fold+1} IA-F1@0.30: ALL={ia_f1['ALL']:.4f} MF={ia_f1['MF']:.4f} BP={ia_f1['BP']:.4f} CC={ia_f1['CC']:.4f}\")\n",
    "\n",
    "            torch.save(model.state_dict(), ARTEFACTS_DIR / 'features' / f'level1_dnn_seed{seed}_fold{fold}.pth')\n",
    "\n",
    "        oof_sum += oof_state\n",
    "        test_sum += test_state\n",
    "\n",
    "    oof_preds_dnn = (oof_sum / n_states).astype(np.float32)\n",
    "    test_preds_dnn = (test_sum / n_states).astype(np.float32)\n",
    "\n",
    "    np.save(ARTEFACTS_DIR / 'features' / 'oof_dnn.npy', oof_preds_dnn)\n",
    "    np.save(ARTEFACTS_DIR / 'features' / 'test_pred_dnn.npy', test_preds_dnn)\n",
    "    print(\"DNN OOF + test preds saved (multi-state averaged).\")\n",
    "\n",
    "    # Diagnostic: IA-F1 vs threshold curve (OOF)\n",
    "    thrs = np.linspace(0.05, 0.60, 23)\n",
    "    curves = {k: [] for k in ['ALL', 'MF', 'BP', 'CC']}\n",
    "    for thr in thrs:\n",
    "        s = ia_weighted_f1(Y, oof_preds_dnn, thr=float(thr))\n",
    "        for k in curves.keys():\n",
    "            curves[k].append(s[k])\n",
    "\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    for k in ['ALL', 'MF', 'BP', 'CC']:\n",
    "        plt.plot(thrs, curves[k], label=k)\n",
    "    plt.title('DNN OOF: IA-weighted F1 vs threshold (multi-state)')\n",
    "    plt.xlabel('threshold')\n",
    "    plt.ylabel('IA-F1')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Phase 2 Complete. OOF + test predictions generated.\")\n",
    "else:\n",
    "    print(\"Skipping Phase 2.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8adec12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. PHASE 3: HIERARCHY-AWARE STACKING (GCN)\n",
    "# ==========================================\n",
    "# NOTE: This cell is kept as a lightweight alternative stacker.\n",
    "# The main stacker used by Phase 4 is implemented in the next cell.\n",
    "\n",
    "TRAIN_STACKER = False\n",
    "\n",
    "if TRAIN_STACKER:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    from sklearn.metrics import f1_score\n",
    "\n",
    "    print(\"Loading OOF Predictions for stacking...\")\n",
    "    oof_logreg = np.load(ARTEFACTS_DIR / 'features' / 'oof_logreg.npy')\n",
    "\n",
    "    try:\n",
    "        oof_gbdt = np.load(ARTEFACTS_DIR / 'features' / 'oof_gbdt.npy')\n",
    "    except Exception:\n",
    "        oof_gbdt = np.zeros_like(oof_logreg)\n",
    "\n",
    "    try:\n",
    "        oof_dnn = np.load(ARTEFACTS_DIR / 'features' / 'oof_dnn.npy')\n",
    "    except Exception:\n",
    "        oof_dnn = np.zeros_like(oof_logreg)\n",
    "\n",
    "    X_stack = np.hstack([oof_logreg, oof_gbdt, oof_dnn])\n",
    "    Y_stack = Y\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    X_stack_t = torch.tensor(X_stack, dtype=torch.float32).to(device)\n",
    "    Y_stack_t = torch.tensor(Y_stack, dtype=torch.float32).to(device)\n",
    "\n",
    "    class Stacker(nn.Module):\n",
    "        def __init__(self, input_dim, output_dim):\n",
    "            super().__init__()\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(input_dim, 2048),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(2048, output_dim),\n",
    "                nn.Sigmoid(),\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.net(x)\n",
    "\n",
    "    stacker = Stacker(X_stack.shape[1], Y_stack.shape[1]).to(device)\n",
    "    optimizer = torch.optim.Adam(stacker.parameters(), lr=1e-3)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    stacker.train()\n",
    "    for epoch in range(20):\n",
    "        optimizer.zero_grad()\n",
    "        out = stacker(X_stack_t)\n",
    "        loss = criterion(out, Y_stack_t)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            with torch.no_grad():\n",
    "                preds = (out > 0.3).float()\n",
    "                f1 = f1_score(Y_stack_t.cpu().numpy()[:1000], preds.cpu().numpy()[:1000], average='micro')\n",
    "            print(f\"Epoch {epoch}: Loss {loss.item():.4f}, Approx micro-F1@0.30 {f1:.4f}\")\n",
    "\n",
    "    torch.save(stacker.state_dict(), ARTEFACTS_DIR / 'features' / 'final_stacker.pth')\n",
    "    print(\"Saved: final_stacker.pth\")\n",
    "\n",
    "    stacker.eval()\n",
    "    with torch.no_grad():\n",
    "        final_preds = stacker(X_stack_t).cpu().numpy()\n",
    "        final_f1 = f1_score(Y_stack, (final_preds > 0.3).astype(int), average='micro')\n",
    "    print(f\"Final Stacker micro-F1@0.30: {final_f1:.4f}\")\n",
    "else:\n",
    "    print(\"Skipping Phase 3 (lightweight stacker).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f451671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5b. PHASE 3: HIERARCHY-AWARE STACKING (GRAPH SMOOTHING GCN)\n",
    "    # ===========================================================\n",
    "    # This is the stacker used by Phase 4.\n",
    "    # - Train on Level-1 OOF predictions (features)\n",
    "    # - Infer on Level-1 test predictions\n",
    "    # - Trains 3 specialised models: BP/MF/CC (Rank-1 style)\n",
    "    # - Save `gcn_stacker_{bp|mf|cc}.pth`, `top_terms_1500.json`, `test_pred_gcn.npy`\n",
    "\n",
    "TRAIN_GCN = True\n",
    "\n",
    "if TRAIN_GCN:\n",
    "    import json\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    from sklearn.metrics import f1_score\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # 1. Load OOF Predictions (Features for GCN)\n",
    "    print(\"Loading OOF predictions...\")\n",
    "    features_list = []\n",
    "\n",
    "    if (ARTEFACTS_DIR / 'features' / 'oof_logreg.npy').exists():\n",
    "        features_list.append(np.load(ARTEFACTS_DIR / 'features' / 'oof_logreg.npy'))\n",
    "    if (ARTEFACTS_DIR / 'features' / 'oof_gbdt.npy').exists():\n",
    "        features_list.append(np.load(ARTEFACTS_DIR / 'features' / 'oof_gbdt.npy'))\n",
    "    if (ARTEFACTS_DIR / 'features' / 'oof_dnn.npy').exists():\n",
    "        features_list.append(np.load(ARTEFACTS_DIR / 'features' / 'oof_dnn.npy'))\n",
    "\n",
    "    if not features_list:\n",
    "        raise FileNotFoundError(\"No OOF predictions found. Run Phase 2.\")\n",
    "\n",
    "    X_stack = np.mean(features_list, axis=0).astype(np.float32)  # (N_train, TOP_K)\n",
    "    print(f\"Train stack shape: {X_stack.shape}\")\n",
    "\n",
    "    # 2. Rebuild term list + targets\n",
    "    train_terms = pd.read_parquet(ARTEFACTS_DIR / 'parsed' / 'train_terms.parquet')\n",
    "    train_ids = pd.read_feather(ARTEFACTS_DIR / 'parsed' / 'train_seq.feather')['id'].astype(str)\n",
    "\n",
    "    TOP_K = X_stack.shape[1]\n",
    "    top_terms = train_terms['term'].value_counts().head(TOP_K).index.tolist()\n",
    "\n",
    "    train_terms_top = train_terms[train_terms['term'].isin(top_terms)]\n",
    "    Y_df = train_terms_top.pivot_table(index='EntryID', columns='term', aggfunc='size', fill_value=0)\n",
    "    Y_df = Y_df.reindex(train_ids, fill_value=0)\n",
    "    Y = Y_df.values.astype(np.float32)\n",
    "\n",
    "    # 2b. External priors (Phase 1 Step 4 outputs) -> inject as *conservative* extra signal\n",
    "    EXTERNAL_PRIOR_WEIGHT = 0.25\n",
    "    ext_dir = ARTEFACTS_DIR / 'external'\n",
    "    prior_train_path = ext_dir / 'prop_train_no_kaggle.tsv.gz'\n",
    "    if PROCESS_EXTERNAL and prior_train_path.exists():\n",
    "        prior_train = pd.read_csv(prior_train_path, sep='\\t')\n",
    "        prior_train = prior_train[prior_train['term'].isin(top_terms)]\n",
    "        prior_mat = prior_train.pivot_table(index='EntryID', columns='term', values='score', aggfunc='max', fill_value=0.0)\n",
    "        prior_mat = prior_mat.reindex(train_ids.tolist(), fill_value=0.0)\n",
    "        prior_mat = prior_mat.reindex(columns=top_terms, fill_value=0.0)\n",
    "        prior_np = prior_mat.values.astype(np.float32)\n",
    "        X_stack = np.maximum(X_stack, EXTERNAL_PRIOR_WEIGHT * prior_np)\n",
    "        print(f\"Injected external IEA prior into train stack (weight={EXTERNAL_PRIOR_WEIGHT}).\")\n",
    "    else:\n",
    "        print(\"No external train prior found (or PROCESS_EXTERNAL=False); training without GOA priors.\")\n",
    "\n",
    "    (ARTEFACTS_DIR / 'features').mkdir(parents=True, exist_ok=True)\n",
    "    with open(ARTEFACTS_DIR / 'features' / 'top_terms_1500.json', 'w') as f:\n",
    "        json.dump(top_terms, f)\n",
    "    print(\"Saved: top_terms_1500.json\")\n",
    "\n",
    "    # 3. Graph adjacency from go-basic.obo parse (from Phase 1)\n",
    "    if 'go_parents' not in locals():\n",
    "        raise RuntimeError(\"go_parents not found. Run Phase 1 OBO parsing cell first.\")\n",
    "\n",
    "    def build_adjacency(terms_list, parents_dict):\n",
    "        term_to_idx = {t: i for i, t in enumerate(terms_list)}\n",
    "        n_terms = len(terms_list)\n",
    "        src, dst = [], []\n",
    "\n",
    "        for child in terms_list:\n",
    "            parents = parents_dict.get(child, set())\n",
    "            if not parents:\n",
    "                continue\n",
    "            child_idx = term_to_idx[child]\n",
    "            for parent in parents:\n",
    "                if parent in term_to_idx:\n",
    "                    parent_idx = term_to_idx[parent]\n",
    "                    src.append(child_idx); dst.append(parent_idx)\n",
    "                    src.append(parent_idx); dst.append(child_idx)\n",
    "\n",
    "        # self-loops\n",
    "        src.extend(range(n_terms))\n",
    "        dst.extend(range(n_terms))\n",
    "\n",
    "        indices = torch.tensor([src, dst], dtype=torch.long)\n",
    "        values = torch.ones(len(src), dtype=torch.float32)\n",
    "        return torch.sparse_coo_tensor(indices, values, (n_terms, n_terms)).coalesce().to(device)\n",
    "\n",
    "    class SimpleGCN(nn.Module):\n",
    "        def __init__(self, input_dim, hidden_dim, output_dim, adj_matrix):\n",
    "            super().__init__()\n",
    "            self.adj = adj_matrix\n",
    "            self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "            self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "            self.relu = nn.ReLU()\n",
    "            self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.relu(self.fc1(x))\n",
    "            x = self.dropout(x)\n",
    "            x = self.fc2(x)\n",
    "            x = torch.sparse.mm(self.adj, x.t()).t()\n",
    "            return torch.sigmoid(x)\n",
    "\n",
    "    # 4. Build test stack once (and inject external priors once), then split by ontology\n",
    "    print(\"\\nPreparing test stack...\")\n",
    "    test_feats = []\n",
    "    for fname in ['test_pred_logreg.npy', 'test_pred_gbdt.npy', 'test_pred_dnn.npy']:\n",
    "        p = ARTEFACTS_DIR / 'features' / fname\n",
    "        if p.exists():\n",
    "            test_feats.append(np.load(p))\n",
    "\n",
    "    if not test_feats:\n",
    "        raise FileNotFoundError(\"No Level-1 test predictions found. Run Phase 2 first.\")\n",
    "\n",
    "    X_test_stack = np.mean(test_feats, axis=0).astype(np.float32)\n",
    "\n",
    "    prior_test_path = ext_dir / 'prop_test_no_kaggle.tsv.gz'\n",
    "    if PROCESS_EXTERNAL and prior_test_path.exists():\n",
    "        test_ids = pd.read_feather(ARTEFACTS_DIR / 'parsed' / 'test_seq.feather')['id'].astype(str)\n",
    "        prior_test = pd.read_csv(prior_test_path, sep='\\t')\n",
    "        prior_test = prior_test[prior_test['term'].isin(top_terms)]\n",
    "        prior_t = prior_test.pivot_table(index='EntryID', columns='term', values='score', aggfunc='max', fill_value=0.0)\n",
    "        prior_t = prior_t.reindex(test_ids.tolist(), fill_value=0.0)\n",
    "        prior_t = prior_t.reindex(columns=top_terms, fill_value=0.0)\n",
    "        prior_test_np = prior_t.values.astype(np.float32)\n",
    "        X_test_stack = np.maximum(X_test_stack, EXTERNAL_PRIOR_WEIGHT * prior_test_np)\n",
    "        print(f\"Injected external IEA prior into test stack (weight={EXTERNAL_PRIOR_WEIGHT}).\")\n",
    "    else:\n",
    "        print(\"No external test prior found (or PROCESS_EXTERNAL=False); inferring without GOA priors.\")\n",
    "\n",
    "    # 5. Ontology split (BP/MF/CC)\n",
    "    if 'go_namespaces' not in locals():\n",
    "        raise RuntimeError(\"go_namespaces not found. Run Phase 1 OBO parsing cell first.\")\n",
    "\n",
    "    ns_to_aspect = {\n",
    "        'molecular_function': 'MF',\n",
    "        'biological_process': 'BP',\n",
    "        'cellular_component': 'CC',\n",
    "    }\n",
    "    aspects = []\n",
    "    for t in top_terms:\n",
    "        asp = ns_to_aspect.get(go_namespaces.get(t, ''), 'BP')  # default BP to keep full coverage\n",
    "        aspects.append(asp)\n",
    "    aspects = np.array(aspects)\n",
    "\n",
    "    aspect_to_idx = {\n",
    "        'BP': np.where(aspects == 'BP')[0].tolist(),\n",
    "        'MF': np.where(aspects == 'MF')[0].tolist(),\n",
    "        'CC': np.where(aspects == 'CC')[0].tolist(),\n",
    "    }\n",
    "    for k in ['BP', 'MF', 'CC']:\n",
    "        print(f\"Terms[{k}]={len(aspect_to_idx[k])}\")\n",
    "\n",
    "    # 6. Train 3 specialised GCNs and stitch outputs back\n",
    "    test_pred_gcn = np.zeros_like(X_test_stack, dtype=np.float32)\n",
    "    X_tensor_full = torch.tensor(X_stack, dtype=torch.float32, device=device)\n",
    "    Y_tensor_full = torch.tensor(Y, dtype=torch.float32, device=device)\n",
    "    X_test_full = torch.tensor(X_test_stack, dtype=torch.float32, device=device)\n",
    "\n",
    "    def train_one(aspect_name: str, idx_cols: list[int]):\n",
    "        if not idx_cols:\n",
    "            print(f\"[{aspect_name}] No terms; skipping.\")\n",
    "            return None\n",
    "        terms_sub = [top_terms[i] for i in idx_cols]\n",
    "        adj = build_adjacency(terms_sub, go_parents)\n",
    "        model = SimpleGCN(input_dim=len(idx_cols), hidden_dim=1024, output_dim=len(idx_cols), adj_matrix=adj).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "        criterion = nn.BCELoss()\n",
    "\n",
    "        X_t = X_tensor_full[:, idx_cols]\n",
    "        Y_t = Y_tensor_full[:, idx_cols]\n",
    "        n_samples = X_t.shape[0]\n",
    "        BS = 256\n",
    "        EPOCHS = 5\n",
    "\n",
    "        model.train()\n",
    "        print(f\"\\n=== Training GCN[{aspect_name}] terms={len(idx_cols)} ===\")\n",
    "        for epoch in range(EPOCHS):\n",
    "            total_loss = 0.0\n",
    "            perm = torch.randperm(n_samples, device=device)\n",
    "            for i in range(0, n_samples, BS):\n",
    "                b = perm[i:i + BS]\n",
    "                optimizer.zero_grad()\n",
    "                out = model(X_t[b])\n",
    "                loss = criterion(out, Y_t[b])\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += float(loss.item())\n",
    "\n",
    "            with torch.no_grad():\n",
    "                pred = (model(X_t[:2000]) > 0.3).float().cpu().numpy()\n",
    "                f1 = f1_score(Y[:2000, idx_cols], pred, average='micro')\n",
    "            denom = max(1.0, (n_samples / BS))\n",
    "            print(f\"Epoch {epoch+1}: loss={total_loss / denom:.4f}, micro-F1@0.30≈{f1:.4f}\")\n",
    "\n",
    "        torch.save(model.state_dict(), ARTEFACTS_DIR / 'features' / f'gcn_stacker_{aspect_name.lower()}.pth')\n",
    "        print(f\"Saved: gcn_stacker_{aspect_name.lower()}.pth\")\n",
    "        return model\n",
    "\n",
    "    models = {}\n",
    "    for asp in ['BP', 'MF', 'CC']:\n",
    "        models[asp] = train_one(asp, aspect_to_idx[asp])\n",
    "\n",
    "    print(\"\\nInferring test predictions with specialised GCNs...\")\n",
    "    for asp in ['BP', 'MF', 'CC']:\n",
    "        idx_cols = aspect_to_idx[asp]\n",
    "        if not idx_cols:\n",
    "            continue\n",
    "        model = models[asp]\n",
    "        model.eval()\n",
    "        X_te = X_test_full[:, idx_cols]\n",
    "        preds = []\n",
    "        BS = 2048\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, X_te.shape[0], BS):\n",
    "                preds.append(model(X_te[i:i + BS]).cpu().numpy())\n",
    "        pred_sub = np.vstack(preds).astype(np.float32)\n",
    "        test_pred_gcn[:, idx_cols] = pred_sub\n",
    "\n",
    "    np.save(ARTEFACTS_DIR / 'features' / 'test_pred_gcn.npy', test_pred_gcn)\n",
    "    print(\"Saved: test_pred_gcn.npy\")\n",
    "else:\n",
    "    print(\"Skipping GCN training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988e86ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. PHASE 4: POST-PROCESSING & SUBMISSION\n",
    "# ========================================\n",
    "# HARDWARE: CPU / GPU\n",
    "# ========================================\n",
    "\n",
    "# This phase applies the \"Strict Post-Processing\" rules (Max/Min Propagation)\n",
    "# and generates the final submission file.\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Check if submission already exists\n",
    "if (ARTEFACTS_DIR / 'submission.tsv').exists():\n",
    "    print(\"submission.tsv already exists. Skipping Phase 4.\")\n",
    "else:\n",
    "    print(\"Starting Phase 4: Post-processing & submission...\")\n",
    "\n",
    "    # Ensure go_parents is available (from Phase 1)\n",
    "    if 'go_parents' not in locals() or 'go_namespaces' not in locals():\n",
    "        print(\"Reloading GO graph (parse_obo)...\")\n",
    "\n",
    "        def parse_obo(path: Path):\n",
    "            parents = {}\n",
    "            namespaces = {}\n",
    "            cur_id, cur_ns = None, None\n",
    "            with path.open('r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    if line == '[Term]':\n",
    "                        if cur_id and cur_ns:\n",
    "                            namespaces[cur_id] = cur_ns\n",
    "                        cur_id, cur_ns = None, None\n",
    "                    elif line.startswith('id: GO:'):\n",
    "                        cur_id = line.split('id: ', 1)[1]\n",
    "                    elif line.startswith('namespace:'):\n",
    "                        cur_ns = line.split('namespace: ', 1)[1]\n",
    "                    elif line.startswith('is_a:') and cur_id:\n",
    "                        parent = line.split('is_a: ', 1)[1].split(' ! ')[0]\n",
    "                        parents.setdefault(cur_id, set()).add(parent)\n",
    "                if cur_id and cur_ns:\n",
    "                    namespaces[cur_id] = cur_ns\n",
    "            return parents, namespaces\n",
    "\n",
    "        go_parents, go_namespaces = parse_obo(PATH_GO_OBO)\n",
    "\n",
    "    # Load test IDs\n",
    "    test_ids = pd.read_feather(ARTEFACTS_DIR / 'parsed' / 'test_seq.feather')['id']\n",
    "\n",
    "    # Load stacker predictions\n",
    "    pred_path = ARTEFACTS_DIR / 'features' / 'test_pred_gcn.npy'\n",
    "    if not pred_path.exists():\n",
    "        raise FileNotFoundError(\"Missing `test_pred_gcn.npy`. Run Phase 3 (GCN stacker) first.\")\n",
    "    preds = np.load(pred_path)\n",
    "\n",
    "    # Load term list (must match Phase 3)\n",
    "    terms_path = ARTEFACTS_DIR / 'features' / 'top_terms_1500.json'\n",
    "    if terms_path.exists():\n",
    "        with open(terms_path, 'r') as f:\n",
    "            top_terms = json.load(f)\n",
    "    else:\n",
    "        print(\"Warning: top_terms_1500.json missing; rebuilding from train_terms counts (may mismatch Phase 3).\")\n",
    "        train_terms = pd.read_parquet(ARTEFACTS_DIR / 'parsed' / 'train_terms.parquet')\n",
    "        top_terms = train_terms['term'].value_counts().head(preds.shape[1]).index.tolist()\n",
    "\n",
    "    if preds.shape[1] != len(top_terms):\n",
    "        raise ValueError(f\"Shape mismatch: preds has {preds.shape[1]} terms, top_terms has {len(top_terms)}.\")\n",
    "\n",
    "    # ------------------------------------------\n",
    "    # Strict post-processing (Max/Min Propagation)\n",
    "    # ------------------------------------------\n",
    "    print(f\"Applying hierarchy rules on {len(top_terms)} terms...\")\n",
    "    df_pred = pd.DataFrame(preds, columns=top_terms)\n",
    "\n",
    "    term_set = set(top_terms)\n",
    "    term_to_parents = {}\n",
    "    term_to_children = {}\n",
    "\n",
    "    for term in top_terms:\n",
    "        parents = go_parents.get(term, set())\n",
    "        if not parents:\n",
    "            continue\n",
    "        parents = parents.intersection(term_set)\n",
    "        if not parents:\n",
    "            continue\n",
    "        term_to_parents[term] = list(parents)\n",
    "        for p in parents:\n",
    "            term_to_children.setdefault(p, []).append(term)\n",
    "\n",
    "    # Max Propagation (Child -> Parent)\n",
    "    for _ in range(2):\n",
    "        for child, parents in term_to_parents.items():\n",
    "            child_scores = df_pred[child].values\n",
    "            for parent in parents:\n",
    "                df_pred[parent] = np.maximum(df_pred[parent].values, child_scores)\n",
    "\n",
    "    # Min Propagation (Parent -> Child)\n",
    "    for _ in range(2):\n",
    "        for parent, children in term_to_children.items():\n",
    "            parent_scores = df_pred[parent].values\n",
    "            for child in children:\n",
    "                df_pred[child] = np.minimum(df_pred[child].values, parent_scores)\n",
    "\n",
    "    # ------------------------------------------\n",
    "    # Submission formatting (CAFA rules)\n",
    "    # - tab-separated, no header\n",
    "    # - score in (0, 1.000]\n",
    "    # - up to 3 significant figures\n",
    "    # - <= 1500 terms per target (MF/BP/CC combined)\n",
    "    # ------------------------------------------\n",
    "    df_pred['EntryID'] = test_ids.values\n",
    "    submission = df_pred.melt(id_vars='EntryID', var_name='term', value_name='score')\n",
    "\n",
    "    # Enforce score range + remove zeros\n",
    "    submission['score'] = submission['score'].clip(lower=0.0, upper=1.0)\n",
    "    submission = submission[submission['score'] > 0.0]\n",
    "\n",
    "    # Light pruning (keeps file size sane; still rule-compliant)\n",
    "    submission = submission[submission['score'] >= 0.001]\n",
    "\n",
    "    # Keep top 1500 per protein (rule)\n",
    "    submission = submission.sort_values(['EntryID', 'score'], ascending=[True, False])\n",
    "    submission = submission.groupby('EntryID', sort=False).head(1500)\n",
    "\n",
    "    # Write with <= 3 significant figures\n",
    "    submission.to_csv(\n",
    "        ARTEFACTS_DIR / 'submission.tsv',\n",
    "        sep='\\t',\n",
    "        index=False,\n",
    "        header=False,\n",
    "        float_format='%.3g',\n",
    "    )\n",
    "\n",
    "    print(f\"Done! Submission saved to {ARTEFACTS_DIR / 'submission.tsv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf182ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. PHASE 5: FREE TEXT PREDICTION (OPTIONAL)\n",
    "# ==========================================\n",
    "# HARDWARE: CPU\n",
    "# ==========================================\n",
    "\n",
    "# Official CAFA constraints (summary):\n",
    "# - Combined file (GO + Text) allowed\n",
    "# - Text: up to 5 lines per protein; ASCII printable; no tabs; <=3000 chars per protein total\n",
    "# - Scores should be in (0, 1.000] and up to 3 significant figures\n",
    "\n",
    "if (ARTEFACTS_DIR / 'submission_with_text.tsv').exists():\n",
    "    print(\"submission_with_text.tsv already exists. Skipping Phase 5.\")\n",
    "elif not (ARTEFACTS_DIR / 'submission.tsv').exists():\n",
    "    print(\"submission.tsv not found. Please run Phase 4 first.\")\n",
    "else:\n",
    "    print(\"Starting Phase 5: Text Generation...\")\n",
    "\n",
    "    # 1. Load Submission & GO Graph\n",
    "    print(\"Loading submission and GO data...\")\n",
    "    submission = pd.read_csv(\n",
    "        ARTEFACTS_DIR / 'submission.tsv',\n",
    "        sep='\\t',\n",
    "        header=None,\n",
    "        names=['EntryID', 'term', 'score'],\n",
    "    )\n",
    "\n",
    "    if 'graph' not in locals():\n",
    "        import obonet\n",
    "        graph = obonet.read_obo(PATH_GO_OBO)\n",
    "\n",
    "    # 2. Generate Text Descriptions\n",
    "    print(\"Generating descriptions...\")\n",
    "\n",
    "    # Pre-fetch term names to avoid graph lookups in loop\n",
    "    term_names = {node: data.get('name', 'unknown function') for node, data in graph.nodes(data=True)}\n",
    "\n",
    "    text_rows = []\n",
    "    unique_ids = submission['EntryID'].unique()\n",
    "\n",
    "    for protein_id in tqdm(unique_ids, desc=\"Generating Text\"):\n",
    "        prot_preds = submission[submission['EntryID'] == protein_id]\n",
    "        top_go = prot_preds.sort_values('score', ascending=False).head(3)\n",
    "\n",
    "        if top_go.empty:\n",
    "            # If we have no GO lines for this protein, skip text (keeps score>0 rule clean)\n",
    "            continue\n",
    "\n",
    "        term_descs = []\n",
    "        for _, row in top_go.iterrows():\n",
    "            term_id = row['term']\n",
    "            term_descs.append(term_names.get(term_id, term_id))\n",
    "\n",
    "        joined_terms = \", \".join(term_descs)\n",
    "        description = f\"{protein_id} is predicted to be involved in: {joined_terms}.\"\n",
    "\n",
    "        # Ensure no tabs in description\n",
    "        description = description.replace('\\t', ' ')\n",
    "\n",
    "        # Score: strictly > 0 and <= 1\n",
    "        score = float(top_go.iloc[0]['score'])\n",
    "        score = min(max(score, 0.001), 1.0)\n",
    "\n",
    "        # One line per protein (<=5 allowed)\n",
    "        text_rows.append({\n",
    "            'EntryID': protein_id,\n",
    "            'term': 'Text',\n",
    "            'score': score,\n",
    "            'description': description,\n",
    "        })\n",
    "\n",
    "    df_text = pd.DataFrame(text_rows)\n",
    "\n",
    "    print(\"Saving combined submission...\")\n",
    "\n",
    "    with open(ARTEFACTS_DIR / 'submission_with_text.tsv', 'w', encoding='utf-8') as f:\n",
    "        # 1) GO preds (3 cols) already CAFA-formatted in Phase 4\n",
    "        submission.to_csv(f, sep='\\t', index=False, header=False, float_format='%.3g')\n",
    "\n",
    "        # 2) Text preds (4 cols)\n",
    "        for _, row in df_text.iterrows():\n",
    "            # Up to 3 significant figures\n",
    "            score_str = format(float(row['score']), '.3g')\n",
    "            f.write(f\"{row['EntryID']}\\tText\\t{score_str}\\t{row['description']}\\n\")\n",
    "\n",
    "    print(f\"Done! Combined submission saved to {ARTEFACTS_DIR / 'submission_with_text.tsv'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
