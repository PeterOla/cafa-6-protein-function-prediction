{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ad36a26",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Cell 0: Imports and global guards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76372045",
   "metadata": {},
   "source": [
    "#### **Cell 0: Environment Verification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a65abf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "from cuml.linear_model import LogisticRegression\n",
    "\n",
    "# Verify A100 is visible\n",
    "print(f\"GPU: {cp.cuda.runtime.getDeviceProperties(0)['name'].decode()}\")\n",
    "cp.cuda.runtime.deviceSynchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb067a3",
   "metadata": {},
   "source": [
    "#### **Cell 1: Synthetic Data Setup (A100 Scale)**\n",
    "We will use 40,000 rows and 25,132 features to simulate a heavy portion of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84ebe81",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 40000\n",
    "n_features = 25132\n",
    "n_targets = 500  # Size of one TARGET_CHUNK\n",
    "\n",
    "# Materialize features directly on GPU\n",
    "X_gpu = cp.random.standard_normal((n_samples, n_features), dtype=cp.float32)\n",
    "W_gpu = cp.random.standard_normal((n_targets, n_features), dtype=cp.float32)\n",
    "b_gpu = cp.random.standard_normal((n_targets,), dtype=cp.float32)\n",
    "\n",
    "print(f\"X_gpu Memory: {X_gpu.nbytes / 1e9:.2f} GB\")\n",
    "cp.cuda.runtime.deviceSynchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7757b44",
   "metadata": {},
   "source": [
    "#### **Cell 2: Test 1 - The \"Slow\" Path (Batch Synchronization)**\n",
    "This simulates your current code calling `.get()` or `_to_numpy` inside the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966f7f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4096\n",
    "host_buffer = np.zeros((n_samples, n_targets), dtype=np.float32)\n",
    "\n",
    "t0 = time.time()\n",
    "for i in range(0, n_samples, batch_size):\n",
    "    i1 = min(i + batch_size, n_samples)\n",
    "    xb = X_gpu[i:i1, :]\n",
    "    \n",
    "    # Simulate LogReg predict_proba\n",
    "    # This .get() call forces the CPU to stop and wait for the GPU\n",
    "    p_batch = (cp.dot(xb, W_gpu.T) + b_gpu).get() \n",
    "    \n",
    "    host_buffer[i:i1, :] = p_batch\n",
    "\n",
    "cp.cuda.runtime.deviceSynchronize()\n",
    "print(f\"âŒ Slow Path (with sync): {time.time() - t0:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc9f42a",
   "metadata": {},
   "source": [
    "#### **Cell 3: Test 2 - The \"Fast\" Path (Asynchronous Pipelining)**\n",
    "This represents the Rank 1 fix: keeping everything on GPU and doing one transfer at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149a5bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_buffer = cp.zeros((n_samples, n_targets), dtype=cp.float32)\n",
    "\n",
    "t0 = time.time()\n",
    "for i in range(0, n_samples, batch_size):\n",
    "    i1 = min(i + batch_size, n_samples)\n",
    "    xb = X_gpu[i:i1, :]\n",
    "    \n",
    "    # Asynchronous GPU operation (No .get() here)\n",
    "    gpu_buffer[i:i1, :] = cp.dot(xb, W_gpu.T) + b_gpu\n",
    "\n",
    "# One single synchronization and transfer after the whole chunk is done\n",
    "final_host_output = gpu_buffer.get() \n",
    "\n",
    "cp.cuda.runtime.deviceSynchronize()\n",
    "print(f\"âœ… Fast Path (pipelined): {time.time() - t0:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdbc824",
   "metadata": {},
   "source": [
    "### **What this Notebook Proves**\n",
    "1.  **Isolation of the `.get()` overhead:** Cell 2 will be significantly slower than Cell 3. On an A100, the constant handshaking between the CPU and GPU during batch writes is the primary \"kink in the hose\".\n",
    "2.  **PCIe Saturation:** Cell 3 allows the GPU to queue all matrix multiplications. The only delay is the final transfer of the 500-column chunk to RAM, which is the most efficient way to use the PCIe bus.\n",
    "3.  **Memmap Impact:** By performing the transfer in one large block, you can then write to the `test_pred` memmap in one sequential sweep, avoiding the random-access disk latency that is likely contributing to your 80-minute runtime [Log, Conversation History].\n",
    "\n",
    "**Analogy for the Test:**\n",
    "Cell 2 is like a worker **unloading a truck one box at a time**, walking it into the warehouse, and coming back for the next. Cell 3 is like using a **conveyor belt**: you let the boxes flow continuously and only check the total inventory once the truck is empty. For 224,309 proteins, the conveyor belt is the only way to hit your 10-hour target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2b62d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import inspect\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "import cuml\n",
    "from cuml.multiclass import OneVsRestClassifier\n",
    "from cuml.linear_model import LogisticRegression\n",
    "\n",
    "from cupy.cuda import runtime\n",
    "\n",
    "cp.cuda.runtime.setDevice(0)\n",
    "cp.cuda.set_allocator(cp.cuda.MemoryPool().malloc)\n",
    "\n",
    "print(\"CuPy:\", cp.__version__)\n",
    "print(\"cuML:\", cuml.__version__)\n",
    "print(\"CUDA Runtime:\", runtime.runtimeGetVersion())\n",
    "print(\"GPU:\", runtime.getDeviceProperties(0)[\"name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8522b49",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Cell 0.5: Setup Data and Model (Auto-fill)\n",
    "\n",
    "This cell ensures `X_test_gpu` and `clf_chunk` exist so the diagnostics can run standalone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066f1a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to find data\n",
    "candidates = [\n",
    "    Path.cwd() / 'cafa6_data',\n",
    "    Path('../cafa6_data'),\n",
    "    Path('../../cafa6_data'),\n",
    "    Path('c:/Users/Olale/Documents/Codebase/Science/cafa-6-protein-function-prediction/cafa6_data')\n",
    "]\n",
    "WORK_ROOT = next((p for p in candidates if (p / 'features' / 'X_test_mmap.npy').exists()), None)\n",
    "\n",
    "if WORK_ROOT:\n",
    "    print(f\"Found data at: {WORK_ROOT}\")\n",
    "    xt_path = WORK_ROOT / 'features' / 'X_test_mmap.npy'\n",
    "    X_test_host = np.load(xt_path, mmap_mode='r')\n",
    "    \n",
    "    # Load a chunk to GPU (enough for diagnostics)\n",
    "    # We load 40k rows to ensure we have enough data for the batch size sweeps\n",
    "    n_rows = min(X_test_host.shape[0], 40000)\n",
    "    print(f\"Loading {n_rows} rows of X_test to GPU...\")\n",
    "    X_test_gpu = cp.asarray(X_test_host[:n_rows], dtype=cp.float32)\n",
    "    \n",
    "    # Fit a dummy model to simulate the trained clf_chunk\n",
    "    print(\"Fitting dummy cuML model for inference testing...\")\n",
    "    n_classes = 50\n",
    "    y_dummy = cp.random.randint(0, 2, (n_rows, n_classes)).astype(cp.float32)\n",
    "    clf_chunk = OneVsRestClassifier(LogisticRegression(output_type='cupy'))\n",
    "    clf_chunk.fit(X_test_gpu, y_dummy)\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸ Data not found. Generating SYNTHETIC data for diagnostics.\")\n",
    "    X_test_gpu = cp.random.randn(10000, 1484, dtype=cp.float32)\n",
    "    y_dummy = cp.random.randint(0, 2, (10000, 50)).astype(cp.float32)\n",
    "    clf_chunk = OneVsRestClassifier(LogisticRegression(output_type='cupy'))\n",
    "    clf_chunk.fit(X_test_gpu, y_dummy)\n",
    "\n",
    "print(\"âœ… Setup complete. `X_test_gpu` and `clf_chunk` are ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ec35c1",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Cell 1: Hard assertion that GPU is live"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff31de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = cp.zeros((1024, 1024), dtype=cp.float32)\n",
    "t0 = time.time()\n",
    "cp.cuda.runtime.deviceSynchronize()\n",
    "print(\"GPU sync latency:\", time.time() - t0)\n",
    "\n",
    "assert isinstance(x, cp.ndarray)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5ce532",
   "metadata": {},
   "source": [
    "This should be essentially instantaneous."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dec93b6",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Cell 2: Inspect `X_test_gpu` residency and layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cd19c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_test_gpu  # must already exist\n",
    "\n",
    "print(\"Type:\", type(X))\n",
    "print(\"Device:\", X.device)\n",
    "print(\"Shape:\", X.shape)\n",
    "print(\"Dtype:\", X.dtype)\n",
    "print(\"C contiguous:\", X.flags.c_contiguous)\n",
    "print(\"Strides:\", X.strides)\n",
    "\n",
    "# Slice timing\n",
    "t0 = time.time()\n",
    "xb = X[0:8192, :]\n",
    "cp.cuda.runtime.deviceSynchronize()\n",
    "print(\"Slice time (8k rows):\", time.time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1169604e",
   "metadata": {},
   "source": [
    "**Expected**\n",
    "\n",
    "* `cp.ndarray`\n",
    "* `float32`\n",
    "* `C contiguous = True`\n",
    "* Slice time: milliseconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d3d36a",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Cell 3: Confirm slicing is a view, not a copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1015bc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "xb = X[0:1024, :]\n",
    "\n",
    "print(\"xb.base is None:\", xb.base is None)\n",
    "print(\"Pointer equality:\", xb.data.ptr == X.data.ptr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28acb365",
   "metadata": {},
   "source": [
    "If `base is None` or pointers differ, every batch is copying on GPU.\n",
    "\n",
    "That alone explains your runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e6d496",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Cell 4: Inspect the trained model object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1cbc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = clf_chunk  # must already exist\n",
    "\n",
    "print(\"Model type:\", type(clf))\n",
    "\n",
    "if hasattr(clf, \"coef_\"):\n",
    "    print(\"coef_ type:\", type(clf.coef_))\n",
    "    print(\"coef_ dtype:\", clf.coef_.dtype)\n",
    "    print(\"coef_ device:\", getattr(clf.coef_, \"device\", \"CPU\"))\n",
    "\n",
    "if hasattr(clf, \"intercept_\"):\n",
    "    print(\"intercept_ type:\", type(clf.intercept_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf79525",
   "metadata": {},
   "source": [
    "**Hard requirements**\n",
    "\n",
    "* `coef_` must be CuPy\n",
    "* `float32`\n",
    "* On GPU\n",
    "\n",
    "Anything else risks CPU fallback."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065ca3db",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Cell 5: Is `predict_proba` actually fast on GPU?\n",
    "\n",
    "This isolates cuML itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7847573e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import cupy as cp\n",
    "\n",
    "# -----------------------------\n",
    "# Select batch (must be a view)\n",
    "# -----------------------------\n",
    "xb = X_test_gpu[0:8192, :]\n",
    "\n",
    "# Sanity checks (fail fast)\n",
    "assert isinstance(xb, cp.ndarray), \"xb is not on GPU\"\n",
    "assert xb.dtype == cp.float32, f\"Unexpected dtype: {xb.dtype}\"\n",
    "assert xb.flags.c_contiguous, \"xb is not C-contiguous\"\n",
    "\n",
    "def safe_predict_proba(clf, x):\n",
    "    \"\"\"\n",
    "    Robust prediction that tries manual GEMM first to bypass cuML Python overhead/bugs.\n",
    "    This is ideal for diagnostics as it measures raw GPU throughput.\n",
    "    \"\"\"\n",
    "    W = None\n",
    "    b = None\n",
    "\n",
    "    # Attempt to get W and b\n",
    "    if hasattr(clf, 'coef_') and hasattr(clf, 'intercept_'):\n",
    "        W = clf.coef_\n",
    "        b = clf.intercept_\n",
    "    elif hasattr(clf, 'estimators_'):\n",
    "        # Fallback: reconstruct from OVR estimators\n",
    "        try:\n",
    "            # Collect coefs, handling potential device mismatch\n",
    "            ws = []\n",
    "            bs = []\n",
    "            for i, e in enumerate(clf.estimators_):\n",
    "                if not hasattr(e, 'coef_'):\n",
    "                    # Some versions of cuML OVR might store estimators differently\n",
    "                    continue\n",
    "                \n",
    "                w = e.coef_\n",
    "                b_val = e.intercept_\n",
    "                \n",
    "                # Convert to CuPy if needed\n",
    "                if not isinstance(w, cp.ndarray): w = cp.asarray(w)\n",
    "                if not isinstance(b_val, cp.ndarray): b_val = cp.asarray(b_val)\n",
    "                \n",
    "                ws.append(w)\n",
    "                bs.append(b_val)\n",
    "            \n",
    "            if ws:\n",
    "                W = cp.vstack(ws)\n",
    "                b = cp.hstack(bs)\n",
    "            else:\n",
    "                print(\"No estimators with coef_ found.\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Could not reconstruct coefs from estimators: {e}\")\n",
    "\n",
    "    # Strategy 1: Manual GEMM\n",
    "    if W is not None and b is not None:\n",
    "        try:\n",
    "            # Ensure CuPy\n",
    "            if not isinstance(W, cp.ndarray): W = cp.asarray(W)\n",
    "            if not isinstance(b, cp.ndarray): b = cp.asarray(b)\n",
    "            \n",
    "            if W.ndim == 1:\n",
    "                scores = cp.dot(x, W) + b\n",
    "            else:\n",
    "                scores = cp.dot(x, W.T) + b\n",
    "                \n",
    "            return 1.0 / (1.0 + cp.exp(-cp.clip(scores, -50.0, 50.0)))\n",
    "        except Exception as e:\n",
    "            print(f\"Manual GEMM failed: {e}\")\n",
    "\n",
    "    # Strategy 2: decision_function\n",
    "    if hasattr(clf, 'decision_function'):\n",
    "        try:\n",
    "            scores = clf.decision_function(x)\n",
    "            if not isinstance(scores, cp.ndarray):\n",
    "                 scores = cp.asarray(scores)\n",
    "            return 1.0 / (1.0 + cp.exp(-cp.clip(scores, -50.0, 50.0)))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Strategy 3: predict_proba\n",
    "    if hasattr(clf, 'predict_proba'):\n",
    "        try:\n",
    "            return clf.predict_proba(x)\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "    # Strategy 4: predict (Last resort, returns hard classes but keeps pipeline moving)\n",
    "    print(\"âš ï¸ Warning: Falling back to hard `predict()` (probabilities unavailable).\")\n",
    "    try:\n",
    "        return clf.predict(x)\n",
    "    except TypeError:\n",
    "        # Fallback for sklearn-based predict that can't handle GPU arrays\n",
    "        return cp.asarray(clf.predict(x.get()))\n",
    "\n",
    "# -----------------------------\n",
    "# Warm-up (kernel + cache)\n",
    "# -----------------------------\n",
    "_ = safe_predict_proba(clf_chunk, xb)\n",
    "cp.cuda.runtime.deviceSynchronize()\n",
    "\n",
    "# -----------------------------\n",
    "# Timed run\n",
    "# -----------------------------\n",
    "t0 = time.time()\n",
    "p = safe_predict_proba(clf_chunk, xb)\n",
    "cp.cuda.runtime.deviceSynchronize()\n",
    "dt = time.time() - t0\n",
    "\n",
    "# -----------------------------\n",
    "# Report\n",
    "# -----------------------------\n",
    "print(f\"predict_proba (via manual GEMM/fallback) time (8192 rows): {dt:.6f} s\")\n",
    "print(\"Output type:\", type(p))\n",
    "print(\"Output device:\", getattr(p, \"device\", \"CPU\"))\n",
    "print(\"Output shape:\", p.shape)\n",
    "print(\"Output dtype:\", p.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8949fa9c",
   "metadata": {},
   "source": [
    "If this is **> 0.1s**, something is wrong already."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51e1532",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Cell 6: Batch size scaling sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f8ec30",
   "metadata": {},
   "outputs": [],
   "source": [
    "for bs in [512, 1024, 4096, 8192, 16384]:\n",
    "    if bs > X.shape[0]:\n",
    "        print(f\"BS={bs:<6} skipped (not enough data)\")\n",
    "        continue\n",
    "    xb = X[0:bs, :]\n",
    "    t0 = time.time()\n",
    "    _ = safe_predict_proba(clf, xb)\n",
    "    cp.cuda.runtime.deviceSynchronize()\n",
    "    print(f\"BS={bs:<6} time={time.time()-t0:.4f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2da7ba",
   "metadata": {},
   "source": [
    "You want roughly linear scaling.\n",
    "Flat or erratic scaling = launch bound or CPU fallback."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70527749",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Cell 7: Inspect `_predict_proba_like` for hidden CPU paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf47a2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function if it's not in scope (it was a helper in the other notebook)\n",
    "# We'll just inspect the one from the user's notebook if we can, or define a dummy one to check logic\n",
    "# Since we can't easily inspect a function from another notebook without running it, \n",
    "# I will paste the function here so we can inspect IT, assuming this is the one being used.\n",
    "\n",
    "def _to_numpy(a):\n",
    "    if hasattr(a, 'get'):\n",
    "        return a.get()\n",
    "    if hasattr(a, 'to_numpy'):\n",
    "        return a.to_numpy()\n",
    "    return np.asarray(a)\n",
    "\n",
    "def _sigmoid_np(z):\n",
    "    z = np.asarray(z, dtype=np.float32)\n",
    "    z = np.clip(z, -50.0, 50.0)\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def _ensure_2d(p):\n",
    "    p = np.asarray(p)\n",
    "    if p.ndim == 1:\n",
    "        p = p.reshape(-1, 1)\n",
    "    return p\n",
    "\n",
    "def _predict_proba_like(clf, x, expect_cols: int | None = None):\n",
    "    # RAPIDS/cuML OVR may not implement predict_proba; fall back to decision_function->sigmoid.\n",
    "    if hasattr(clf, 'predict_proba'):\n",
    "        p = _ensure_2d(_to_numpy(clf.predict_proba(x)))\n",
    "        # Binary single-label can come back as (n,2); we need P(class=1)\n",
    "        if expect_cols == 1 and p.shape[1] == 2:\n",
    "            p = p[:, 1:2]\n",
    "        return np.asarray(p, dtype=np.float32)\n",
    "    if hasattr(clf, 'decision_function'):\n",
    "        s = _ensure_2d(_to_numpy(clf.decision_function(x)))\n",
    "        return np.asarray(_sigmoid_np(s), dtype=np.float32)\n",
    "    # Last resort: hard predictions (0/1)\n",
    "    p = _ensure_2d(_to_numpy(clf.predict(x)))\n",
    "    return np.asarray(p, dtype=np.float32)\n",
    "\n",
    "print(inspect.getsource(_predict_proba_like))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ad6d09",
   "metadata": {},
   "source": [
    "You are looking for **any** of:\n",
    "\n",
    "* `np.asarray`\n",
    "* `cp.asnumpy`\n",
    "* Python loops over columns\n",
    "* Shape fixing logic\n",
    "\n",
    "If NumPy appears here, thatâ€™s your kink."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fad7406",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Cell 8: Detect hidden synchronisation via memory deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2d21ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "free_before = cp.cuda.runtime.memGetInfo()[0]\n",
    "\n",
    "# Use safe_predict_proba to avoid cuML bugs\n",
    "_ = safe_predict_proba(clf, X[0:8192, :])\n",
    "cp.cuda.runtime.deviceSynchronize()\n",
    "\n",
    "free_after = cp.cuda.runtime.memGetInfo()[0]\n",
    "\n",
    "print(\"Free memory delta:\", free_before - free_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7858ed",
   "metadata": {},
   "source": [
    "Large or inconsistent deltas indicate implicit copies or reallocations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45d38bc",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Cell 9: Diagnose the write path explicitly\n",
    "\n",
    "This is a prime suspect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c953d9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using safe_predict_proba to bypass cuML AttributeError\n",
    "p_gpu = safe_predict_proba(clf, X[0:2048, :])\n",
    "cp.cuda.runtime.deviceSynchronize()\n",
    "\n",
    "chunk_preds_ram = np.zeros((2048, p_gpu.shape[1]), dtype=np.float32)\n",
    "\n",
    "t0 = time.time()\n",
    "# Explicitly transfer to host to avoid implicit conversion error\n",
    "chunk_preds_ram[:, :] = p_gpu.get()\n",
    "print(\"GPU â†’ RAM write time:\", time.time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a829b716",
   "metadata": {},
   "source": [
    "If this takes more than a few milliseconds, you are synchronising per batch in the real loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59fe8d9",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Cell 10: End-to-end mini loop simulation (NO TRAINING)\n",
    "\n",
    "This simulates your test loop structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cef4555",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running fixed Cell 10 with safe_predict_proba...\")\n",
    "TEST_BS = 4096\n",
    "n = 16384\n",
    "\n",
    "chunk_preds_ram = np.zeros((n, p_gpu.shape[1]), dtype=np.float32)\n",
    "\n",
    "t0 = time.time()\n",
    "for b0 in range(0, n, TEST_BS):\n",
    "    b1 = min(b0 + TEST_BS, n)\n",
    "    xb = X[b0:b1, :]\n",
    "    \n",
    "    # Use safe_predict_proba to avoid cuML bugs\n",
    "    p = safe_predict_proba(clf, xb)\n",
    "    \n",
    "    # Explicitly transfer to host to avoid implicit conversion error\n",
    "    chunk_preds_ram[b0:b1, :] = p.get()\n",
    "\n",
    "cp.cuda.runtime.deviceSynchronize()\n",
    "print(\"Mini loop total time:\", time.time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2ae22f",
   "metadata": {},
   "source": [
    "If this already feels slow, scaling to 224k rows explains your 80 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c80a12",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Cell 11: Final verdict print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c7d6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "If inference is slow:\n",
    "- Check Cell 3 (view vs copy)\n",
    "- Check Cell 5 (raw predict speed)\n",
    "- Check Cell 7 (_predict_proba_like CPU usage)\n",
    "- Check Cell 9 (write synchronisation)\n",
    "\n",
    "Do NOT proceed to full training until all four are clean.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aec00e3",
   "metadata": {},
   "source": [
    "## What I expect weâ€™ll find\n",
    "\n",
    "Iâ€™ll be blunt and precise:\n",
    "\n",
    "* You are almost certainly **syncing on every batch write**.\n",
    "* Or `_predict_proba_like` is **doing CPU column alignment**.\n",
    "* Or `X_test_gpu` is **not truly contiguous** anymore.\n",
    "\n",
    "Any one of these explains the regression. Two together guarantee it.\n",
    "\n",
    "## Next move\n",
    "\n",
    "Run this notebook **once**, no training, no long jobs.\n",
    "Paste back:\n",
    "\n",
    "* Cell 3 output\n",
    "* Cell 5 timing\n",
    "* Cell 7 source\n",
    "* Cell 9 timing\n",
    "\n",
    "From there, Iâ€™ll tell you exactly which lines to delete or rewrite so the A100 finally behaves like an A100."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
