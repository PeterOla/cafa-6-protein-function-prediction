{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d2dbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment configuration\n",
    "ENVIRONMENT = 'local'  # Change to 'kaggle' when running on Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc182a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch transformers pandas numpy scikit-learn tqdm biopython -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cc857d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from Bio import SeqIO\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"‚úÖ Imports successful | Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca86c1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set base directory\n",
    "if ENVIRONMENT == 'kaggle':\n",
    "    base_dir = Path(\"/kaggle/input/cafa-6-dataset\")\n",
    "else:\n",
    "    base_dir = Path.cwd().parent\n",
    "\n",
    "print(f\"üìÅ Base directory: {base_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb448b6",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9043e288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sequences\n",
    "print(\"Loading sequences...\")\n",
    "sequences = {}\n",
    "for record in SeqIO.parse(base_dir / \"Train\" / \"train_sequences.fasta\", \"fasta\"):\n",
    "    sequences[record.id] = str(record.seq)\n",
    "\n",
    "print(f\"Loaded {len(sequences)} sequences\")\n",
    "\n",
    "# Load annotations\n",
    "print(\"\\nLoading annotations...\")\n",
    "train_terms = pd.read_csv(base_dir / \"Train\" / \"train_terms.tsv\", sep='\\t')\n",
    "print(f\"Total annotations: {len(train_terms)}\")\n",
    "\n",
    "# Load IA weights\n",
    "print(\"\\nLoading IA weights...\")\n",
    "ia_df = pd.read_csv(base_dir / \"IA.tsv\", sep='\\t')\n",
    "ia_weights = dict(zip(ia_df['term'], ia_df['IA']))\n",
    "print(f\"IA weights: {len(ia_weights)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276a2432",
   "metadata": {},
   "source": [
    "## 2. Train/Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5c6e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split proteins\n",
    "all_proteins = [p for p in train_terms['EntryID'].unique() if p in sequences]\n",
    "train_proteins, val_proteins = train_test_split(\n",
    "    all_proteins, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train proteins: {len(train_proteins)}\")\n",
    "print(f\"Val proteins: {len(val_proteins)}\")\n",
    "\n",
    "# Get validation ground truth\n",
    "val_data = train_terms[train_terms['EntryID'].isin(val_proteins)]\n",
    "print(f\"Val annotations: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a170f1",
   "metadata": {},
   "source": [
    "## 3. Generate ESM-2 Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb1d21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ESM-2 model\n",
    "print(\"Loading ESM-2 model...\")\n",
    "model_name = \"facebook/esm2_t6_8M_UR50D\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name).to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"‚úÖ Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8931b457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_sequences(protein_ids, sequences_dict, batch_size=8):\n",
    "    \"\"\"Generate embeddings for a list of proteins.\"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(protein_ids), batch_size), desc=\"Embedding\"):\n",
    "            batch_ids = protein_ids[i:i+batch_size]\n",
    "            batch_seqs = [sequences_dict[pid] for pid in batch_ids]\n",
    "            \n",
    "            # Tokenize\n",
    "            inputs = tokenizer(\n",
    "                batch_seqs,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512\n",
    "            ).to(device)\n",
    "            \n",
    "            # Get embeddings\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "            # Mean pooling (excluding padding)\n",
    "            attention_mask = inputs['attention_mask']\n",
    "            token_embeddings = outputs.last_hidden_state\n",
    "            \n",
    "            # Expand mask and apply\n",
    "            input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "            sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "            sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "            pooled = (sum_embeddings / sum_mask).cpu().numpy()\n",
    "            \n",
    "            embeddings.append(pooled)\n",
    "    \n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "print(\"‚úÖ Embedding function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17e49de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate train embeddings\n",
    "print(\"Generating train embeddings...\")\n",
    "train_embeddings = embed_sequences(train_proteins, sequences)\n",
    "print(f\"Train embeddings shape: {train_embeddings.shape}\")\n",
    "\n",
    "# Generate val embeddings\n",
    "print(\"\\nGenerating val embeddings...\")\n",
    "val_embeddings = embed_sequences(val_proteins, sequences)\n",
    "print(f\"Val embeddings shape: {val_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165f4cd8",
   "metadata": {},
   "source": [
    "## 4. Build K-NN Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91505430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build KNN index\n",
    "print(\"Building K-NN index...\")\n",
    "k = 10  # number of neighbours\n",
    "knn = NearestNeighbors(n_neighbors=k, metric='cosine', n_jobs=-1)\n",
    "knn.fit(train_embeddings)\n",
    "\n",
    "print(f\"‚úÖ K-NN index built with k={k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ffa28d",
   "metadata": {},
   "source": [
    "## 5. Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0b76fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find nearest neighbours for validation set\n",
    "print(\"Finding nearest neighbours...\")\n",
    "distances, indices = knn.kneighbors(val_embeddings)\n",
    "\n",
    "print(f\"Distances shape: {distances.shape}\")\n",
    "print(f\"Indices shape: {indices.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6113267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build protein -> terms mapping for train set\n",
    "train_annotations = train_terms[train_terms['EntryID'].isin(train_proteins)]\n",
    "protein_to_terms = train_annotations.groupby('EntryID')['term'].apply(list).to_dict()\n",
    "\n",
    "print(f\"Train proteins with annotations: {len(protein_to_terms)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151c7327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions by aggregating neighbour annotations\n",
    "print(\"Aggregating neighbour annotations...\")\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for val_idx, val_protein in enumerate(tqdm(val_proteins, desc=\"Predicting\")):\n",
    "    # Get neighbours\n",
    "    neighbour_indices = indices[val_idx]\n",
    "    neighbour_distances = distances[val_idx]\n",
    "    \n",
    "    # Collect all terms from neighbours with weights\n",
    "    term_scores = Counter()\n",
    "    \n",
    "    for nei_idx, distance in zip(neighbour_indices, neighbour_distances):\n",
    "        nei_protein = train_proteins[nei_idx]\n",
    "        nei_terms = protein_to_terms.get(nei_protein, [])\n",
    "        \n",
    "        # Weight by similarity (1 - distance for cosine)\n",
    "        similarity = 1 - distance\n",
    "        \n",
    "        for term in nei_terms:\n",
    "            term_scores[term] += similarity\n",
    "    \n",
    "    # Normalize scores to probabilities\n",
    "    if term_scores:\n",
    "        max_score = max(term_scores.values())\n",
    "        for term, score in term_scores.items():\n",
    "            predictions.append({\n",
    "                'EntryID': val_protein,\n",
    "                'term': term,\n",
    "                'probability': score / max_score\n",
    "            })\n",
    "\n",
    "predictions_df = pd.DataFrame(predictions)\n",
    "print(f\"\\nTotal predictions: {len(predictions_df)}\")\n",
    "print(f\"Avg predictions per protein: {len(predictions_df) / len(val_proteins):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1429e09",
   "metadata": {},
   "source": [
    "## 6. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20504865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions(predictions_df, ground_truth_df, ia_weights_dict, threshold=0.01):\n",
    "    \"\"\"Evaluate predictions against ground truth.\"\"\"\n",
    "    # Filter predictions by threshold\n",
    "    pred_filtered = predictions_df[predictions_df['probability'] >= threshold]\n",
    "    \n",
    "    # Group by protein\n",
    "    pred_grouped = pred_filtered.groupby('EntryID')['term'].apply(set).to_dict()\n",
    "    true_grouped = ground_truth_df.groupby('EntryID')['term'].apply(set).to_dict()\n",
    "    \n",
    "    # Compute per-protein metrics\n",
    "    f1_scores = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    \n",
    "    for protein in true_grouped.keys():\n",
    "        true_terms = true_grouped[protein]\n",
    "        pred_terms = pred_grouped.get(protein, set())\n",
    "        \n",
    "        if len(pred_terms) == 0:\n",
    "            f1_scores.append(0.0)\n",
    "            precisions.append(0.0)\n",
    "            recalls.append(0.0)\n",
    "            continue\n",
    "        \n",
    "        # Compute weighted metrics\n",
    "        tp_weight = sum(ia_weights_dict.get(t, 1.0) for t in true_terms & pred_terms)\n",
    "        fp_weight = sum(ia_weights_dict.get(t, 1.0) for t in pred_terms - true_terms)\n",
    "        fn_weight = sum(ia_weights_dict.get(t, 1.0) for t in true_terms - pred_terms)\n",
    "        \n",
    "        precision = tp_weight / (tp_weight + fp_weight) if (tp_weight + fp_weight) > 0 else 0\n",
    "        recall = tp_weight / (tp_weight + fn_weight) if (tp_weight + fn_weight) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        f1_scores.append(f1)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "    \n",
    "    return {\n",
    "        'f1': np.mean(f1_scores),\n",
    "        'precision': np.mean(precisions),\n",
    "        'recall': np.mean(recalls),\n",
    "        'coverage': len([p for p in pred_grouped if len(pred_grouped[p]) > 0]) / len(true_grouped)\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Evaluation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56394a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing different thresholds...\\n\")\n",
    "\n",
    "thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "results = []\n",
    "\n",
    "for thr in thresholds:\n",
    "    metrics = evaluate_predictions(predictions_df, val_data, ia_weights, threshold=thr)\n",
    "    results.append({\n",
    "        'threshold': thr,\n",
    "        **metrics\n",
    "    })\n",
    "    print(f\"Threshold {thr:.2f}: F1={metrics['f1']:.4f}, P={metrics['precision']:.4f}, \"\n",
    "          f\"R={metrics['recall']:.4f}, Coverage={metrics['coverage']:.2%}\")\n",
    "\n",
    "# Find best threshold\n",
    "best_result = max(results, key=lambda x: x['f1'])\n",
    "print(f\"\\nüèÜ Best F1: {best_result['f1']:.4f} at threshold {best_result['threshold']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd4bf66",
   "metadata": {},
   "source": [
    "## 7. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcaea86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "results_df = pd.DataFrame(results)\n",
    "output_path = Path(\"02_knn_baseline_results.csv\")\n",
    "results_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"‚úÖ Results saved to {output_path}\")\n",
    "print(\"\\nüìä Results:\")\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b79cd2",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**K-NN Baseline Performance:**\n",
    "- Uses ESM-2 embeddings for sequence similarity\n",
    "- Transfers annotations from k=10 nearest neighbours\n",
    "- Weights by cosine similarity\n",
    "- Expected F1: ~0.18 (from previous experiments)\n",
    "\n",
    "**Next:** 03_model_esm_finetuned.ipynb - Train classifier on embeddings"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
