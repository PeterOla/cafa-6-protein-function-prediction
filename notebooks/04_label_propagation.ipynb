{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b879868",
   "metadata": {},
   "source": [
    "# 04: Label Propagation with ESM-2\n",
    "\n",
    "**Graph-based label propagation**: Use GO ontology structure to propagate predictions to ancestor terms, improving consistency.\n",
    "\n",
    "Expected improvement: +0.02-0.04 F1 over base model\n",
    "\n",
    "**Prerequisites:** Trained model from `03_model_esm_finetuned.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e9c161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "%pip install torch obonet biopython transformers scikit-learn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f215bcff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports successful\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Set, Optional, Union\n",
    "\n",
    "# Bio imports\n",
    "import networkx as nx\n",
    "import obonet\n",
    "from Bio import SeqIO\n",
    "\n",
    "print(\"‚úÖ Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af87f751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Environment: LOCAL\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# ENVIRONMENT CONFIGURATION\n",
    "# ========================================\n",
    "# Set your environment: 'local' or 'kaggle'\n",
    "ENVIRONMENT = 'local'  # Change to 'kaggle' when running on Kaggle\n",
    "\n",
    "print(f\"üîß Environment: {ENVIRONMENT.upper()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59dec1c5",
   "metadata": {},
   "source": [
    "## 1. Helper Classes (Data Loaders)\n",
    "\n",
    "Embedded versions of OntologyLoader, SequenceLoader, and LabelLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f81e0737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data loader classes defined\n"
     ]
    }
   ],
   "source": [
    "class OntologyLoader:\n",
    "    \"\"\"Handles loading and traversing the Gene Ontology (GO) graph.\"\"\"\n",
    "    \n",
    "    def __init__(self, obo_path: Union[str, Path]):\n",
    "        self.obo_path = Path(obo_path)\n",
    "        self.graph = None\n",
    "        self._load()\n",
    "\n",
    "    def _load(self):\n",
    "        if not self.obo_path.exists():\n",
    "            raise FileNotFoundError(f\"OBO file not found: {self.obo_path}\")\n",
    "        print(f\"Loading ontology from {self.obo_path}...\")\n",
    "        self.graph = obonet.read_obo(self.obo_path)\n",
    "        print(f\"Loaded {len(self.graph)} terms.\")\n",
    "\n",
    "    def get_ancestors(self, term: str) -> Set[str]:\n",
    "        \"\"\"Get all ancestor terms (parents, grandparents, etc.)\"\"\"\n",
    "        if term not in self.graph:\n",
    "            return set()\n",
    "        return nx.ancestors(self.graph, term)\n",
    "\n",
    "    def get_parents(self, term: str) -> Set[str]:\n",
    "        \"\"\"Get immediate parent terms only\"\"\"\n",
    "        if term not in self.graph:\n",
    "            return set()\n",
    "        return set(self.graph.successors(term))\n",
    "\n",
    "    def get_namespace(self, term: str) -> Optional[str]:\n",
    "        \"\"\"Get ontology aspect (MF, BP, or CC)\"\"\"\n",
    "        if term in self.graph:\n",
    "            return self.graph.nodes[term].get('namespace')\n",
    "        return None\n",
    "\n",
    "\n",
    "class SequenceLoader:\n",
    "    \"\"\"Handles loading protein sequences from FASTA files.\"\"\"\n",
    "    \n",
    "    def __init__(self, fasta_path: Union[str, Path]):\n",
    "        self.fasta_path = Path(fasta_path)\n",
    "        self.sequences = {}\n",
    "        self._load()\n",
    "\n",
    "    def _load(self):\n",
    "        if not self.fasta_path.exists():\n",
    "            raise FileNotFoundError(f\"FASTA file not found: {self.fasta_path}\")\n",
    "        \n",
    "        print(f\"Loading sequences from {self.fasta_path}...\")\n",
    "        for record in SeqIO.parse(self.fasta_path, \"fasta\"):\n",
    "            header = record.id\n",
    "            # Handle different FASTA formats\n",
    "            if \"|\" in header:\n",
    "                parts = header.split(\"|\")\n",
    "                clean_id = parts[1] if len(parts) >= 2 else header\n",
    "            else:\n",
    "                clean_id = header.split()[0]\n",
    "            self.sequences[clean_id] = str(record.seq)\n",
    "        print(f\"Loaded {len(self.sequences)} sequences.\")\n",
    "\n",
    "    def get_sequence(self, protein_id: str) -> Optional[str]:\n",
    "        return self.sequences.get(protein_id)\n",
    "\n",
    "    def get_all_ids(self) -> List[str]:\n",
    "        return list(self.sequences.keys())\n",
    "\n",
    "\n",
    "class LabelLoader:\n",
    "    \"\"\"Handles loading ground truth GO annotations.\"\"\"\n",
    "    \n",
    "    def __init__(self, tsv_path: Union[str, Path], ontology_loader: Optional[OntologyLoader] = None):\n",
    "        self.tsv_path = Path(tsv_path)\n",
    "        self.ontology = ontology_loader\n",
    "        self.df = None\n",
    "        self.protein_to_terms = {}\n",
    "        self._load()\n",
    "\n",
    "    def _load(self):\n",
    "        if not self.tsv_path.exists():\n",
    "            raise FileNotFoundError(f\"Label file not found: {self.tsv_path}\")\n",
    "        \n",
    "        print(f\"Loading labels from {self.tsv_path}...\")\n",
    "        self.df = pd.read_csv(self.tsv_path, sep='\\t')\n",
    "        self.protein_to_terms = self.df.groupby('EntryID')['term'].apply(set).to_dict()\n",
    "        print(f\"Loaded annotations for {len(self.protein_to_terms)} proteins.\")\n",
    "\n",
    "    def get_terms(self, protein_id: str, propagate: bool = False) -> Set[str]:\n",
    "        \"\"\"Get GO terms for a protein, optionally with ancestors\"\"\"\n",
    "        terms = self.protein_to_terms.get(protein_id, set())\n",
    "        \n",
    "        if propagate and self.ontology:\n",
    "            propagated_terms = set(terms)\n",
    "            for term in terms:\n",
    "                propagated_terms.update(self.ontology.get_ancestors(term))\n",
    "            return propagated_terms\n",
    "        return terms\n",
    "\n",
    "\n",
    "print(\"‚úÖ Data loader classes defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3706093d",
   "metadata": {},
   "source": [
    "## 2. Label Propagation Functions\n",
    "\n",
    "Core propagation logic using GO hierarchy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d8b2724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Propagation functions defined\n"
     ]
    }
   ],
   "source": [
    "def propagate_predictions(\n",
    "    predictions: Union[np.ndarray, torch.Tensor],\n",
    "    term_list: List[str],\n",
    "    ontology_loader: OntologyLoader,\n",
    "    strategy: str = 'max'\n",
    ") -> Union[np.ndarray, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Propagate predictions to ancestor terms using GO hierarchy.\n",
    "    \n",
    "    Args:\n",
    "        predictions: (N, K) array of probabilities for K GO terms\n",
    "        term_list: List of K GO term IDs corresponding to columns\n",
    "        ontology_loader: Loaded GO ontology graph\n",
    "        strategy: 'max' (take max of current and children), 'copy', or 'threshold'\n",
    "    \n",
    "    Returns:\n",
    "        Propagated predictions with same shape as input\n",
    "    \"\"\"\n",
    "    # Convert to numpy for processing\n",
    "    is_torch = isinstance(predictions, torch.Tensor)\n",
    "    if is_torch:\n",
    "        device = predictions.device\n",
    "        predictions = predictions.cpu().numpy()\n",
    "    \n",
    "    propagated = predictions.copy()\n",
    "    term_to_idx = {term: idx for idx, term in enumerate(term_list)}\n",
    "    \n",
    "    # For each term, propagate to its ancestors\n",
    "    for child_idx, child_term in enumerate(term_list):\n",
    "        ancestors = ontology_loader.get_ancestors(child_term)\n",
    "        ancestor_indices = [term_to_idx[anc] for anc in ancestors if anc in term_to_idx]\n",
    "        \n",
    "        if not ancestor_indices:\n",
    "            continue\n",
    "        \n",
    "        child_probs = propagated[:, child_idx]\n",
    "        \n",
    "        for anc_idx in ancestor_indices:\n",
    "            if strategy == 'max':\n",
    "                # Ancestor = max(current, child)\n",
    "                propagated[:, anc_idx] = np.maximum(\n",
    "                    propagated[:, anc_idx],\n",
    "                    child_probs\n",
    "                )\n",
    "            elif strategy == 'copy':\n",
    "                # Copy if child higher\n",
    "                mask = propagated[:, anc_idx] < child_probs\n",
    "                propagated[mask, anc_idx] = child_probs[mask]\n",
    "            elif strategy == 'threshold':\n",
    "                # Binary: set to 1 if child predicted\n",
    "                mask = child_probs > 0.5\n",
    "                propagated[mask, anc_idx] = 1.0\n",
    "    \n",
    "    # Convert back to torch if needed\n",
    "    if is_torch:\n",
    "        propagated = torch.from_numpy(propagated).to(device)\n",
    "    \n",
    "    return propagated\n",
    "\n",
    "\n",
    "def get_propagated_terms(\n",
    "    predicted_terms: Set[str],\n",
    "    ontology_loader: OntologyLoader\n",
    ") -> Set[str]:\n",
    "    \"\"\"Expand predicted terms to include all ancestors.\"\"\"\n",
    "    propagated = set(predicted_terms)\n",
    "    for term in predicted_terms:\n",
    "        ancestors = ontology_loader.get_ancestors(term)\n",
    "        propagated.update(ancestors)\n",
    "    return propagated\n",
    "\n",
    "\n",
    "print(\"‚úÖ Propagation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22186c5",
   "metadata": {},
   "source": [
    "## 3. Load GO Ontology\n",
    "\n",
    "**üìÅ Update path to your GO ontology file:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f010b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Base directory: c:\\Users\\Olale\\Documents\\Codebase\\Science\n",
      "\n",
      "Loading GO ontology...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "OBO file not found: c:\\Users\\Olale\\Documents\\Codebase\\Science\\Train\\go-basic.obo",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müìÅ Base directory: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mLoading GO ontology...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m ontology = \u001b[43mOntologyLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_dir\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgo-basic.obo\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTotal GO terms: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(ontology.graph)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNodes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00montology.graph.number_of_nodes()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mOntologyLoader.__init__\u001b[39m\u001b[34m(self, obo_path)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mself\u001b[39m.obo_path = Path(obo_path)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mself\u001b[39m.graph = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mOntologyLoader._load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_load\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.obo_path.exists():\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOBO file not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.obo_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading ontology from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.obo_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m     \u001b[38;5;28mself\u001b[39m.graph = obonet.read_obo(\u001b[38;5;28mself\u001b[39m.obo_path)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: OBO file not found: c:\\Users\\Olale\\Documents\\Codebase\\Science\\Train\\go-basic.obo"
     ]
    }
   ],
   "source": [
    "# Set base directory and data paths\n",
    "if ENVIRONMENT == 'kaggle':\n",
    "    base_dir = Path(\"/kaggle/input/cafa-6-dataset\")\n",
    "else:  # local\n",
    "    base_dir = Path.cwd().parent\n",
    "\n",
    "# Define all data paths\n",
    "TRAIN_SEQ = base_dir / 'Train' / 'train_sequences.fasta'\n",
    "TRAIN_TERMS = base_dir / 'Train' / 'train_terms.tsv'\n",
    "GO_OBO = base_dir / 'Train' / 'go-basic.obo'\n",
    "IA_TSV = base_dir / 'IA.tsv'\n",
    "TEST_FASTA = base_dir / 'Test' / 'testsuperset.fasta'\n",
    "\n",
    "print(f\"üìÅ Base directory: {base_dir}\")\n",
    "print(f\"üìÑ Data files:\")\n",
    "print(f\"  - GO ontology: {GO_OBO.name}\")\n",
    "print(f\"  - Training sequences: {TRAIN_SEQ.name}\")\n",
    "print(f\"  - Training terms: {TRAIN_TERMS.name}\")\n",
    "print(f\"  - IA weights: {IA_TSV.name}\")\n",
    "\n",
    "print(\"\\nLoading GO ontology...\")\n",
    "ontology = OntologyLoader(GO_OBO)\n",
    "\n",
    "print(f\"Total GO terms: {len(ontology.graph)}\")\n",
    "print(f\"Nodes: {ontology.graph.number_of_nodes()}\")\n",
    "print(f\"Edges: {ontology.graph.number_of_edges()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d4fb73",
   "metadata": {},
   "source": [
    "## 4. Test Propagation with Example\n",
    "\n",
    "Verify propagation works: if we predict \"nuclease activity\", ancestors get boosted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704dc90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: nuclease activity\n",
    "nuclease = 'GO:0004518'\n",
    "ancestors = ontology.get_ancestors(nuclease)\n",
    "\n",
    "print(f\"Term: {nuclease}\")\n",
    "if nuclease in ontology.graph:\n",
    "    print(f\"Name: {ontology.graph.nodes[nuclease].get('name', 'N/A')}\")\n",
    "    print(f\"Namespace: {ontology.get_namespace(nuclease)}\")\n",
    "\n",
    "print(f\"\\nNumber of ancestors: {len(ancestors)}\")\n",
    "print(f\"\\nFirst 10 ancestors:\")\n",
    "for anc in list(ancestors)[:10]:\n",
    "    name = ontology.graph.nodes[anc].get('name', 'N/A')\n",
    "    print(f\"  {anc}: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af156db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test array propagation\n",
    "test_terms = [\n",
    "    'GO:0003674',  # molecular_function (root)\n",
    "    'GO:0016787',  # hydrolase activity\n",
    "    'GO:0004518',  # nuclease activity (child of hydrolase)\n",
    "    'GO:0008150',  # biological_process (root)\n",
    "    'GO:0006281',  # DNA repair\n",
    "]\n",
    "\n",
    "# High confidence only for nuclease (0.9)\n",
    "preds_before = np.array([[0.05, 0.10, 0.90, 0.03, 0.15]])\n",
    "\n",
    "print(\"Before propagation:\")\n",
    "for term, prob in zip(test_terms, preds_before[0]):\n",
    "    name = ontology.graph.nodes[term].get('name', 'N/A')\n",
    "    print(f\"  {term} ({name}): {prob:.3f}\")\n",
    "\n",
    "# Apply propagation\n",
    "preds_after = propagate_predictions(preds_before, test_terms, ontology, strategy='max')\n",
    "\n",
    "print(\"\\nAfter propagation:\")\n",
    "for term, prob in zip(test_terms, preds_after[0]):\n",
    "    name = ontology.graph.nodes[term].get('name', 'N/A')\n",
    "    change = '‚úÖ' if prob > preds_before[0][test_terms.index(term)] else ''\n",
    "    print(f\"  {term} ({name}): {prob:.3f} {change}\")\n",
    "\n",
    "print(f\"\\nüéØ Hydrolase boosted: {preds_before[0][1]:.3f} ‚Üí {preds_after[0][1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f472b84",
   "metadata": {},
   "source": [
    "## 5. Load Training Data\n",
    "\n",
    "**üìÅ Update paths to your training data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2c79ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading sequences...\")\n",
    "seq_loader = SequenceLoader(TRAIN_SEQ)\n",
    "\n",
    "print(\"Loading labels...\")\n",
    "label_loader = LabelLoader(TRAIN_TERMS)\n",
    "\n",
    "print(\"Loading IA weights...\")\n",
    "ia_df = pd.read_csv(IA_TSV, sep='\\t')\n",
    "ia_weights = dict(zip(ia_df['term'], ia_df['IA']))\n",
    "\n",
    "print(f\"\\nTotal proteins: {len(seq_loader.sequences)}\")\n",
    "print(f\"Total annotations: {len(label_loader.df)}\")\n",
    "print(f\"IA weights available: {len(ia_weights)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4e475a",
   "metadata": {},
   "source": [
    "## 6. Evaluation Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72de23de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_f1_with_threshold(y_true, y_pred, threshold, ia_weights_dict, term_list):\n",
    "    \"\"\"\n",
    "    Compute weighted F1 score using IA weights.\n",
    "    \n",
    "    Args:\n",
    "        y_true: (N, K) binary labels\n",
    "        y_pred: (N, K) probabilities\n",
    "        threshold: float, prediction threshold\n",
    "        ia_weights_dict: dict mapping GO terms to IA weights\n",
    "        term_list: list of GO term IDs\n",
    "    \"\"\"\n",
    "    # Threshold predictions\n",
    "    y_pred_binary = (y_pred >= threshold).astype(int)\n",
    "    \n",
    "    # Get IA weights for vocabulary\n",
    "    weights = np.array([ia_weights_dict.get(term, 1.0) for term in term_list])\n",
    "    \n",
    "    # Compute per-sample F1\n",
    "    f1_scores = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    \n",
    "    for i in range(len(y_true)):\n",
    "        true_pos = (y_true[i] == 1) & (y_pred_binary[i] == 1)\n",
    "        pred_pos = (y_pred_binary[i] == 1)\n",
    "        actual_pos = (y_true[i] == 1)\n",
    "        \n",
    "        # Weighted counts\n",
    "        tp = (true_pos * weights).sum()\n",
    "        fp = ((pred_pos & ~true_pos) * weights).sum()\n",
    "        fn = ((actual_pos & ~true_pos) * weights).sum()\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        f1_scores.append(f1)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "    \n",
    "    return {\n",
    "        'f1': np.mean(f1_scores),\n",
    "        'precision': np.mean(precisions),\n",
    "        'recall': np.mean(recalls)\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Evaluation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c163c57",
   "metadata": {},
   "source": [
    "## 7. Load Your Model and Generate Predictions\n",
    "\n",
    "**‚ö†Ô∏è CRITICAL: Replace this section with your actual model loading and prediction code.**\n",
    "\n",
    "This is a placeholder. You need to:\n",
    "1. Load your trained model (ESM-2, CNN, etc.)\n",
    "2. Create your validation dataset\n",
    "3. Generate predictions in shape (N_samples, N_terms)\n",
    "4. Generate true labels in same shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf8a1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EsmModel, AutoConfig, AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from collections import Counter\n",
    "\n",
    "class ESMForGOPrediction(torch.nn.Module):\n",
    "    \"\"\"ESM-2 model with classification head for GO term prediction.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"facebook/esm2_t6_8M_UR50D\", \n",
    "                 num_labels: int = 5000, dropout: float = 0.3):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.num_labels = num_labels\n",
    "        \n",
    "        # Load pre-trained ESM-2\n",
    "        self.esm = EsmModel.from_pretrained(model_name)\n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "        self.hidden_dim = config.hidden_size\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(self.hidden_dim, num_labels)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass with mean pooling.\"\"\"\n",
    "        outputs = self.esm(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        \n",
    "        # Mean pooling (ignore padding)\n",
    "        mask_expanded = attention_mask.unsqueeze(-1).expand(sequence_output.size()).float()\n",
    "        sum_embeddings = torch.sum(sequence_output * mask_expanded, dim=1)\n",
    "        sum_mask = torch.clamp(mask_expanded.sum(dim=1), min=1e-9)\n",
    "        pooled_output = sum_embeddings / sum_mask\n",
    "        \n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n",
    "    \n",
    "    @classmethod\n",
    "    def from_pretrained(cls, load_directory: str):\n",
    "        \"\"\"Load model from directory.\"\"\"\n",
    "        import json\n",
    "        load_path = Path(load_directory)\n",
    "        \n",
    "        with open(load_path / \"config.json\", \"r\") as f:\n",
    "            config = json.load(f)\n",
    "        \n",
    "        model = cls(model_name=config[\"model_name\"], num_labels=config[\"num_labels\"])\n",
    "        state_dict = torch.load(load_path / \"pytorch_model.bin\", map_location=\"cpu\")\n",
    "        model.load_state_dict(state_dict)\n",
    "        \n",
    "        return model\n",
    "\n",
    "\n",
    "class FineTuneDataset(Dataset):\n",
    "    \"\"\"Dataset for ESM-2 fine-tuning.\"\"\"\n",
    "    \n",
    "    def __init__(self, sequences: dict, labels: dict, vocab: list, \n",
    "                 model_name: str = \"facebook/esm2_t6_8M_UR50D\", max_length: int = 512):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.term_to_idx = {term: idx for idx, term in enumerate(vocab)}\n",
    "        self.num_classes = len(vocab)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        self.protein_ids = [pid for pid in sequences.keys() \n",
    "                           if pid in labels and len(labels[pid]) > 0]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.protein_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        protein_id = self.protein_ids[idx]\n",
    "        sequence = self.sequences[protein_id]\n",
    "        go_terms = self.labels[protein_id]\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = self.tokenizer(sequence, max_length=self.max_length, \n",
    "                               padding='max_length', truncation=True, \n",
    "                               return_tensors='pt')\n",
    "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "        \n",
    "        # Create label vector\n",
    "        label_vector = torch.zeros(self.num_classes, dtype=torch.float32)\n",
    "        for term in go_terms:\n",
    "            if term in self.term_to_idx:\n",
    "                label_vector[self.term_to_idx[term]] = 1.0\n",
    "        \n",
    "        return {'input_ids': inputs['input_ids'], \n",
    "                'attention_mask': inputs['attention_mask'],\n",
    "                'labels': label_vector}\n",
    "\n",
    "\n",
    "print(\"‚úÖ Model and dataset classes defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf6f29a",
   "metadata": {},
   "source": [
    "## 8. Build Vocabulary and Create Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d03de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PLACEHOLDER - REPLACE WITH YOUR MODEL CODE\n",
    "# ============================================\n",
    "\n",
    "# Example structure (uncomment and adapt to your model):\n",
    "\n",
    "# from transformers import AutoModel, AutoTokenizer\n",
    "# import torch.nn as nn\n",
    "\n",
    "# # 1. Load your model\n",
    "# model_path = base_dir / \"models\" / \"best_model\"\n",
    "# model = YourModelClass.from_pretrained(model_path)\n",
    "# model.eval()\n",
    "\n",
    "# # 2. Create validation dataset\n",
    "# # ... your dataset code ...\n",
    "\n",
    "# # 3. Generate predictions\n",
    "# all_preds = []  # Shape: (N_samples, N_terms)\n",
    "# all_labels = []  # Shape: (N_samples, N_terms)\n",
    "# term_list = [...]  # List of GO term IDs corresponding to columns\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for batch in dataloader:\n",
    "#         outputs = model(batch)\n",
    "#         probs = torch.sigmoid(outputs.logits)\n",
    "#         all_preds.append(probs.cpu().numpy())\n",
    "#         all_labels.append(batch['labels'].cpu().numpy())\n",
    "\n",
    "# all_preds = np.vstack(all_preds)\n",
    "# all_labels = np.vstack(all_labels)\n",
    "\n",
    "# ============================================\n",
    "# FOR DEMO: Create dummy predictions\n",
    "# ============================================\n",
    "print(\"‚ö†Ô∏è Using DUMMY predictions for demonstration\")\n",
    "print(\"Replace this with your actual model predictions!\\n\")\n",
    "\n",
    "# Dummy data (500 samples, 100 terms)\n",
    "n_samples = 500\n",
    "n_terms = 100\n",
    "term_list = [f\"GO:{str(i).zfill(7)}\" for i in range(3674, 3674 + n_terms)]\n",
    "\n",
    "# Random predictions and labels\n",
    "np.random.seed(42)\n",
    "all_preds = np.random.rand(n_samples, n_terms) * 0.3  # Low confidence predictions\n",
    "all_labels = (np.random.rand(n_samples, n_terms) > 0.95).astype(int)  # Sparse labels\n",
    "\n",
    "print(f\"Predictions shape: {all_preds.shape}\")\n",
    "print(f\"Labels shape: {all_labels.shape}\")\n",
    "print(f\"Vocabulary size: {len(term_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ea95fa",
   "metadata": {},
   "source": [
    "## 9. Load Fine-Tuned Model and Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6316011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model_path = base_dir / \"models\" / \"esm_finetuned\" / \"best_model\"\n",
    "\n",
    "print(f\"Loading model from {model_path}...\")\n",
    "\n",
    "if not model_path.exists():\n",
    "    print(\"‚ùå Model not found!\")\n",
    "    print(\"‚ö†Ô∏è Using DUMMY predictions for demonstration\\n\")\n",
    "    \n",
    "    # Dummy predictions\n",
    "    np.random.seed(42)\n",
    "    n_samples = len(val_dataset)\n",
    "    n_terms = len(term_list)\n",
    "    all_preds = np.random.rand(n_samples, n_terms) * 0.3\n",
    "    all_labels = np.zeros((n_samples, n_terms))\n",
    "    for i in range(n_samples):\n",
    "        sample = val_dataset[i]\n",
    "        all_labels[i] = sample['labels'].numpy()\n",
    "    \n",
    "    print(f\"Predictions shape: {all_preds.shape}\")\n",
    "    print(f\"Labels shape: {all_labels.shape}\")\n",
    "else:\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Device: {device}\")\n",
    "    \n",
    "    model = ESMForGOPrediction.from_pretrained(str(model_path))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"‚úÖ Model loaded\")\n",
    "    print(f\"Output dimension: {model.num_labels}\")\n",
    "    \n",
    "    # Generate predictions\n",
    "    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "    \n",
    "    print(\"\\nGenerating predictions...\")\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Inference\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].cpu().numpy()\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "            \n",
    "            all_preds.append(probs)\n",
    "            all_labels.append(labels)\n",
    "    \n",
    "    all_preds = np.vstack(all_preds)\n",
    "    all_labels = np.vstack(all_labels)\n",
    "    \n",
    "    print(f\"\\nPredictions shape: {all_preds.shape}\")\n",
    "    print(f\"Labels shape: {all_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d486d998",
   "metadata": {},
   "source": [
    "## 8. Evaluate WITHOUT Propagation (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a775c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating WITHOUT propagation...\")\n",
    "print(\"Testing thresholds...\\n\")\n",
    "\n",
    "thresholds = [0.01, 0.05, 0.10, 0.15, 0.20, 0.30, 0.40, 0.50]\n",
    "results_baseline = []\n",
    "\n",
    "for thr in thresholds:\n",
    "    metrics = compute_f1_with_threshold(\n",
    "        all_labels, \n",
    "        all_preds, \n",
    "        thr, \n",
    "        ia_weights, \n",
    "        term_list\n",
    "    )\n",
    "    results_baseline.append({\n",
    "        'threshold': thr,\n",
    "        'f1': metrics['f1'],\n",
    "        'precision': metrics['precision'],\n",
    "        'recall': metrics['recall']\n",
    "    })\n",
    "    print(f\"Thr={thr:.2f}: F1={metrics['f1']:.4f}, P={metrics['precision']:.4f}, R={metrics['recall']:.4f}\")\n",
    "\n",
    "best_baseline = max(results_baseline, key=lambda x: x['f1'])\n",
    "print(f\"\\nüèÜ Best WITHOUT propagation:\")\n",
    "print(f\"   F1 = {best_baseline['f1']:.4f} at threshold {best_baseline['threshold']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafc48e5",
   "metadata": {},
   "source": [
    "## 9. Apply Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5793630",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Applying label propagation...\")\n",
    "\n",
    "# Convert to torch and propagate\n",
    "all_preds_torch = torch.from_numpy(all_preds)\n",
    "all_preds_propagated = propagate_predictions(\n",
    "    all_preds_torch,\n",
    "    term_list,\n",
    "    ontology,\n",
    "    strategy='max'\n",
    ")\n",
    "all_preds_propagated = all_preds_propagated.numpy()\n",
    "\n",
    "print(f\"‚úÖ Propagation complete\")\n",
    "print(f\"Shape unchanged: {all_preds_propagated.shape}\")\n",
    "\n",
    "# Check impact\n",
    "increased = (all_preds_propagated > all_preds).sum()\n",
    "total = all_preds.size\n",
    "print(f\"Probabilities boosted: {increased:,} / {total:,} ({100*increased/total:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b06195d",
   "metadata": {},
   "source": [
    "## 10. Evaluate WITH Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e207743",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating WITH propagation...\")\n",
    "print(\"Testing thresholds...\\n\")\n",
    "\n",
    "results_propagated = []\n",
    "\n",
    "for thr in thresholds:\n",
    "    metrics = compute_f1_with_threshold(\n",
    "        all_labels, \n",
    "        all_preds_propagated, \n",
    "        thr, \n",
    "        ia_weights, \n",
    "        term_list\n",
    "    )\n",
    "    results_propagated.append({\n",
    "        'threshold': thr,\n",
    "        'f1': metrics['f1'],\n",
    "        'precision': metrics['precision'],\n",
    "        'recall': metrics['recall']\n",
    "    })\n",
    "    print(f\"Thr={thr:.2f}: F1={metrics['f1']:.4f}, P={metrics['precision']:.4f}, R={metrics['recall']:.4f}\")\n",
    "\n",
    "best_propagated = max(results_propagated, key=lambda x: x['f1'])\n",
    "print(f\"\\nüèÜ Best WITH propagation:\")\n",
    "print(f\"   F1 = {best_propagated['f1']:.4f} at threshold {best_propagated['threshold']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79333f5",
   "metadata": {},
   "source": [
    "## 11. Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0349bb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"üìä COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nBaseline (no propagation):\")\n",
    "print(f\"  F1:        {best_baseline['f1']:.4f}\")\n",
    "print(f\"  Precision: {best_baseline['precision']:.4f}\")\n",
    "print(f\"  Recall:    {best_baseline['recall']:.4f}\")\n",
    "print(f\"  Threshold: {best_baseline['threshold']}\")\n",
    "\n",
    "print(f\"\\nWith Propagation:\")\n",
    "print(f\"  F1:        {best_propagated['f1']:.4f}\")\n",
    "print(f\"  Precision: {best_propagated['precision']:.4f}\")\n",
    "print(f\"  Recall:    {best_propagated['recall']:.4f}\")\n",
    "print(f\"  Threshold: {best_propagated['threshold']}\")\n",
    "\n",
    "improvement = best_propagated['f1'] - best_baseline['f1']\n",
    "pct_improvement = 100 * improvement / best_baseline['f1'] if best_baseline['f1'] > 0 else 0\n",
    "\n",
    "print(f\"\\n{'üéâ' if improvement > 0 else '‚ö†Ô∏è'} Improvement:\")\n",
    "print(f\"  Œî F1:      {improvement:+.4f} ({pct_improvement:+.2f}%)\")\n",
    "\n",
    "if best_propagated['f1'] >= 0.25:\n",
    "    print(f\"\\n‚úÖ TARGET REACHED! F1 ‚â• 0.25\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Target not reached (goal: 0.25, got: {best_propagated['f1']:.4f})\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2788d6dd",
   "metadata": {},
   "source": [
    "## 12. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe9ebbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: F1 curves\n",
    "ax1 = axes[0]\n",
    "baseline_f1s = [r['f1'] for r in results_baseline]\n",
    "propagated_f1s = [r['f1'] for r in results_propagated]\n",
    "\n",
    "ax1.plot(thresholds, baseline_f1s, 'o-', label='Without Propagation', linewidth=2)\n",
    "ax1.plot(thresholds, propagated_f1s, 's-', label='With Propagation', linewidth=2)\n",
    "ax1.axhline(y=0.25, color='red', linestyle='--', alpha=0.5, label='Target (0.25)')\n",
    "ax1.set_xlabel('Threshold', fontsize=12)\n",
    "ax1.set_ylabel('F1 Score', fontsize=12)\n",
    "ax1.set_title('F1 Score vs Threshold', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Right: Precision-Recall\n",
    "ax2 = axes[1]\n",
    "baseline_prec = [r['precision'] for r in results_baseline]\n",
    "baseline_rec = [r['recall'] for r in results_baseline]\n",
    "propagated_prec = [r['precision'] for r in results_propagated]\n",
    "propagated_rec = [r['recall'] for r in results_propagated]\n",
    "\n",
    "ax2.plot(baseline_rec, baseline_prec, 'o-', label='Without Propagation', linewidth=2)\n",
    "ax2.plot(propagated_rec, propagated_prec, 's-', label='With Propagation', linewidth=2)\n",
    "ax2.set_xlabel('Recall', fontsize=12)\n",
    "ax2.set_ylabel('Precision', fontsize=12)\n",
    "ax2.set_title('Precision-Recall Curve', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìà Plots generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d040ecf5",
   "metadata": {},
   "source": [
    "## 13. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f980fb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comparison to CSV\n",
    "results_df = pd.DataFrame({\n",
    "    'threshold': thresholds,\n",
    "    'f1_baseline': [r['f1'] for r in results_baseline],\n",
    "    'f1_propagated': [r['f1'] for r in results_propagated],\n",
    "    'precision_baseline': [r['precision'] for r in results_baseline],\n",
    "    'precision_propagated': [r['precision'] for r in results_propagated],\n",
    "    'recall_baseline': [r['recall'] for r in results_baseline],\n",
    "    'recall_propagated': [r['recall'] for r in results_propagated],\n",
    "})\n",
    "\n",
    "output_path = Path(\"propagation_comparison.csv\")\n",
    "results_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"‚úÖ Results saved to {output_path}\")\n",
    "print(\"\\nüìä Results preview:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8861454",
   "metadata": {},
   "source": [
    "## 14. Summary\n",
    "\n",
    "**What we did:**\n",
    "1. ‚úÖ Implemented label propagation using GO hierarchy\n",
    "2. ‚úÖ Evaluated with/without propagation\n",
    "3. ‚úÖ Measured F1 improvement\n",
    "\n",
    "**Key takeaway:** Propagation ensures ontological consistency ‚Äî if you predict a specific term, all its parents should also be predicted. This typically boosts F1 by 2-4%.\n",
    "\n",
    "**Next steps:**\n",
    "- Per-aspect thresholds (MF/BP/CC separate optimization)\n",
    "- Simple ensemble (KNN + ESM)\n",
    "- Larger backbone (ESM-2 35M or 150M)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
