{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e43e23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment configuration\n",
    "ENVIRONMENT = 'local'  # Change to 'kaggle' when running on Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a934104c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas numpy scikit-learn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4e7488",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"‚úÖ Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd3dd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set base directory\n",
    "if ENVIRONMENT == 'kaggle':\n",
    "    base_dir = Path(\"/kaggle/input/cafa-6-dataset\")\n",
    "else:\n",
    "    base_dir = Path.cwd().parent\n",
    "\n",
    "print(f\"üìÅ Base directory: {base_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349e1f22",
   "metadata": {},
   "source": [
    "## 1. Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77e404b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load annotations\n",
    "print(\"Loading training annotations...\")\n",
    "train_terms = pd.read_csv(base_dir / \"Train\" / \"train_terms.tsv\", sep='\\t')\n",
    "\n",
    "print(f\"Total annotations: {len(train_terms)}\")\n",
    "print(f\"Unique proteins: {train_terms['EntryID'].nunique()}\")\n",
    "print(f\"Unique GO terms: {train_terms['term'].nunique()}\")\n",
    "\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(train_terms.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57245c32",
   "metadata": {},
   "source": [
    "## 2. Load IA Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265765bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Information Accretion weights\n",
    "print(\"Loading IA weights...\")\n",
    "ia_df = pd.read_csv(base_dir / \"IA.tsv\", sep='\\t')\n",
    "ia_weights = dict(zip(ia_df['term'], ia_df['IA']))\n",
    "\n",
    "print(f\"IA weights available: {len(ia_weights)}\")\n",
    "print(f\"\\nExample weights:\")\n",
    "for term in list(ia_weights.keys())[:5]:\n",
    "    print(f\"  {term}: {ia_weights[term]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e08609",
   "metadata": {},
   "source": [
    "## 3. Build Frequency Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb232b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count term frequencies\n",
    "print(\"Counting GO term frequencies...\")\n",
    "term_counts = Counter(train_terms['term'])\n",
    "\n",
    "# Convert to probabilities\n",
    "total_annotations = len(train_terms)\n",
    "term_probs = {term: count / total_annotations for term, count in term_counts.items()}\n",
    "\n",
    "print(f\"Total unique terms: {len(term_probs)}\")\n",
    "print(f\"\\nTop 10 most frequent terms:\")\n",
    "for term, count in term_counts.most_common(10):\n",
    "    prob = term_probs[term]\n",
    "    print(f\"  {term}: {count} ({prob:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298ea2d5",
   "metadata": {},
   "source": [
    "## 4. Create Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2100e38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split proteins into train/val\n",
    "all_proteins = train_terms['EntryID'].unique()\n",
    "train_proteins, val_proteins = train_test_split(\n",
    "    all_proteins, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train proteins: {len(train_proteins)}\")\n",
    "print(f\"Val proteins: {len(val_proteins)}\")\n",
    "\n",
    "# Get validation annotations\n",
    "val_data = train_terms[train_terms['EntryID'].isin(val_proteins)]\n",
    "print(f\"\\nValidation annotations: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24caf8f",
   "metadata": {},
   "source": [
    "## 5. Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683a5808",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions(predictions_df, ground_truth_df, ia_weights_dict, threshold=0.01):\n",
    "    \"\"\"\n",
    "    Evaluate predictions against ground truth.\n",
    "    \n",
    "    Args:\n",
    "        predictions_df: DataFrame with columns [EntryID, term, probability]\n",
    "        ground_truth_df: DataFrame with columns [EntryID, term]\n",
    "        ia_weights_dict: Dict mapping terms to IA weights\n",
    "        threshold: Probability threshold for predictions\n",
    "    \"\"\"\n",
    "    # Filter predictions by threshold\n",
    "    pred_filtered = predictions_df[predictions_df['probability'] >= threshold]\n",
    "    \n",
    "    # Group by protein\n",
    "    pred_grouped = pred_filtered.groupby('EntryID')['term'].apply(set).to_dict()\n",
    "    true_grouped = ground_truth_df.groupby('EntryID')['term'].apply(set).to_dict()\n",
    "    \n",
    "    # Compute per-protein metrics\n",
    "    f1_scores = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    \n",
    "    for protein in true_grouped.keys():\n",
    "        true_terms = true_grouped[protein]\n",
    "        pred_terms = pred_grouped.get(protein, set())\n",
    "        \n",
    "        if len(pred_terms) == 0:\n",
    "            f1_scores.append(0.0)\n",
    "            precisions.append(0.0)\n",
    "            recalls.append(0.0)\n",
    "            continue\n",
    "        \n",
    "        # Compute weighted metrics\n",
    "        tp_weight = sum(ia_weights_dict.get(t, 1.0) for t in true_terms & pred_terms)\n",
    "        fp_weight = sum(ia_weights_dict.get(t, 1.0) for t in pred_terms - true_terms)\n",
    "        fn_weight = sum(ia_weights_dict.get(t, 1.0) for t in true_terms - pred_terms)\n",
    "        \n",
    "        precision = tp_weight / (tp_weight + fp_weight) if (tp_weight + fp_weight) > 0 else 0\n",
    "        recall = tp_weight / (tp_weight + fn_weight) if (tp_weight + fn_weight) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        f1_scores.append(f1)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "    \n",
    "    return {\n",
    "        'f1': np.mean(f1_scores),\n",
    "        'precision': np.mean(precisions),\n",
    "        'recall': np.mean(recalls),\n",
    "        'coverage': len([p for p in pred_grouped if len(pred_grouped[p]) > 0]) / len(true_grouped)\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Evaluation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4fd293",
   "metadata": {},
   "source": [
    "## 6. Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54011678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each validation protein, assign all terms with their frequencies\n",
    "print(\"Generating frequency-based predictions...\")\n",
    "\n",
    "predictions = []\n",
    "for protein in val_proteins:\n",
    "    for term, prob in term_probs.items():\n",
    "        predictions.append({\n",
    "            'EntryID': protein,\n",
    "            'term': term,\n",
    "            'probability': prob\n",
    "        })\n",
    "\n",
    "predictions_df = pd.DataFrame(predictions)\n",
    "print(f\"\\nTotal predictions: {len(predictions_df)}\")\n",
    "print(f\"Predictions per protein: {len(predictions_df) / len(val_proteins):.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1998f053",
   "metadata": {},
   "source": [
    "## 7. Evaluate with Different Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b74d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing different thresholds...\\n\")\n",
    "\n",
    "thresholds = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.02, 0.05]\n",
    "results = []\n",
    "\n",
    "for thr in thresholds:\n",
    "    metrics = evaluate_predictions(predictions_df, val_data, ia_weights, threshold=thr)\n",
    "    results.append({\n",
    "        'threshold': thr,\n",
    "        **metrics\n",
    "    })\n",
    "    print(f\"Threshold {thr:.4f}: F1={metrics['f1']:.4f}, P={metrics['precision']:.4f}, \"\n",
    "          f\"R={metrics['recall']:.4f}, Coverage={metrics['coverage']:.2%}\")\n",
    "\n",
    "# Find best threshold\n",
    "best_result = max(results, key=lambda x: x['f1'])\n",
    "print(f\"\\nüèÜ Best F1: {best_result['f1']:.4f} at threshold {best_result['threshold']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3d4f6a",
   "metadata": {},
   "source": [
    "## 8. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55ce714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "results_df = pd.DataFrame(results)\n",
    "output_path = Path(\"01_frequency_baseline_results.csv\")\n",
    "results_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"‚úÖ Results saved to {output_path}\")\n",
    "print(\"\\nüìä Results:\")\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df666236",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Frequency Baseline Performance:**\n",
    "- Simple approach: predict terms by training frequency\n",
    "- No protein-specific information used\n",
    "- Expected F1: ~0.14 (from your previous experiments)\n",
    "\n",
    "**Next:** 02_baseline_knn.ipynb - Add sequence similarity"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
