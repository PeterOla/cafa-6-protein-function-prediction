{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72fb337a",
   "metadata": {},
   "source": [
    "# 05 CAFA E2E — Run DNN for CC (wrapper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a53847e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 01 - Setup (NO REPO)\n",
    "import os\n",
    "import sys\n",
    "import ctypes\n",
    "from pathlib import Path\n",
    "\n",
    "# CUDA loader fix (PyTorch/RAPIDS coexistence): preload venv nvjitlink so we don't pick /usr/local/cuda/lib64\n",
    "try:\n",
    "    _venv_root = Path(sys.executable).resolve().parent.parent\n",
    "    _nvjit_dir = (\n",
    "        _venv_root\n",
    "        / \"lib\"\n",
    "        / f\"python{sys.version_info.major}.{sys.version_info.minor}\"\n",
    "        / \"site-packages\"\n",
    "        / \"nvidia\"\n",
    "        / \"nvjitlink\"\n",
    "        / \"lib\"\n",
    "    )\n",
    "    _nvjit_so = _nvjit_dir / \"libnvJitLink.so.12\"\n",
    "    if _nvjit_so.exists():\n",
    "        ctypes.CDLL(str(_nvjit_so), mode=ctypes.RTLD_GLOBAL)\n",
    "        os.environ[\"LD_LIBRARY_PATH\"] = f\"{_nvjit_dir}:{os.environ.get('LD_LIBRARY_PATH','')}\"\n",
    "        print(f\"[ENV] Preloaded nvjitlink: {_nvjit_so}\")\n",
    "except Exception as _e:\n",
    "    print(f\"[ENV] nvjitlink preload skipped: {_e}\")\n",
    "\n",
    "# Always run from a simple writable location; never cd into a repo.\n",
    "if os.path.exists('/content'):\n",
    "    os.chdir('/content')\n",
    "RUNTIME_ROOT = Path.cwd()\n",
    "DATA_ROOT = (RUNTIME_ROOT / 'cafa6_data')\n",
    "DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "TRAIN_LEVEL1 = True\n",
    "print(f'CWD: {Path.cwd()}')\n",
    "print(f'DATA_ROOT: {DATA_ROOT.resolve()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d917eb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 13.1 - Calibrate per-aspect thresholds (BP/MF/CC) from OOF (read-only)\n",
    "# ========================================================================\n",
    "# Goal: derive thresholds per GO aspect and persist to features/aspect_thresholds.json.\n",
    "# These thresholds are used later in Phase 3/4 (stacking + submission filtering).\n",
    "#\n",
    "# We fit thresholds on a deterministic subsample of rows to keep runtime bounded.\n",
    "# This is *calibration*, not training.\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "WORK_ROOT = Path(WORK_ROOT)\n",
    "FEAT_DIR = WORK_ROOT / 'features'\n",
    "PRED_DIR = FEAT_DIR / 'level1_preds'\n",
    "FEAT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PRED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "thr_path = FEAT_DIR / 'aspect_thresholds.json'\n",
    "thr_meta_path = FEAT_DIR / 'aspect_thresholds_meta.json'\n",
    "\n",
    "if thr_path.exists():\n",
    "    thr_map = json.loads(thr_path.read_text(encoding='utf-8'))\n",
    "    print('Loaded existing aspect thresholds:', thr_map)\n",
    "else:\n",
    "    # ---- term contract ----\n",
    "    top_terms_path = FEAT_DIR / 'top_terms_13500.json'\n",
    "    if not top_terms_path.exists():\n",
    "        raise FileNotFoundError(f'Missing {top_terms_path}. Run Phase 2 setup first.')\n",
    "    top_terms = [str(t) for t in json.loads(top_terms_path.read_text(encoding='utf-8'))]\n",
    "    if len(top_terms) != 13500:\n",
    "        raise RuntimeError(f'Expected top_terms=13500, got {len(top_terms)}')\n",
    "\n",
    "    # ---- GO namespaces -> aspects ----\n",
    "    if 'go_namespaces' in globals() and isinstance(globals()['go_namespaces'], dict):\n",
    "        go_namespaces = globals()['go_namespaces']\n",
    "    elif 'term_to_ns' in globals() and isinstance(globals()['term_to_ns'], dict):\n",
    "        go_namespaces = globals()['term_to_ns']\n",
    "    else:\n",
    "        import obonet\n",
    "        obo_path = None\n",
    "        for p in [WORK_ROOT / 'Train' / 'go-basic.obo', WORK_ROOT / 'go-basic.obo', Path('Train/go-basic.obo'), Path('go-basic.obo')]:\n",
    "            if p.exists():\n",
    "                obo_path = p\n",
    "                break\n",
    "        if obo_path is None:\n",
    "            raise FileNotFoundError('go-basic.obo not found for namespace mapping')\n",
    "        graph = obonet.read_obo(obo_path)\n",
    "        go_namespaces = {node: data.get('namespace', 'unknown') for node, data in graph.nodes(data=True)}\n",
    "\n",
    "    ns_to_aspect = {\n",
    "        'biological_process': 'BP',\n",
    "        'molecular_function': 'MF',\n",
    "        'cellular_component': 'CC',\n",
    "    }\n",
    "    aspects = np.array([ns_to_aspect.get(go_namespaces.get(t, 'unknown'), 'UNK') for t in top_terms], dtype='<U3')\n",
    "\n",
    "    # ---- IA weights ----\n",
    "    ia_path = WORK_ROOT / 'IA.tsv'\n",
    "    if not ia_path.exists():\n",
    "        raise FileNotFoundError(f'Missing IA.tsv at {ia_path}')\n",
    "    ia_df = pd.read_csv(ia_path, sep='\\t')\n",
    "    cols = [c.lower() for c in ia_df.columns]\n",
    "    term_col = ia_df.columns[cols.index('term')] if 'term' in cols else ia_df.columns[0]\n",
    "    if 'ia' in cols:\n",
    "        ia_col = ia_df.columns[cols.index('ia')]\n",
    "    elif 'information_accretion' in cols:\n",
    "        ia_col = ia_df.columns[cols.index('information_accretion')]\n",
    "    else:\n",
    "        ia_col = ia_df.columns[1] if len(ia_df.columns) > 1 else ia_df.columns[0]\n",
    "    ia_map = dict(zip(ia_df[term_col].astype(str), ia_df[ia_col].astype(np.float32)))\n",
    "    weights = np.asarray([ia_map.get(t, np.float32(1.0)) for t in top_terms], dtype=np.float32)\n",
    "\n",
    "    # ---- load Y + OOF predictions ----\n",
    "    if 'Y' in globals():\n",
    "        Y_full = np.asarray(globals()['Y'], dtype=np.float32)\n",
    "    else:\n",
    "        train_terms = pd.read_parquet(WORK_ROOT / 'parsed' / 'train_terms.parquet')\n",
    "        train_ids_raw = pd.read_feather(WORK_ROOT / 'parsed' / 'train_seq.feather')['id'].astype(str)\n",
    "        train_ids = train_ids_raw.str.extract(r\"\\|(.*?)\\|\", expand=False).fillna(train_ids_raw)\n",
    "        train_terms_top = train_terms[train_terms['term'].isin(top_terms)]\n",
    "        y_df = train_terms_top.pivot_table(index='EntryID', columns='term', aggfunc='size', fill_value=0)\n",
    "        y_df = y_df.reindex(train_ids.tolist(), fill_value=0)\n",
    "        y_df = y_df.reindex(columns=top_terms, fill_value=0)\n",
    "        Y_full = y_df.values.astype(np.float32)\n",
    "        del train_terms, train_ids_raw, train_ids, train_terms_top, y_df\n",
    "\n",
    "    def _load_oof(name: str) -> np.ndarray | None:\n",
    "        for p in [PRED_DIR / name, FEAT_DIR / name]:\n",
    "            if p.exists():\n",
    "                return np.load(p)\n",
    "        return None\n",
    "\n",
    "    oof_candidates = {\n",
    "        'logreg': _load_oof('oof_pred_logreg.npy'),\n",
    "        'gbdt': _load_oof('oof_pred_gbdt.npy'),\n",
    "        'dnn': _load_oof('oof_pred_dnn.npy'),\n",
    "        'knn': _load_oof('oof_pred_knn.npy'),\n",
    "    }\n",
    "    oof_candidates = {k: v for k, v in oof_candidates.items() if v is not None}\n",
    "    if not oof_candidates:\n",
    "        raise FileNotFoundError('No OOF preds found. Expected features/level1_preds/oof_pred_*.npy')\n",
    "\n",
    "    for k, v in oof_candidates.items():\n",
    "        if v.shape != Y_full.shape:\n",
    "            raise RuntimeError(f'OOF shape mismatch for {k}: got {v.shape}, expected {Y_full.shape}')\n",
    "\n",
    "    oof_mean = np.mean(np.stack(list(oof_candidates.values()), axis=0), axis=0).astype(np.float32)\n",
    "    print('Threshold calibration base = mean OOF of:', sorted(oof_candidates.keys()))\n",
    "\n",
    "    # deterministic subsample\n",
    "    DIAG_N = int(globals().get('CAFA_DIAG_N', 20000)) if 'CAFA_DIAG_N' in globals() else 20000\n",
    "    n = int(Y_full.shape[0])\n",
    "    m = min(n, int(DIAG_N))\n",
    "    idx = np.linspace(0, n - 1, num=m, dtype=np.int64)\n",
    "    Y_sub = (Y_full[idx] > 0).astype(np.uint8)\n",
    "    S_sub = oof_mean[idx].astype(np.float32, copy=False)\n",
    "\n",
    "    THRS = np.linspace(0.05, 0.60, 23, dtype=np.float32)\n",
    "    COL_CHUNK = 512\n",
    "\n",
    "    def _ia_f1_for_cols(cols: np.ndarray, thr: float) -> float:\n",
    "        w_tp = 0.0\n",
    "        w_pred = 0.0\n",
    "        w_true = 0.0\n",
    "        for c0 in range(0, int(cols.shape[0]), COL_CHUNK):\n",
    "            c = cols[c0 : c0 + COL_CHUNK]\n",
    "            yt = Y_sub[:, c].astype(bool, copy=False)\n",
    "            yp = (S_sub[:, c] >= float(thr))\n",
    "            tp = (yt & yp).sum(axis=0).astype(np.float64)\n",
    "            pred = yp.sum(axis=0).astype(np.float64)\n",
    "            true = yt.sum(axis=0).astype(np.float64)\n",
    "            w = weights[c].astype(np.float64)\n",
    "            w_tp += float((w * tp).sum())\n",
    "            w_pred += float((w * pred).sum())\n",
    "            w_true += float((w * true).sum())\n",
    "        p = (w_tp / w_pred) if w_pred > 0 else 0.0\n",
    "        r = (w_tp / w_true) if w_true > 0 else 0.0\n",
    "        return (2 * p * r / (p + r)) if (p + r) > 0 else 0.0\n",
    "\n",
    "    def _best_thr(cols: np.ndarray) -> tuple[float, float]:\n",
    "        best_t = float(THRS[0])\n",
    "        best_s = -1.0\n",
    "        for t in THRS:\n",
    "            s = _ia_f1_for_cols(cols, float(t))\n",
    "            if s > best_s:\n",
    "                best_s = float(s)\n",
    "                best_t = float(t)\n",
    "        return best_t, best_s\n",
    "\n",
    "    cols_all = np.arange(len(top_terms), dtype=np.int64)\n",
    "    cols_bp = np.where(aspects == 'BP')[0].astype(np.int64)\n",
    "    cols_mf = np.where(aspects == 'MF')[0].astype(np.int64)\n",
    "    cols_cc = np.where(aspects == 'CC')[0].astype(np.int64)\n",
    "    cols_unk = np.where(aspects == 'UNK')[0].astype(np.int64)\n",
    "\n",
    "    thr_all, s_all = _best_thr(cols_all)\n",
    "    thr_bp, s_bp = _best_thr(cols_bp)\n",
    "    thr_mf, s_mf = _best_thr(cols_mf)\n",
    "    thr_cc, s_cc = _best_thr(cols_cc)\n",
    "    thr_unk, s_unk = _best_thr(cols_unk) if int(cols_unk.shape[0]) > 0 else (thr_all, s_all)\n",
    "\n",
    "    thr_map = {\n",
    "        'ALL': thr_all,\n",
    "        'BP': thr_bp,\n",
    "        'MF': thr_mf,\n",
    "        'CC': thr_cc,\n",
    "        'UNK': thr_unk,\n",
    "    }\n",
    "    thr_path.write_text(json.dumps(thr_map, indent=2), encoding='utf-8')\n",
    "    thr_meta_path.write_text(\n",
    "        json.dumps(\n",
    "            {\n",
    "                'calibration_base': 'mean_ensemble_oof',\n",
    "                'models_used': sorted(oof_candidates.keys()),\n",
    "                'n_rows_total': int(Y_full.shape[0]),\n",
    "                'n_rows_used': int(m),\n",
    "                'thr_grid': THRS.tolist(),\n",
    "                'ia_f1_at_best': {'ALL': s_all, 'BP': s_bp, 'MF': s_mf, 'CC': s_cc, 'UNK': s_unk},\n",
    "                'aspect_counts': {'BP': int(cols_bp.shape[0]), 'MF': int(cols_mf.shape[0]), 'CC': int(cols_cc.shape[0]), 'UNK': int(cols_unk.shape[0])},\n",
    "            },\n",
    "            indent=2,\n",
    "        ),\n",
    "        encoding='utf-8',\n",
    "    )\n",
    "    print('Saved aspect thresholds:', thr_map)\n",
    "\n",
    "# Expose to later cells\n",
    "ASPECT_THRESHOLDS = thr_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d933ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 13D - DNN (PyTorch, multi-branch per modality) + checkpoint push\n",
    "# ===================================================================\n",
    "# Rank-1 style correction: each modality gets its own head before fusion.\n",
    "# Extreme ensembling correction: 5 seeds × 5 folds = 25 models.\n",
    "# Implementation guardrails:\n",
    "#   - IA-weighted BCE is mandatory (class_weight per term).\n",
    "#   - Outputs must remain full 13,500 columns for the Phase 3 GCN contract.\n",
    "\n",
    "if not TRAIN_LEVEL1:\n",
    "    print('Skipping DNN (TRAIN_LEVEL1=False).')\n",
    "else:\n",
    "    import gc\n",
    "    import json\n",
    "    from pathlib import Path\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "    from sklearn.model_selection import KFold\n",
    "\n",
    "    WORK_ROOT = Path(WORK_ROOT)\n",
    "    FEAT_DIR = WORK_ROOT / 'features'\n",
    "    PRED_DIR = FEAT_DIR / 'level1_preds'\n",
    "    PRED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # RANK-1: Load aspect-specific thresholds (prerequisite: Cell 13F must run first)\n",
    "    thr_path = FEAT_DIR / 'aspect_thresholds.json'\n",
    "    if not thr_path.exists():\n",
    "        print(f'[WARN] aspect_thresholds.json not found at {thr_path}. Run Cell 13F first for per-aspect thresholds (proven +3.3% F1 boost).')\n",
    "        ASPECT_THRESHOLDS = {'ALL': 0.3, 'BP': 0.25, 'MF': 0.35, 'CC': 0.35, 'UNK': 0.3}  # fallback defaults\n",
    "    else:\n",
    "        ASPECT_THRESHOLDS = json.loads(thr_path.read_text(encoding='utf-8'))\n",
    "        print(f'[DNN] Loaded aspect thresholds: {ASPECT_THRESHOLDS}')\n",
    "\n",
    "    dnn_oof_path = PRED_DIR / 'oof_pred_dnn.npy'\n",
    "    dnn_test_path = PRED_DIR / 'test_pred_dnn.npy'\n",
    "\n",
    "    if dnn_oof_path.exists() and dnn_test_path.exists():\n",
    "        oof_pred_dnn = np.load(dnn_oof_path)\n",
    "        test_pred_dnn = np.load(dnn_test_path)\n",
    "        print('Loaded existing DNN preds:', dnn_oof_path.name, dnn_test_path.name)\n",
    "    else:\n",
    "        if 'features_train' not in globals() or 'features_test' not in globals():\n",
    "            raise RuntimeError('Missing `features_train`/`features_test`. Run Cell 13a first (Phase 2 setup).')\n",
    "        if 'Y' not in globals():\n",
    "            raise RuntimeError('Missing Y. Run Cell 13a first (targets).')\n",
    "\n",
    "        # ---- IA class weights (per-term) ----\n",
    "        top_terms_path = FEAT_DIR / 'top_terms_13500.json'\n",
    "        if not top_terms_path.exists():\n",
    "            raise FileNotFoundError(f'Missing {top_terms_path}. Run Cell 13a first.')\n",
    "        top_terms = [str(t) for t in json.loads(top_terms_path.read_text(encoding='utf-8'))]\n",
    "\n",
    "        ia_path = WORK_ROOT / 'IA.tsv'\n",
    "        if not ia_path.exists():\n",
    "            raise FileNotFoundError(f'Missing IA.tsv at {ia_path}')\n",
    "        ia_df = pd.read_csv(ia_path, sep='\\t')\n",
    "        # robust column naming (some IA files use 'term' or '#term')\n",
    "        term_col = 'term' if 'term' in ia_df.columns else ('#term' if '#term' in ia_df.columns else ia_df.columns[0])\n",
    "        ia_col = 'ia' if 'ia' in ia_df.columns else (ia_df.columns[1] if len(ia_df.columns) > 1 else ia_df.columns[0])\n",
    "        ia_map = dict(zip(ia_df[term_col].astype(str).values, ia_df[ia_col].astype(np.float32).values))\n",
    "        weights = np.asarray([ia_map.get(t, np.float32(1.0)) for t in top_terms], dtype=np.float32)\n",
    "        # weights broadcast over classes: (1, L)\n",
    "        w_t = torch.from_numpy(weights).view(1, -1)\n",
    "        print(f'[DNN] IA weights ready: shape={weights.shape} min={float(weights.min()):.4f} max={float(weights.max()):.4f}')\n",
    "\n",
    "        # ---- label contract (must stay 13,500-wide) ----\n",
    "        out_dim = int(Y.shape[1])\n",
    "        if out_dim != len(top_terms):\n",
    "            raise RuntimeError(f'DNN label contract mismatch: Y has {out_dim} cols but top_terms has {len(top_terms)}')\n",
    "        if out_dim != 13500:\n",
    "            raise RuntimeError(f'DNN expects 13,500 labels; got out_dim={out_dim}')\n",
    "\n",
    "        # ---- modality inputs ----\n",
    "        # Use per-modality arrays directly (no flat X copy).\n",
    "        required_keys = ['t5', 'esm2_650m', 'esm2_3b', 'ankh', 'text', 'taxa']\n",
    "        missing = [k for k in required_keys if k not in features_train]\n",
    "        if missing:\n",
    "            raise FileNotFoundError(f'Missing mandatory DNN modalities: {missing}. Run embeddings/text/taxa stages to materialise them.')\n",
    "\n",
    "        # Optional 7th branch: PB/GBDT probabilities (teacher features).\n",
    "        # If Cell 13b already produced OOF/test predictions, treat them as an additional modality input.\n",
    "        # This is leakage-safe because we use OOF for train rows and a proper test_pred for test rows.\n",
    "        gbdt_oof_path = PRED_DIR / 'oof_pred_gbdt.npy'\n",
    "        gbdt_test_path = PRED_DIR / 'test_pred_gbdt.npy'\n",
    "        use_pb = bool(gbdt_oof_path.exists() and gbdt_test_path.exists())\n",
    "\n",
    "        # Build local feature dicts for the DNN (may include pb)\n",
    "        dnn_train = dict(features_train)\n",
    "        dnn_test = dict(features_test)\n",
    "        dnn_keys = list(required_keys)\n",
    "\n",
    "        if use_pb:\n",
    "            pb_oof = np.load(gbdt_oof_path, mmap_mode='r')\n",
    "            pb_test = np.load(gbdt_test_path, mmap_mode='r')\n",
    "            if int(pb_oof.shape[0]) != int(Y.shape[0]):\n",
    "                raise RuntimeError(f'PB/GBDT OOF rows mismatch: {pb_oof.shape[0]} vs Y rows {Y.shape[0]}')\n",
    "            if int(pb_oof.shape[1]) != out_dim or int(pb_test.shape[1]) != out_dim:\n",
    "                raise RuntimeError(f'PB/GBDT pred cols mismatch: expected {out_dim}, got oof={pb_oof.shape}, test={pb_test.shape}')\n",
    "            dnn_train['pb'] = pb_oof\n",
    "            dnn_test['pb'] = pb_test\n",
    "            dnn_keys.append('pb')\n",
    "            print('[DNN] Using 7th modality branch: pb (= GBDT OOF/test probabilities)')\n",
    "        else:\n",
    "            print('[DNN] PB/GBDT branch not available (missing GBDT OOF/test preds); proceeding with 6 modalities.')\n",
    "\n",
    "        print(f'DNN modality heads: {dnn_keys} (n={len(dnn_keys)})')\n",
    "        dims = {k: int(dnn_train[k].shape[1]) for k in dnn_keys}\n",
    "\n",
    "        class MultiModalDataset(Dataset):\n",
    "            def __init__(self, X_dict, y, keys, idx):\n",
    "                self.X_dict = X_dict\n",
    "                self.y = y\n",
    "                self.keys = keys\n",
    "                self.idx = np.asarray(idx, dtype=np.int64)\n",
    "            def __len__(self):\n",
    "                return int(self.idx.shape[0])\n",
    "            def __getitem__(self, i):\n",
    "                j = int(self.idx[i])\n",
    "                xs = [np.asarray(self.X_dict[k][j], dtype=np.float32) for k in self.keys]\n",
    "                if self.y is None:\n",
    "                    return xs\n",
    "                yy = np.asarray(self.y[j], dtype=np.float32)\n",
    "                return xs, yy\n",
    "\n",
    "        def _collate(batch):\n",
    "            # batch: list of (xs, y) or xs\n",
    "            if isinstance(batch[0], tuple):\n",
    "                xs_list, ys = zip(*batch)\n",
    "                xs_by_key = list(zip(*xs_list))\n",
    "                xs_t = [torch.from_numpy(np.stack(v, axis=0)) for v in xs_by_key]\n",
    "                y_t = torch.from_numpy(np.stack(ys, axis=0))\n",
    "                return xs_t, y_t\n",
    "            else:\n",
    "                xs_by_key = list(zip(*batch))\n",
    "                xs_t = [torch.from_numpy(np.stack(v, axis=0)) for v in xs_by_key]\n",
    "                return xs_t\n",
    "\n",
    "        class ModalityHead(nn.Module):\n",
    "            def __init__(self, in_dim: int, p: float = 0.2):\n",
    "                super().__init__()\n",
    "                # Keep heads uniform but avoid huge parameter blow-ups for very wide modalities\n",
    "                if int(in_dim) >= 8000:\n",
    "                    hidden1, hidden2 = 1024, 1024\n",
    "                elif int(in_dim) >= 2000:\n",
    "                    hidden1, hidden2 = 2048, 1024\n",
    "                else:\n",
    "                    hidden1, hidden2 = 1024, 512\n",
    "                self.out_dim = int(hidden2)\n",
    "                self.net = nn.Sequential(\n",
    "                    nn.Linear(int(in_dim), int(hidden1)),\n",
    "                    nn.BatchNorm1d(int(hidden1)),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(p),\n",
    "                    nn.Linear(int(hidden1), int(hidden2)),\n",
    "                    nn.BatchNorm1d(int(hidden2)),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(p),\n",
    "                )\n",
    "            def forward(self, x):\n",
    "                return self.net(x)\n",
    "\n",
    "        class MultiBranchDNN(nn.Module):\n",
    "            def __init__(self, dims_by_key: dict, out_dim: int, p: float = 0.2):\n",
    "                super().__init__()\n",
    "                self.keys = list(dims_by_key.keys())\n",
    "                self.heads = nn.ModuleDict({k: ModalityHead(in_dim=int(dims_by_key[k]), p=p) for k in self.keys})\n",
    "                fused_dim = int(sum(self.heads[k].out_dim for k in self.keys))\n",
    "                self.trunk = nn.Sequential(\n",
    "                    nn.Linear(fused_dim, 2048),\n",
    "                    nn.BatchNorm1d(2048),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(p),\n",
    "                    nn.Linear(2048, out_dim),\n",
    "                )\n",
    "            def forward(self, xs):\n",
    "                hs = []\n",
    "                for k, x in zip(self.keys, xs):\n",
    "                    hs.append(self.heads[k](x))\n",
    "                h = torch.cat(hs, dim=1)\n",
    "                return self.trunk(h)\n",
    "\n",
    "        def train_one_seed_fold(train_idx, val_idx, seed: int, epochs: int, batch_size: int, lr: float, device: torch.device):\n",
    "            torch.manual_seed(42 + seed)\n",
    "            np.random.seed(42 + seed)\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "            torch.backends.cudnn.benchmark = False\n",
    "            \n",
    "            model = MultiBranchDNN(dims_by_key=dims, out_dim=out_dim, p=0.2).to(device)\n",
    "            opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "            w = w_t.to(device)\n",
    "\n",
    "            ds_tr = MultiModalDataset(dnn_train, Y, dnn_keys, train_idx)\n",
    "            ds_va = MultiModalDataset(dnn_train, Y, dnn_keys, val_idx)\n",
    "            dl_tr = DataLoader(ds_tr, batch_size=batch_size, shuffle=True, drop_last=False, num_workers=0, collate_fn=_collate)\n",
    "            dl_va = DataLoader(ds_va, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=0, collate_fn=_collate)\n",
    "\n",
    "            for ep in range(1, epochs + 1):\n",
    "                model.train()\n",
    "                for xs, yb in dl_tr:\n",
    "                    xs = [x.to(device, non_blocking=True).float() for x in xs]\n",
    "                    yb = yb.to(device, non_blocking=True).float()\n",
    "                    logits = model(xs)\n",
    "                    # IA-weighted BCE: class weights applied per term (broadcast over batch)\n",
    "                    loss_per = F.binary_cross_entropy_with_logits(logits, yb, reduction='none')\n",
    "                    loss = (loss_per * w).mean()\n",
    "                    opt.zero_grad(set_to_none=True)\n",
    "                    loss.backward()\n",
    "                    opt.step()\n",
    "\n",
    "                # RANK-1 UPDATE #2: Aspect-specific threshold validation\n",
    "                if 'ia_weighted_f1' in globals():\n",
    "                    model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        va_scores = []\n",
    "                        va_true = []\n",
    "                        for xs, yb in dl_va:\n",
    "                            xs = [x.to(device, non_blocking=True).float() for x in xs]\n",
    "                            logits = model(xs)\n",
    "                            va_scores.append(torch.sigmoid(logits).cpu().numpy())\n",
    "                            va_true.append(yb.numpy())\n",
    "                        va_scores = np.vstack(va_scores)\n",
    "                        va_true = np.vstack(va_true)\n",
    "                        \n",
    "                        # Use aspect-specific thresholds (not hardcoded 0.3)\n",
    "                        # Compute ALL metric with average of aspect thresholds\n",
    "                        avg_thr = np.mean([ASPECT_THRESHOLDS.get('BP', 0.25), \n",
    "                                          ASPECT_THRESHOLDS.get('MF', 0.35), \n",
    "                                          ASPECT_THRESHOLDS.get('CC', 0.35)])\n",
    "                        s = ia_weighted_f1(va_true, va_scores, thr=float(avg_thr))\n",
    "                    print(f'  seed={seed} ep={ep}/{epochs} IA-F1={s} (thr={avg_thr:.2f})')\n",
    "\n",
    "            return model\n",
    "\n",
    "        # RANK-1 UPDATE #1: GPU Fast Path for predict_on_split\n",
    "        def predict_on_split(model: nn.Module, X_dict, idx, batch_size: int, device: torch.device):\n",
    "            ds = MultiModalDataset(X_dict, None, dnn_keys, idx)\n",
    "            dl = DataLoader(ds, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=0, collate_fn=_collate)\n",
    "            \n",
    "            # Pre-allocate GPU buffer (Conveyor Belt)\n",
    "            n = len(idx)\n",
    "            preds_gpu = torch.zeros((n, out_dim), dtype=torch.float32, device=device)\n",
    "            \n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                offset = 0\n",
    "                for xs in dl:\n",
    "                    xs = [x.to(device, non_blocking=True).float() for x in xs]\n",
    "                    logits = model(xs)\n",
    "                    b = logits.shape[0]\n",
    "                    preds_gpu[offset:offset+b] = torch.sigmoid(logits)  # Direct GPU store\n",
    "                    offset += b\n",
    "            \n",
    "            # Single transfer to host at the end\n",
    "            return preds_gpu.cpu().numpy().astype(np.float32)\n",
    "        \n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print('DNN device:', device)\n",
    "\n",
    "        n_splits = 5\n",
    "        n_seeds = 5\n",
    "        epochs = 10\n",
    "        batch_size = 128\n",
    "        lr = 1e-3\n",
    "        \n",
    "        # Initialize accumulators\n",
    "        train_n = int(Y.shape[0])\n",
    "        test_n = int(dnn_test[dnn_keys[0]].shape[0])\n",
    "        oof_pred_dnn = np.zeros((train_n, out_dim), dtype=np.float32)\n",
    "        test_pred_dnn = np.zeros((test_n, out_dim), dtype=np.float32)\n",
    "        counts = np.zeros((train_n, 1), dtype=np.float32)\n",
    "        \n",
    "        # RANK-1: Extreme Ensembling 5×5 (Auditor-approved v2)\n",
    "        # Outer seed loop + aggressive cleanup prevents cuBLAS handle accumulation\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        for fold, (tr_idx, va_idx) in enumerate(kf.split(np.arange(train_n)), start=1):\n",
    "            print(f'[DNN] Fold {fold}/{n_splits}')\n",
    "            fold_test = np.zeros((test_n, out_dim), dtype=np.float32)\n",
    "            \n",
    "            for seed in range(n_seeds):\n",
    "                print(f'  [Seed {seed+1}/{n_seeds}] Training...')\n",
    "                model = train_one_seed_fold(tr_idx, va_idx, seed=seed, epochs=epochs, batch_size=batch_size, lr=lr, device=device)\n",
    "\n",
    "                # OOF preds\n",
    "                preds_va = predict_on_split(model, dnn_train, va_idx, batch_size=1024, device=device)\n",
    "                \n",
    "                # RANK-1 UPDATE #3: Quality Gate - Check for finite values\n",
    "                if not np.isfinite(preds_va).all():\n",
    "                    raise RuntimeError(f\"DNN produced non-finite predictions at Fold {fold}, Seed {seed}\")\n",
    "                \n",
    "                oof_pred_dnn[va_idx] += preds_va\n",
    "                counts[va_idx] += 1.0\n",
    "\n",
    "                # TEST preds\n",
    "                te_idx = np.arange(test_n, dtype=np.int64)\n",
    "                preds_te = predict_on_split(model, dnn_test, te_idx, batch_size=1024, device=device)\n",
    "                \n",
    "                # RANK-1 UPDATE #3: Quality Gate - Check for finite values\n",
    "                if not np.isfinite(preds_te).all():\n",
    "                    raise RuntimeError(f\"DNN produced non-finite test predictions at Fold {fold}, Seed {seed}\")\n",
    "                \n",
    "                fold_test += preds_te\n",
    "\n",
    "                # RANK-1: Aggressive VRAM cleanup (prevents A100 handle exhaustion across 25 models)\n",
    "                del model\n",
    "                gc.collect()\n",
    "                if device.type == 'cuda':\n",
    "                    torch.cuda.empty_cache()\n",
    "            \n",
    "            test_pred_dnn += (fold_test / float(n_seeds))\n",
    "            print(f'  Fold {fold} complete: {n_seeds} seeds averaged')\n",
    "        \n",
    "        oof_pred_dnn = (oof_pred_dnn / np.maximum(counts, 1.0)).astype(np.float32)\n",
    "        test_pred_dnn = (test_pred_dnn / float(n_splits)).astype(np.float32)\n",
    "\n",
    "        # Contract guardrail: MUST remain (n_train, 13500) and (n_test, 13500)\n",
    "        if int(oof_pred_dnn.shape[1]) != 13500 or int(test_pred_dnn.shape[1]) != 13500:\n",
    "            raise RuntimeError(f'DNN output contract violated: oof={oof_pred_dnn.shape} test={test_pred_dnn.shape}')\n",
    "\n",
    "        np.save(dnn_oof_path, oof_pred_dnn)\n",
    "        np.save(dnn_test_path, test_pred_dnn)\n",
    "        print('Saved:', dnn_oof_path)\n",
    "        print('Saved:', dnn_test_path)\n",
    "\n",
    "    # Checkpoint push (always)\n",
    "    STORE.maybe_push(\n",
    "        stage='stage_07c_level1_dnn',\n",
    "        required_paths=[\n",
    "            str((WORK_ROOT / 'features' / 'top_terms_13500.json').as_posix()),\n",
    "            str(dnn_oof_path.as_posix()),\n",
    "            str(dnn_test_path.as_posix()),\n",
    "        ],\n",
    "        note='Level-1 DNN predictions (OOF + test).',\n",
    "    )\n",
    "\n",
    "    # Diagnostics: probability histograms + IA-F1 vs threshold (sampled)\n",
    "    try:\n",
    "        import os\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.rcParams.update({'font.size': 12})\n",
    "        DIAG_N = int(os.getenv('CAFA_DIAG_N', '20000'))\n",
    "        \n",
    "        def _sub(y_true: np.ndarray, y_score: np.ndarray):\n",
    "            n = int(y_true.shape[0])\n",
    "            m = min(n, int(DIAG_N))\n",
    "            if m <= 0:\n",
    "                return y_true[:0], y_score[:0]\n",
    "            idx = np.linspace(0, n - 1, num=m, dtype=np.int64)\n",
    "            return y_true[idx], y_score[idx]\n",
    "        \n",
    "        y_t, y_s = _sub(Y, oof_pred_dnn)\n",
    "        row_max_oof = y_s.max(axis=1)\n",
    "        row_mean_oof = y_s.mean(axis=1)\n",
    "        \n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.hist(row_max_oof, bins=60, alpha=0.7)\n",
    "        plt.title('DNN OOF: max probability per protein (sampled)')\n",
    "        plt.xlabel('max prob')\n",
    "        plt.ylabel('count')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "        \n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.hist(row_mean_oof, bins=60, alpha=0.7)\n",
    "        plt.title('DNN OOF: mean probability per protein (sampled)')\n",
    "        plt.xlabel('mean prob')\n",
    "        plt.ylabel('count')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "        \n",
    "        if test_pred_dnn is not None:\n",
    "            te_s = test_pred_dnn\n",
    "            te_m = min(int(te_s.shape[0]), int(DIAG_N))\n",
    "            te_idx = np.linspace(0, int(te_s.shape[0]) - 1, num=te_m, dtype=np.int64) if te_m > 0 else np.array([], dtype=np.int64)\n",
    "            row_max_te = te_s[te_idx].max(axis=1) if te_m > 0 else np.array([], dtype=np.float32)\n",
    "            \n",
    "            plt.figure(figsize=(10, 4))\n",
    "            plt.hist(row_max_oof, bins=60, alpha=0.5, label='OOF')\n",
    "            plt.hist(row_max_te, bins=60, alpha=0.5, label='test')\n",
    "            plt.title('DNN: max probability per protein (OOF vs test; sampled)')\n",
    "            plt.xlabel('max prob')\n",
    "            plt.ylabel('count')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        \n",
    "        if 'ia_weighted_f1' in globals():\n",
    "            thrs = np.linspace(0.05, 0.60, 23)\n",
    "            curves = {k: [] for k in ['ALL', 'MF', 'BP', 'CC']}\n",
    "            for thr in thrs:\n",
    "                s = ia_weighted_f1(y_t, y_s, thr=float(thr))\n",
    "                for k in curves.keys():\n",
    "                    curves[k].append(s[k])\n",
    "            \n",
    "            plt.figure(figsize=(10, 3))\n",
    "            for k in ['ALL', 'MF', 'BP', 'CC']:\n",
    "                plt.plot(thrs, curves[k], label=k)\n",
    "            plt.title('DNN OOF: IA-weighted F1 vs threshold (sampled)')\n",
    "            plt.xlabel('threshold')\n",
    "            plt.ylabel('IA-F1')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print('DNN diagnostics skipped:', repr(e))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
