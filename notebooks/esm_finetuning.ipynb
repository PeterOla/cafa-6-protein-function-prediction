{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f44084c2",
   "metadata": {},
   "source": [
    "## Step 1: Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf3c929d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m sys.path.append(\u001b[38;5;28mstr\u001b[39m(Path.cwd().parent))\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Core libraries\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# Add project root to path so we can import our modules\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "# Core libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Transformers (for ESM-2)\n",
    "from transformers import AutoTokenizer, get_linear_schedule_with_warmup\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "# Our custom modules\n",
    "from src.data.finetune_dataset import create_datasets\n",
    "from src.models.esm_classifier import ESMForGOPrediction\n",
    "\n",
    "print(\"âœ… Imports complete\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee56428b",
   "metadata": {},
   "source": [
    "## Step 2: Configuration\n",
    "\n",
    "These are our hyperparameters. Think of them as the \"training plan\" settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3711327d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Hyperparameters ===\n",
    "BATCH_SIZE = 8                    # How many proteins we process at once (limited by GPU memory)\n",
    "GRADIENT_ACCUMULATION = 4         # Effective batch size = 8 * 4 = 32 (trick to handle large batches)\n",
    "LEARNING_RATE = 2e-5              # How fast the model learns (too high = unstable, too low = slow)\n",
    "NUM_EPOCHS = 10                   # How many times we go through the entire dataset\n",
    "VOCAB_SIZE = 5000                 # Top 5000 most common GO terms (out of 26k total)\n",
    "MIN_COUNT = 10                    # Only include terms that appear at least 10 times\n",
    "PATIENCE = 3                      # Stop if no improvement for 3 epochs (early stopping)\n",
    "\n",
    "# === Paths ===\n",
    "BASE_DIR = Path.cwd().parent\n",
    "FASTA_PATH = BASE_DIR / \"Train/train_sequences.fasta\"   # Protein sequences\n",
    "LABELS_PATH = BASE_DIR / \"Train/train_terms.tsv\"        # GO term labels\n",
    "SAVE_DIR = BASE_DIR / \"models/esm_finetuned\"            # Where to save the trained model\n",
    "\n",
    "# === Device ===\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Batch size: {BATCH_SIZE} (effective: {BATCH_SIZE * GRADIENT_ACCUMULATION})\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Max epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Vocabulary: Top {VOCAB_SIZE} GO terms\")\n",
    "print(f\"  Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c2e8af",
   "metadata": {},
   "source": [
    "## Step 3: Load & Prepare Data\n",
    "\n",
    "This step:\n",
    "1. Loads the 82k protein sequences from the FASTA file\n",
    "2. Loads the GO term labels from the TSV file\n",
    "3. Builds a vocabulary of the 5000 most common GO terms\n",
    "4. Splits into 80% training, 20% validation\n",
    "5. Creates PyTorch DataLoaders for batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e199ce39",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading datasets...\")\n",
    "print(\"This will take ~2 minutes to load 82k sequences and tokenize them.\")\n",
    "\n",
    "# Create train and validation datasets\n",
    "# The dataset class handles:\n",
    "#   - Loading sequences and labels\n",
    "#   - Tokenizing sequences (MVLSP... -> [23, 5, 12, 15, 1, ...])\n",
    "#   - Creating multi-hot label vectors (5000-dim vector of 0s and 1s)\n",
    "train_dataset, val_dataset, vocab = create_datasets(\n",
    "    FASTA_PATH,\n",
    "    LABELS_PATH,\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    min_count=MIN_COUNT,\n",
    "    val_split=0.2\n",
    ")\n",
    "\n",
    "# Save the vocabulary so we can use it later for predictions\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "with open(SAVE_DIR / \"vocab.json\", \"w\") as f:\n",
    "    json.dump(vocab, f, indent=2)\n",
    "print(f\"âœ… Vocabulary saved to {SAVE_DIR / 'vocab.json'}\")\n",
    "\n",
    "# Create DataLoaders (handles batching and shuffling)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,          # Shuffle training data each epoch\n",
    "    num_workers=0          # 0 for Windows (multiprocessing issues)\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,         # Don't shuffle validation data\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Data ready:\")\n",
    "print(f\"  Train batches: {len(train_loader)} ({len(train_dataset)} proteins)\")\n",
    "print(f\"  Val batches: {len(val_loader)} ({len(val_dataset)} proteins)\")\n",
    "print(f\"  Vocabulary size: {len(vocab)} GO terms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1bc0cb",
   "metadata": {},
   "source": [
    "## Step 4: Initialize the Model\n",
    "\n",
    "Our model is:\n",
    "1. **ESM-2 Backbone** (Pre-trained on 250M protein sequences)\n",
    "2. **Classification Head** (Maps from ESM-2's 320-dim output to 5000 GO terms)\n",
    "\n",
    "We're fine-tuning **all layers** (not freezing the backbone)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604507a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initializing model...\")\n",
    "\n",
    "# Create the model\n",
    "# This loads the pre-trained ESM-2 weights from HuggingFace\n",
    "model = ESMForGOPrediction(\n",
    "    model_name=\"facebook/esm2_t6_8M_UR50D\",  # ESM-2 Tiny (8M parameters)\n",
    "    num_labels=VOCAB_SIZE,                    # 5000 GO terms\n",
    "    dropout=0.3,                              # Dropout for regularization\n",
    "    freeze_layers=0                           # Fine-tune all layers (0 = no freezing)\n",
    ")\n",
    "\n",
    "# Move to GPU\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"\\nâœ… Model ready on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dfa51c",
   "metadata": {},
   "source": [
    "## Step 5: Setup Training Components\n",
    "\n",
    "We need:\n",
    "1. **Optimizer** (How to update the weights)\n",
    "2. **Scheduler** (Gradually decrease learning rate)\n",
    "3. **Loss Function** (How to measure mistakes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b953ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer (AdamW is the standard for transformers)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=0.01      # L2 regularization to prevent overfitting\n",
    ")\n",
    "\n",
    "# Learning rate scheduler (warmup + linear decay)\n",
    "# Start with low LR, ramp up, then gradually decrease\n",
    "total_steps = len(train_loader) * NUM_EPOCHS // GRADIENT_ACCUMULATION\n",
    "warmup_steps = 100\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Loss function (Binary Cross-Entropy with Logits)\n",
    "# Measures how far our predictions are from the true labels\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "print(\"Training components initialized:\")\n",
    "print(f\"  Total training steps: {total_steps}\")\n",
    "print(f\"  Warmup steps: {warmup_steps}\")\n",
    "print(f\"  Optimizer: AdamW\")\n",
    "print(f\"  Loss: BCEWithLogitsLoss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d84f7fd",
   "metadata": {},
   "source": [
    "## Step 6: Training Functions\n",
    "\n",
    "### 6.1 Train One Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9cd463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, scheduler, criterion, device, gradient_accumulation_steps=4):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    \n",
    "    Returns:\n",
    "        Average training loss\n",
    "    \"\"\"\n",
    "    model.train()  # Set to training mode (enables dropout, etc.)\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=\"Training\")\n",
    "    for i, (inputs, labels) in enumerate(pbar):\n",
    "        # Move data to GPU\n",
    "        input_ids = inputs['input_ids'].to(device)\n",
    "        attention_mask = inputs['attention_mask'].to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass (get predictions)\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        \n",
    "        # Calculate loss (how wrong are we?)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        # Normalize loss by accumulation steps\n",
    "        # This is a trick to simulate larger batch sizes\n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        \n",
    "        # Backward pass (calculate gradients)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights after accumulating gradients\n",
    "        if (i + 1) % gradient_accumulation_steps == 0:\n",
    "            # Clip gradients to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item() * gradient_accumulation_steps\n",
    "        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "print(\"âœ… Training function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c79ccc",
   "metadata": {},
   "source": [
    "### 6.2 Evaluate on Validation Set\n",
    "\n",
    "**Key Fix:** We now try multiple thresholds (0.01, 0.05, 0.1, etc.) instead of just 0.5.\n",
    "\n",
    "This solves the \"F1 = 0.0000\" problem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dd3385",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()  # Disable gradient calculation (faster, saves memory)\n",
    "def evaluate(model, val_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model on validation set.\n",
    "    Tries multiple thresholds and picks the best one.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with val_loss, f1, precision, recall, and best threshold\n",
    "    \"\"\"\n",
    "    model.eval()  # Set to evaluation mode (disables dropout)\n",
    "    total_loss = 0\n",
    "    all_probs = []  # Store raw probabilities (not binary predictions)\n",
    "    all_labels = []\n",
    "    \n",
    "    pbar = tqdm(val_loader, desc=\"Evaluating\")\n",
    "    for inputs, labels in pbar:\n",
    "        # Move to GPU\n",
    "        input_ids = inputs['input_ids'].to(device)\n",
    "        attention_mask = inputs['attention_mask'].to(device)\n",
    "        labels_dev = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = criterion(logits, labels_dev)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Convert logits to probabilities (0 to 1 range)\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()\n",
    "        all_probs.append(probs)\n",
    "        all_labels.append(labels.numpy())\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    all_probs = np.vstack(all_probs)\n",
    "    all_labels = np.vstack(all_labels)\n",
    "    \n",
    "    # Calculate loss\n",
    "    val_loss = total_loss / len(val_loader)\n",
    "    \n",
    "    # Try multiple thresholds and pick the best\n",
    "    # This is crucial! With 5000 classes and only ~6 positives per sample,\n",
    "    # a threshold of 0.5 is way too high.\n",
    "    thresholds = [0.01, 0.05, 0.1, 0.15, 0.2, 0.3, 0.4, 0.5]\n",
    "    best_f1 = 0.0\n",
    "    best_threshold = 0.5\n",
    "    best_metrics = {}\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        # Convert probabilities to binary predictions\n",
    "        preds = (all_probs > threshold).astype(int)\n",
    "        \n",
    "        # Calculate F1 (sample-wise average)\n",
    "        f1 = f1_score(all_labels, preds, average='samples', zero_division=0)\n",
    "        \n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "            best_metrics = {\n",
    "                \"f1\": f1,\n",
    "                \"precision\": precision_score(all_labels, preds, average='samples', zero_division=0),\n",
    "                \"recall\": recall_score(all_labels, preds, average='samples', zero_division=0),\n",
    "                \"threshold\": threshold\n",
    "            }\n",
    "    \n",
    "    return {\n",
    "        \"val_loss\": val_loss,\n",
    "        **best_metrics\n",
    "    }\n",
    "\n",
    "print(\"âœ… Evaluation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62cb427",
   "metadata": {},
   "source": [
    "## Step 7: Main Training Loop\n",
    "\n",
    "This is where the magic happens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96423d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tracking variables\n",
    "best_f1 = 0.0\n",
    "best_epoch = 0\n",
    "epochs_without_improvement = 0\n",
    "history = {\n",
    "    \"train_loss\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_f1\": [],\n",
    "    \"val_precision\": [],\n",
    "    \"val_recall\": [],\n",
    "    \"best_threshold\": []\n",
    "}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STARTING FINE-TUNING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Target: Beat KNN baseline (F1 > 0.17)\")\n",
    "print(f\"Time estimate: ~45 minutes per epoch on RTX 2070\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # === TRAIN ===\n",
    "    train_loss = train_epoch(\n",
    "        model, train_loader, optimizer, scheduler, criterion, device, GRADIENT_ACCUMULATION\n",
    "    )\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    \n",
    "    # === EVALUATE ===\n",
    "    metrics = evaluate(model, val_loader, criterion, device)\n",
    "    history[\"val_loss\"].append(metrics[\"val_loss\"])\n",
    "    history[\"val_f1\"].append(metrics[\"f1\"])\n",
    "    history[\"val_precision\"].append(metrics[\"precision\"])\n",
    "    history[\"val_recall\"].append(metrics[\"recall\"])\n",
    "    history[\"best_threshold\"].append(metrics[\"threshold\"])\n",
    "    \n",
    "    # === PRINT RESULTS ===\n",
    "    print(f\"\\nðŸ“Š Epoch {epoch + 1} Results:\")\n",
    "    print(f\"  Train Loss:    {train_loss:.4f}\")\n",
    "    print(f\"  Val Loss:      {metrics['val_loss']:.4f}\")\n",
    "    print(f\"  Val F1:        {metrics['f1']:.4f} (threshold: {metrics['threshold']:.2f})\")\n",
    "    print(f\"  Val Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"  Val Recall:    {metrics['recall']:.4f}\")\n",
    "    \n",
    "    # === CHECK FOR IMPROVEMENT ===\n",
    "    if metrics['f1'] > best_f1:\n",
    "        best_f1 = metrics['f1']\n",
    "        best_epoch = epoch + 1\n",
    "        epochs_without_improvement = 0\n",
    "        \n",
    "        # Save best model\n",
    "        save_path = SAVE_DIR / \"best_model\"\n",
    "        model.save_pretrained(str(save_path))\n",
    "        print(f\"  âœ… New best F1! Model saved to {save_path}\")\n",
    "        \n",
    "        # Compare to baseline\n",
    "        if best_f1 > 0.17:\n",
    "            print(f\"  ðŸŽ‰ BEAT KNN BASELINE! ({best_f1:.4f} > 0.17)\")\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        print(f\"  â³ No improvement for {epochs_without_improvement} epoch(s)\")\n",
    "    \n",
    "    # === EARLY STOPPING ===\n",
    "    if epochs_without_improvement >= PATIENCE:\n",
    "        print(f\"\\nâš ï¸ Early stopping triggered after {epoch + 1} epochs\")\n",
    "        print(f\"No improvement for {PATIENCE} consecutive epochs.\")\n",
    "        break\n",
    "\n",
    "# === SAVE TRAINING HISTORY ===\n",
    "with open(SAVE_DIR / \"history.json\", \"w\") as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINE-TUNING COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Best F1: {best_f1:.4f} (Epoch {best_epoch})\")\n",
    "print(f\"Baseline (KNN): 0.1776\")\n",
    "if best_f1 > 0.1776:\n",
    "    print(f\"ðŸŽ‰ IMPROVEMENT: +{(best_f1 - 0.1776):.4f}\")\n",
    "else:\n",
    "    print(f\"ðŸ˜ž Still behind baseline by {(0.1776 - best_f1):.4f}\")\n",
    "print(f\"\\nModel saved to: {SAVE_DIR / 'best_model'}\")\n",
    "print(f\"History saved to: {SAVE_DIR / 'history.json'}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0543c4",
   "metadata": {},
   "source": [
    "## Step 8: Visualize Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4c77b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss curves\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "axes[0].plot(history['val_loss'], label='Val Loss', marker='s')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training & Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# F1 curve\n",
    "axes[1].plot(history['val_f1'], label='Val F1', marker='o', color='green')\n",
    "axes[1].axhline(y=0.1776, color='red', linestyle='--', label='KNN Baseline (0.1776)')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('F1 Score')\n",
    "axes[1].set_title('Validation F1 Score')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(SAVE_DIR / 'training_curves.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"âœ… Training curves saved to {SAVE_DIR / 'training_curves.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dc1ac6",
   "metadata": {},
   "source": [
    "## Step 9: Load Best Model & Make Predictions (Optional)\n",
    "\n",
    "This shows how to load the trained model and use it for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60153697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "print(\"Loading best model...\")\n",
    "best_model = ESMForGOPrediction.from_pretrained(str(SAVE_DIR / \"best_model\"))\n",
    "best_model = best_model.to(device)\n",
    "best_model.eval()\n",
    "\n",
    "# Load vocabulary\n",
    "with open(SAVE_DIR / \"vocab.json\", \"r\") as f:\n",
    "    vocab = json.load(f)\n",
    "\n",
    "print(f\"âœ… Best model loaded\")\n",
    "print(f\"   Vocabulary: {len(vocab)} GO terms\")\n",
    "\n",
    "# Example: Predict on a single protein\n",
    "# Get first protein from validation set\n",
    "sample_inputs, sample_labels = val_dataset[0]\n",
    "\n",
    "# Add batch dimension and move to device\n",
    "input_ids = sample_inputs['input_ids'].unsqueeze(0).to(device)\n",
    "attention_mask = sample_inputs['attention_mask'].unsqueeze(0).to(device)\n",
    "\n",
    "# Predict\n",
    "with torch.no_grad():\n",
    "    logits = best_model(input_ids, attention_mask)\n",
    "    probs = torch.sigmoid(logits).cpu().numpy()[0]\n",
    "\n",
    "# Get top 10 predictions\n",
    "top_indices = np.argsort(probs)[-10:][::-1]\n",
    "print(\"\\nTop 10 Predictions:\")\n",
    "for idx in top_indices:\n",
    "    go_term = vocab[idx]\n",
    "    confidence = probs[idx]\n",
    "    true_label = \"âœ…\" if sample_labels[idx] == 1 else \"âŒ\"\n",
    "    print(f\"  {go_term}: {confidence:.4f} {true_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8a714f",
   "metadata": {},
   "source": [
    "## ðŸŽ“ Summary\n",
    "\n",
    "### What We Did:\n",
    "1. âœ… Loaded 82k protein sequences and tokenized them\n",
    "2. âœ… Built a vocabulary of the 5000 most common GO terms\n",
    "3. âœ… Fine-tuned ESM-2 (not just the classification head, but the whole model)\n",
    "4. âœ… Used adaptive threshold tuning (fixed the F1=0 bug)\n",
    "5. âœ… Implemented early stopping to prevent overfitting\n",
    "6. âœ… Saved the best model for future use\n",
    "\n",
    "### Key Insights:\n",
    "- **Threshold matters!** With 5000 classes and only ~6 positives per sample, we can't use 0.5.\n",
    "- **Fine-tuning > Frozen embeddings**: We're teaching ESM-2 to understand proteins specifically for GO prediction.\n",
    "- **Gradient accumulation**: Tricks to simulate large batch sizes on limited GPU memory.\n",
    "\n",
    "### Next Steps:\n",
    "1. Generate predictions on the test set\n",
    "2. Create a submission file\n",
    "3. (Optional) Try ensemble methods (combine KNN + Fine-tuned model)\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You've just fine-tuned a state-of-the-art protein language model. ðŸŽ‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
