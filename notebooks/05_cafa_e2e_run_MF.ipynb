{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faae05b1",
   "metadata": {},
   "source": [
    "# 05 CAFA E2E — Run LogReg for MF (wrapper)\n",
    "\n",
    "Self-contained runner: executes `notebooks/05_cafa_e2e.ipynb` with `TARGET_ASPECT=MF`, stopping before the DNN cell.\n",
    "\n",
    "Key behaviour:\n",
    "- No subprocess calls\n",
    "- Logging-first diagnostics (search paths, stop-marker, failures)\n",
    "- Produces per-aspect artefacts: `oof_pred_logreg_MF.npy`, `test_pred_logreg_MF.npy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b933e2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 01 - Setup (NO REPO)\n",
    "import os\n",
    "import sys\n",
    "import ctypes\n",
    "from pathlib import Path\n",
    "\n",
    "# CUDA loader fix (PyTorch/RAPIDS coexistence): preload venv nvjitlink so we don't pick /usr/local/cuda/lib64\n",
    "try:\n",
    "    _venv_root = Path(sys.executable).resolve().parent.parent\n",
    "    _nvjit_dir = (\n",
    "        _venv_root\n",
    "        / \"lib\"\n",
    "        / f\"python{sys.version_info.major}.{sys.version_info.minor}\"\n",
    "        / \"site-packages\"\n",
    "        / \"nvidia\"\n",
    "        / \"nvjitlink\"\n",
    "        / \"lib\"\n",
    "    )\n",
    "    _nvjit_so = _nvjit_dir / \"libnvJitLink.so.12\"\n",
    "    if _nvjit_so.exists():\n",
    "        ctypes.CDLL(str(_nvjit_so), mode=ctypes.RTLD_GLOBAL)\n",
    "        os.environ[\"LD_LIBRARY_PATH\"] = f\"{_nvjit_dir}:{os.environ.get('LD_LIBRARY_PATH','')}\"\n",
    "        print(f\"[ENV] Preloaded nvjitlink: {_nvjit_so}\")\n",
    "except Exception as _e:\n",
    "    print(f\"[ENV] nvjitlink preload skipped: {_e}\")\n",
    "\n",
    "# Always run from a simple writable location; never cd into a repo.\n",
    "if os.path.exists('/content'):\n",
    "    os.chdir('/content')\n",
    "RUNTIME_ROOT = Path.cwd()\n",
    "DATA_ROOT = (RUNTIME_ROOT / 'cafa6_data')\n",
    "DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "TRAIN_LEVEL1 = True\n",
    "print(f'CWD: {Path.cwd()}')\n",
    "print(f'DATA_ROOT: {DATA_ROOT.resolve()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add96f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 13c - Level 1: Logistic Regression — Long tail 13,500 (Aspect Split BP/MF/CC)\n",
    "# ==============================================================================\n",
    "# Track B (auditor): regularised linear model over the full 13,500-term space.\n",
    "# Goal: Train LR per GO aspect (BP/MF/CC) with RAM-safe target handling.\n",
    "#\n",
    "# Critical fix: Avoid NumPy fancy-indexing on memmaps.\n",
    "# - DO NOT do: Y_aspect = Y_full[:, aspect_indices]  (this materialises a huge dense copy in host RAM)\n",
    "# - Instead: slice Y_full per fold+target-chunk via np.ix_(idx_tr, cols)\n",
    "#\n",
    "# Protocol (recommended for stability): run ONE aspect per fresh runtime.\n",
    "# If `TARGET_ASPECT` is not set, we default to BP.\n",
    "# ==============================================================================\n",
    "\n",
    "if not TRAIN_LEVEL1:\n",
    "    print('Skipping LogReg (TRAIN_LEVEL1=False).')\n",
    "else:\n",
    "    import os\n",
    "    import sys\n",
    "    import time\n",
    "    import threading\n",
    "    import gc\n",
    "    import warnings\n",
    "    import psutil\n",
    "    import json\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import joblib\n",
    "    import torch\n",
    "    from pathlib import Path\n",
    "    from tqdm.auto import tqdm\n",
    "    from sklearn.model_selection import KFold\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.linear_model import SGDClassifier\n",
    "    from sklearn.multiclass import OneVsRestClassifier\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "    warnings.filterwarnings('ignore', category=ConvergenceWarning)\n",
    "\n",
    "    # FAIL FAST: RAPIDS Requirement\n",
    "    try:\n",
    "        import cuml\n",
    "        import cupy\n",
    "        print(\"RAPIDS (cuml, cupy): present\")\n",
    "    except ImportError as e:\n",
    "        raise RuntimeError(\"RAPIDS (cuml/cupy) is REQUIRED for LogReg but missing. \"\n",
    "                           \"Aborting to avoid slow CPU fallback.\") from e\n",
    "\n",
    "    def _stage(msg: str) -> None:\n",
    "        print(msg)\n",
    "        try:\n",
    "            sys.stdout.flush()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # -----------------------------\n",
    "    # WORK_ROOT recovery (safety)\n",
    "    # -----------------------------\n",
    "    if 'WORK_ROOT' not in locals() and 'WORK_ROOT' not in globals():\n",
    "        candidates = [\n",
    "            Path('/content/cafa6_data'),\n",
    "            Path('/content/work'),\n",
    "            Path('/kaggle/working/work'),\n",
    "            Path.cwd() / 'cafa6_data',\n",
    "            Path.cwd() / 'artefacts_local' / 'work',\n",
    "        ]\n",
    "        WORK_ROOT = None\n",
    "        for c in candidates:\n",
    "            if (c / 'parsed' / 'train_terms.parquet').exists():\n",
    "                WORK_ROOT = c\n",
    "                break\n",
    "        if WORK_ROOT is None:\n",
    "            for c in candidates:\n",
    "                if c.exists():\n",
    "                    WORK_ROOT = c\n",
    "                    break\n",
    "        if WORK_ROOT is None:\n",
    "            WORK_ROOT = Path.cwd() / 'cafa6_data'\n",
    "        _stage(f\"[AUDITOR] Recovered WORK_ROOT: {WORK_ROOT}\")\n",
    "\n",
    "    FEAT_DIR = Path(WORK_ROOT) / 'features'\n",
    "    PARSED_DIR = Path(WORK_ROOT) / 'parsed'\n",
    "\n",
    "    PRED_DIR = FEAT_DIR / 'level1_preds'\n",
    "    PRED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Load top_terms + term_to_ns\n",
    "    # -----------------------------\n",
    "    top_terms_path = FEAT_DIR / 'top_terms_13500.json'\n",
    "    if not top_terms_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing {top_terms_path}. Run the Phase 2 setup cell first.\")\n",
    "    top_terms = json.loads(top_terms_path.read_text(encoding='utf-8'))\n",
    "    top_terms = [str(t) for t in top_terms]\n",
    "\n",
    "    # Build / reuse term_to_ns mapping\n",
    "    if 'term_to_ns' in globals():\n",
    "        term_to_ns = globals()['term_to_ns']\n",
    "    else:\n",
    "        try:\n",
    "            import obonet\n",
    "        except Exception as e:\n",
    "            raise RuntimeError('obonet is required for aspect split (term namespaces).') from e\n",
    "\n",
    "        possible_paths = []\n",
    "        if 'PATH_GO_OBO' in globals():\n",
    "            try:\n",
    "                possible_paths.append(Path(globals()['PATH_GO_OBO']))\n",
    "            except Exception:\n",
    "                pass\n",
    "        possible_paths += [\n",
    "            Path(WORK_ROOT) / 'Train' / 'go-basic.obo',\n",
    "            Path(WORK_ROOT) / 'go-basic.obo',\n",
    "            Path('/content/cafa6_data/Train/go-basic.obo'),\n",
    "            Path('Train/go-basic.obo'),\n",
    "            Path('go-basic.obo'),\n",
    "        ]\n",
    "        obo_path = None\n",
    "        for p in possible_paths:\n",
    "            if p is not None and Path(p).exists():\n",
    "                obo_path = Path(p)\n",
    "                break\n",
    "        if obo_path is None:\n",
    "            raise FileNotFoundError(f\"go-basic.obo not found. Candidates: {[str(p) for p in possible_paths]}\")\n",
    "\n",
    "        _stage(f\"[AUDITOR] Loading GO OBO for namespaces: {obo_path}\")\n",
    "        graph = obonet.read_obo(obo_path)\n",
    "        term_to_ns = {node: data.get('namespace', 'unknown') for node, data in graph.nodes(data=True)}\n",
    "\n",
    "    aspect_map = {\n",
    "        'biological_process': 'BP',\n",
    "        'molecular_function': 'MF',\n",
    "        'cellular_component': 'CC',\n",
    "    }\n",
    "\n",
    "    def _aspect_of_term(term: str) -> str:\n",
    "        return aspect_map.get(term_to_ns.get(term), 'UNK')\n",
    "\n",
    "    # -----------------------------\n",
    "    # Load X / X_test (memmap)\n",
    "    # -----------------------------\n",
    "    x_path = FEAT_DIR / 'X_train_mmap.npy'\n",
    "    xt_path = FEAT_DIR / 'X_test_mmap.npy'\n",
    "    if not x_path.exists() or not xt_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing X memmaps ({x_path} / {xt_path}). Run the Phase 2 setup cell first.\")\n",
    "\n",
    "    X = np.load(x_path, mmap_mode='r')\n",
    "    X_test = np.load(xt_path, mmap_mode='r')\n",
    "\n",
    "    # -----------------------------\n",
    "    # Load / build Y (full 13,500) as memmap\n",
    "    # -----------------------------\n",
    "    y_full_path = FEAT_DIR / 'Y_target_13500.npy'\n",
    "    if y_full_path.exists():\n",
    "        Y_full = np.load(y_full_path, mmap_mode='r')\n",
    "    else:\n",
    "        _stage('[AUDITOR] Building Y_target_13500.npy (disk-backed) ...')\n",
    "        train_terms = pd.read_parquet(PARSED_DIR / 'train_terms.parquet')\n",
    "        train_ids_raw = pd.read_feather(PARSED_DIR / 'train_seq.feather')['id'].astype(str)\n",
    "        train_ids = train_ids_raw.str.extract(r\"\\|(.*?)\\|\")[0].fillna(train_ids_raw)\n",
    "\n",
    "        train_terms_top = train_terms[train_terms['term'].isin(top_terms)]\n",
    "        Y_df = train_terms_top.pivot_table(index='EntryID', columns='term', aggfunc='size', fill_value=0)\n",
    "        Y_df = Y_df.reindex(train_ids, fill_value=0)\n",
    "        Y_df = Y_df.reindex(columns=top_terms, fill_value=0)\n",
    "        np.save(y_full_path, Y_df.values.astype(np.float32))\n",
    "        del train_terms, train_ids_raw, train_ids, train_terms_top, Y_df\n",
    "        gc.collect()\n",
    "        Y_full = np.load(y_full_path, mmap_mode='r')\n",
    "\n",
    "    # -----------------------------\n",
    "    # IA weights (optional)\n",
    "    # -----------------------------\n",
    "    weights_full = None\n",
    "    try:\n",
    "        ia_candidates = [Path(WORK_ROOT) / 'IA.tsv', FEAT_DIR / 'IA.tsv', Path('IA.tsv')]\n",
    "        ia_path = next((p for p in ia_candidates if p.exists()), None)\n",
    "        if ia_path is not None:\n",
    "            ia = pd.read_csv(ia_path, sep='\\t')\n",
    "            cols = [c.lower() for c in ia.columns]\n",
    "            term_col = ia.columns[cols.index('term')] if 'term' in cols else ia.columns[0]\n",
    "            if 'ia' in cols:\n",
    "                ia_col = ia.columns[cols.index('ia')]\n",
    "            elif 'information_accretion' in cols:\n",
    "                ia_col = ia.columns[cols.index('information_accretion')]\n",
    "            else:\n",
    "                ia_col = ia.columns[1] if len(ia.columns) > 1 else ia.columns[0]\n",
    "            ia_map = dict(zip(ia[term_col].astype(str), ia[ia_col].astype(np.float32)))\n",
    "            weights_full = np.asarray([ia_map.get(t, np.float32(1.0)) for t in top_terms], dtype=np.float32)\n",
    "            _stage(f\"[AUDITOR] Loaded IA weights from {ia_path}\")\n",
    "    except Exception as e:\n",
    "        _stage(f\"[AUDITOR] IA weights unavailable (continuing): {e}\")\n",
    "        weights_full = None\n",
    "\n",
    "    def _log_mem(msg: str = ''):\n",
    "        try:\n",
    "            proc = psutil.Process(os.getpid())\n",
    "            ram = proc.memory_info().rss / (1024**3)\n",
    "            avail = psutil.virtual_memory().available / (1024**3)\n",
    "            if torch.cuda.is_available():\n",
    "                alloc = torch.cuda.memory_allocated() / (1024**3)\n",
    "                res = torch.cuda.memory_reserved() / (1024**3)\n",
    "            else:\n",
    "                alloc = res = 0.0\n",
    "            print(f\"[MEM] {msg:22s} | RSS: {ram:6.2f}GB | RAM_avail: {avail:6.2f}GB | torch_alloc: {alloc:5.2f}GB torch_res: {res:5.2f}GB\")\n",
    "            sys.stdout.flush()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Placeholder; replaced after RAPIDS discovery.\n",
    "    def _log_gpu_mem(msg: str = ''):\n",
    "        return\n",
    "\n",
    "    # -----------------------------\n",
    "    # RAPIDS discovery\n",
    "    # -----------------------------\n",
    "    try:\n",
    "        import cuml  # noqa: F401\n",
    "        from cuml.linear_model import LogisticRegression as cuLogReg\n",
    "        from cuml.multiclass import OneVsRestClassifier as cuOVR\n",
    "        import cupy as cp\n",
    "        import rmm\n",
    "\n",
    "        try:\n",
    "            rmm.reinitialize(managed_memory=True)\n",
    "            _stage('[AUDITOR] RAPIDS (cuML) detected. RMM Managed Memory ENABLED.')\n",
    "        except Exception as e:\n",
    "            _stage(f'[AUDITOR] RAPIDS detected but RMM init failed ({e}); proceeding with default memory.')\n",
    "\n",
    "        HAS_RAPIDS = True\n",
    "        _stage(\n",
    "            f\"[AUDITOR] versions: cupy={getattr(cp, '__version__', '?')} cuml={getattr(cuml, '__version__', '?')} rmm={getattr(rmm, '__version__', '?')}\"\n",
    "        )\n",
    "\n",
    "        def _maybe_bytes(obj, method_name: str):\n",
    "            fn = getattr(obj, method_name, None)\n",
    "            if callable(fn):\n",
    "                try:\n",
    "                    return int(fn())\n",
    "                except Exception:\n",
    "                    return None\n",
    "            return None\n",
    "\n",
    "        def _pinned_used_bytes(pinned_pool):\n",
    "            used = _maybe_bytes(pinned_pool, 'used_bytes')\n",
    "            if used is not None:\n",
    "                return used\n",
    "            # Older CuPy versions sometimes expose total/free but not used.\n",
    "            total = _maybe_bytes(pinned_pool, 'total_bytes')\n",
    "            free = _maybe_bytes(pinned_pool, 'free_bytes')\n",
    "            if total is not None and free is not None:\n",
    "                return int(total - free)\n",
    "            return None\n",
    "\n",
    "        _GPU_LOG_STATE = {'rmm_printed': False}\n",
    "\n",
    "        def _log_gpu_mem(msg: str = ''):\n",
    "            try:\n",
    "                free_b, total_b = cp.cuda.runtime.memGetInfo()\n",
    "                pool = cp.get_default_memory_pool()\n",
    "                pinned = cp.get_default_pinned_memory_pool()\n",
    "\n",
    "                pool_b = _maybe_bytes(pool, 'used_bytes')\n",
    "                pinned_b = _pinned_used_bytes(pinned)\n",
    "\n",
    "                pool_txt = f\"{pool_b/1e9:6.2f}GB\" if pool_b is not None else \"   n/a\"\n",
    "                pinned_txt = f\"{pinned_b/1e9:6.2f}GB\" if pinned_b is not None else \"   n/a\"\n",
    "\n",
    "                print(\n",
    "                    f\"[GPU] {msg:22s} | free {free_b/1e9:6.2f}GB / {total_b/1e9:6.2f}GB | \"\n",
    "                    f\"cupy_pool {pool_txt} | pinned {pinned_txt}\"\n",
    "                )\n",
    "                if not _GPU_LOG_STATE.get('rmm_printed', False):\n",
    "                    try:\n",
    "                        res = rmm.mr.get_current_device_resource()\n",
    "                        res_name = type(res).__name__\n",
    "                        # Keep this single-line and stable (no memory addresses).\n",
    "                        print(f\"[GPU] rmm: enabled=True resource={res_name}\")\n",
    "                    except Exception:\n",
    "                        print('[GPU] rmm: enabled=True (resource unavailable)')\n",
    "                    _GPU_LOG_STATE['rmm_printed'] = True\n",
    "                sys.stdout.flush()\n",
    "            except Exception as e:\n",
    "                print(f\"[GPU] {msg:22s} | unavailable ({e})\")\n",
    "                sys.stdout.flush()\n",
    "\n",
    "    except Exception:\n",
    "        HAS_RAPIDS = False\n",
    "        cp = None  # type: ignore\n",
    "        _stage('[AUDITOR] RAPIDS NOT detected. Falling back to CPU (slow).')\n",
    "\n",
    "    def _gpu_mem_okay(n_rows: int, n_cols: int, safety: float = 1.35) -> bool:\n",
    "        if not (HAS_RAPIDS and torch.cuda.is_available()):\n",
    "            return False\n",
    "        try:\n",
    "            free_b, _total_b = cp.cuda.runtime.memGetInfo()\n",
    "            need_b = int(n_rows) * int(n_cols) * 4\n",
    "            ok = free_b > int(safety * need_b)\n",
    "            if not ok:\n",
    "                _stage(f\"[AUDITOR] VRAM insufficient: free={free_b/1e9:.1f}GB need~{need_b/1e9:.1f}GB\")\n",
    "            return bool(ok)\n",
    "        except Exception:\n",
    "            return True\n",
    "\n",
    "    # -----------------------------\n",
    "    # Aspect selection (default MF)\n",
    "    # -----------------------------\n",
    "    target_aspect = (os.environ.get('TARGET_ASPECT') or '').strip().upper()\n",
    "    if not target_aspect:\n",
    "        target_aspect = (globals().get('TARGET_ASPECT') or '').strip().upper()\n",
    "    if not target_aspect:\n",
    "        target_aspect = 'MF'\n",
    "        _stage('[AUDITOR] TARGET_ASPECT not set -> defaulting to MF (recommended: run one aspect per fresh runtime).')\n",
    "\n",
    "    if target_aspect not in {'BP', 'MF', 'CC'}:\n",
    "        raise RuntimeError(\n",
    "            f\"Invalid TARGET_ASPECT={target_aspect!r}. Must be one of: BP, MF, CC.\"\n",
    "        )\n",
    "\n",
    "    aspects = [target_aspect]\n",
    "    _stage(f\"[AUDITOR] Training only aspect: {target_aspect}\")\n",
    "\n",
    "    # Runtime knobs (keep quality the same; only batching/chunking changes)\n",
    "    # - TARGET_CHUNK: larger chunk reduces per-chunk overhead and improves GPU utilisation.\n",
    "    # - VAL_BS/TEST_BS: larger batches better saturate A100; reduce if you hit OOM.\n",
    "    TARGET_CHUNK = 500\n",
    "    VAL_BS = 1024\n",
    "    TEST_BS = 8192\n",
    "\n",
    "    n_splits = 5\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    tmp_dir = FEAT_DIR / 'tmp_folds_logreg'\n",
    "    tmp_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    oof_pred_logreg_by_aspect = {}\n",
    "    test_pred_logreg_by_aspect = {}\n",
    "    aspect_indices_map = {}\n",
    "\n",
    "    t0_all = time.time()\n",
    "    _log_mem('start')\n",
    "    _log_gpu_mem('start')\n",
    "\n",
    "    # -----------------------------\n",
    "    # Performance: materialise X_test into RAM (best-effort)\n",
    "    # -----------------------------\n",
    "    # X_test is used repeatedly across folds and target-chunks; keeping it disk-backed can become the bottleneck.\n",
    "    try:\n",
    "        _stage('[AUDITOR] LogReg: materialising X_test into RAM for faster inference...')\n",
    "        X_test_ram = np.ascontiguousarray(np.asarray(X_test, dtype=np.float32))\n",
    "        _stage(f'[AUDITOR] LogReg: X_test_ram shape={X_test_ram.shape} dtype={X_test_ram.dtype}')\n",
    "    except MemoryError:\n",
    "        X_test_ram = X_test\n",
    "        _stage('[AUDITOR] WARNING: could not materialise X_test (MemoryError); using memmap (slower).')\n",
    "\n",
    "    # -----------------------------\n",
    "    # Fold streaming helpers\n",
    "    # -----------------------------\n",
    "    def _fmt_seconds(sec: float) -> str:\n",
    "        try:\n",
    "            sec = float(sec)\n",
    "        except Exception:\n",
    "            return '?'\n",
    "        if not np.isfinite(sec) or sec < 0:\n",
    "            return '?'\n",
    "        total = int(sec + 0.5)\n",
    "        m, s = divmod(total, 60)\n",
    "        h, m = divmod(m, 60)\n",
    "        if h > 0:\n",
    "            return f'{h}h{m:02d}m'\n",
    "        if m > 0:\n",
    "            return f'{m}m{s:02d}s'\n",
    "        return f'{s}s'\n",
    "\n",
    "    def _start_heartbeat(tag: str, every_s: float = 60.0):\n",
    "        # Prints a periodic line while a long operation runs (e.g., cuML fit can be silent).\n",
    "        stop = threading.Event()\n",
    "        t0 = time.time()\n",
    "\n",
    "        def _loop():\n",
    "            while not stop.wait(float(every_s)):\n",
    "                _stage(f\"[HEARTBEAT] {tag} (elapsed={_fmt_seconds(time.time() - t0)})\")\n",
    "\n",
    "        th = threading.Thread(target=_loop, daemon=True)\n",
    "        th.start()\n",
    "        return stop, th\n",
    "\n",
    "    def _iter_indexed_rows(src_mm, idx: np.ndarray, bs: int, desc: str):\n",
    "        for i in tqdm(range(0, int(len(idx)), int(bs)), desc=desc, unit='batch', leave=False):\n",
    "            j = min(i + int(bs), int(len(idx)))\n",
    "            rows = idx[i:j]\n",
    "            xb = np.asarray(src_mm[rows], dtype=np.float32)\n",
    "            yield i, j, xb\n",
    "\n",
    "    def _fit_scaler_from_indexed_rows(src_mm, idx: np.ndarray, bs: int, desc: str):\n",
    "        scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "        for _i, _j, xb in _iter_indexed_rows(src_mm, idx, bs=bs, desc=desc):\n",
    "            scaler.partial_fit(xb)\n",
    "        return {'mean': scaler.mean_.astype(np.float32), 'scale': scaler.scale_.astype(np.float32)}\n",
    "\n",
    "    def _alloc_and_fill_gpu_from_indexed_rows(src_mm, idx: np.ndarray, mean: np.ndarray, scale: np.ndarray, bs: int, desc: str):\n",
    "        n_rows = int(len(idx))\n",
    "        n_cols = int(src_mm.shape[1])\n",
    "        out = cp.empty((n_rows, n_cols), dtype=cp.float32)\n",
    "        mean_gpu = cp.asarray(mean)\n",
    "        scale_gpu = cp.asarray(scale)\n",
    "        for i, j, xb in _iter_indexed_rows(src_mm, idx, bs=bs, desc=desc):\n",
    "            xg = cp.asarray(xb)\n",
    "            xg = (xg - mean_gpu) / (scale_gpu + 1e-12)\n",
    "            out[i:j, :] = xg\n",
    "            del xb, xg\n",
    "            try:\n",
    "                cp.get_default_memory_pool().free_all_blocks()\n",
    "                cp.get_default_pinned_memory_pool().free_all_blocks()\n",
    "            except Exception:\n",
    "                pass\n",
    "        return out, mean_gpu, scale_gpu\n",
    "\n",
    "    def _to_numpy(a):\n",
    "        if hasattr(a, 'get'):\n",
    "            return a.get()\n",
    "        if hasattr(a, 'to_numpy'):\n",
    "            return a.to_numpy()\n",
    "        return np.asarray(a)\n",
    "\n",
    "    def _sigmoid_np(z):\n",
    "        z = np.asarray(z, dtype=np.float32)\n",
    "        z = np.clip(z, -50.0, 50.0)\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "    def _ensure_2d(p):\n",
    "        p = np.asarray(p)\n",
    "        if p.ndim == 1:\n",
    "            p = p.reshape(-1, 1)\n",
    "        return p\n",
    "\n",
    "    def _predict_proba_like(clf, x, expect_cols: int | None = None):\n",
    "        # RAPIDS/cuML OVR may not implement predict_proba; fall back to decision_function->sigmoid.\n",
    "        if hasattr(clf, 'predict_proba'):\n",
    "            p = _ensure_2d(_to_numpy(clf.predict_proba(x)))\n",
    "            # Binary single-label can come back as (n,2); we need P(class=1)\n",
    "            if expect_cols == 1 and p.shape[1] == 2:\n",
    "                p = p[:, 1:2]\n",
    "            return np.asarray(p, dtype=np.float32)\n",
    "        if hasattr(clf, 'decision_function'):\n",
    "            s = _ensure_2d(_to_numpy(clf.decision_function(x)))\n",
    "            return np.asarray(_sigmoid_np(s), dtype=np.float32)\n",
    "        # Last resort: hard predictions (0/1)\n",
    "        p = _ensure_2d(_to_numpy(clf.predict(x)))\n",
    "        return np.asarray(p, dtype=np.float32)\n",
    "\n",
    "    def _predict_proba_gpu_batched(clf, src_mm, idx: np.ndarray, mean_gpu, scale_gpu, bs: int, out_np: np.ndarray, out_rows: np.ndarray, col_slice: slice, desc: str):\n",
    "        expect_cols = None\n",
    "        try:\n",
    "            if (col_slice is not None) and (col_slice.start is not None) and (col_slice.stop is not None):\n",
    "                expect_cols = int(col_slice.stop - col_slice.start)\n",
    "        except Exception:\n",
    "            expect_cols = None\n",
    "        for i, j, xb in _iter_indexed_rows(src_mm, idx, bs=bs, desc=desc):\n",
    "            xg = cp.asarray(xb)\n",
    "            xg = (xg - mean_gpu) / (scale_gpu + 1e-12)\n",
    "            p = _predict_proba_like(clf, xg, expect_cols=expect_cols)\n",
    "            out_np[out_rows[i:j], col_slice] = p\n",
    "            del xb, xg, p\n",
    "            try:\n",
    "                cp.get_default_memory_pool().free_all_blocks()\n",
    "                cp.get_default_pinned_memory_pool().free_all_blocks()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    for aspect in aspects:\n",
    "        _stage(f\"\\n=== LogReg Aspect: {aspect} ===\")\n",
    "\n",
    "        aspect_indices = [i for i, t in enumerate(top_terms) if _aspect_of_term(t) == aspect]\n",
    "        if not aspect_indices:\n",
    "            raise RuntimeError(f\"No terms found for aspect {aspect} within top_terms_13500.json\")\n",
    "\n",
    "        aspect_indices = np.asarray(aspect_indices, dtype=np.int64)\n",
    "        aspect_indices_map[aspect] = aspect_indices\n",
    "        n_targets = int(aspect_indices.shape[0])\n",
    "        aspect_terms = [top_terms[i] for i in aspect_indices.tolist()]\n",
    "\n",
    "        # Slice IA weights for diagnostics only\n",
    "        weights_aspect = weights_full[aspect_indices] if weights_full is not None else None\n",
    "\n",
    "        # Persist per-aspect term contract\n",
    "        (PRED_DIR / f'top_terms_{aspect}.json').write_text(json.dumps(aspect_terms), encoding='utf-8')\n",
    "\n",
    "        # Per-aspect artefacts\n",
    "        lr_oof_path = PRED_DIR / f'oof_pred_logreg_{aspect}.npy'\n",
    "        lr_test_path = PRED_DIR / f'test_pred_logreg_{aspect}.npy'\n",
    "        marker_dir = PRED_DIR / f'fold_markers_logreg_{aspect}'\n",
    "        marker_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Done check\n",
    "        if lr_oof_path.exists() and lr_test_path.exists() and (len(list(marker_dir.glob('fold_*_done.flag'))) == n_splits):\n",
    "            _stage(f\"[AUDITOR] {aspect}: all folds completed; loading from disk\")\n",
    "            oof_pred_logreg_by_aspect[aspect] = np.load(lr_oof_path, mmap_mode='r')\n",
    "            test_pred_logreg_by_aspect[aspect] = np.load(lr_test_path, mmap_mode='r')\n",
    "            continue\n",
    "\n",
    "        # Prepare memmaps for this aspect\n",
    "        mode_oof = 'r+' if lr_oof_path.exists() else 'w+'\n",
    "        mode_test = 'r+' if lr_test_path.exists() else 'w+'\n",
    "\n",
    "        oof_pred = np.lib.format.open_memmap(str(lr_oof_path), mode=mode_oof, dtype=np.float32, shape=(X.shape[0], n_targets))\n",
    "        test_pred = np.lib.format.open_memmap(str(lr_test_path), mode=mode_test, dtype=np.float32, shape=(X_test.shape[0], n_targets))\n",
    "        if mode_oof == 'w+':\n",
    "            oof_pred[:] = 0.0\n",
    "            oof_pred.flush()\n",
    "        if mode_test == 'w+':\n",
    "            test_pred[:] = 0.0\n",
    "            test_pred.flush()\n",
    "\n",
    "        fold_iter = tqdm(kf.split(np.arange(X.shape[0])), total=kf.get_n_splits(), desc=f'LogReg {aspect} folds', unit='fold')\n",
    "        fold_wall_s = []\n",
    "        already_done = int(len(list(marker_dir.glob('fold_*_done.flag'))))\n",
    "\n",
    "        for fold, (idx_tr, idx_val) in enumerate(fold_iter):\n",
    "            marker_path = marker_dir / f'fold_{fold}_done.flag'\n",
    "            if marker_path.exists():\n",
    "                _stage(f\"{aspect} Fold {fold+1}/{n_splits} already completed (marker found). Skipping.\")\n",
    "                continue\n",
    "\n",
    "            t0_fold = time.time()\n",
    "            _stage(f\"{aspect} Fold {fold+1}/{n_splits}\")\n",
    "            _log_mem(f\"{aspect} fold {fold+1} start\")\n",
    "            _log_gpu_mem(f\"{aspect} fold {fold+1} start\")\n",
    "\n",
    "            idx_tr = np.asarray(idx_tr, dtype=np.int64)\n",
    "            idx_val = np.asarray(idx_val, dtype=np.int64)\n",
    "\n",
    "            # Performance: materialise fold validation rows into RAM once.\n",
    "            # This avoids re-reading X (disk-backed) for every target-chunk during predict_proba.\n",
    "            try:\n",
    "                X_val_ram = np.ascontiguousarray(np.asarray(X[idx_val], dtype=np.float32))\n",
    "                val_src = X_val_ram\n",
    "                val_rows = np.arange(int(len(idx_val)), dtype=np.int64)\n",
    "                _stage(f'[AUDITOR] {aspect} Fold {fold+1}: X_val_ram in RAM: {X_val_ram.shape}')\n",
    "            except MemoryError:\n",
    "                val_src = X\n",
    "                val_rows = idx_val\n",
    "                _stage(f'[AUDITOR] {aspect} Fold {fold+1}: WARNING: could not materialise X_val (MemoryError); using memmap (slower).')\n",
    "\n",
    "            # Fit scaler by streaming fold rows from the global memmap (no fold copies).\n",
    "            scaler_path = PRED_DIR / f'logreg_scaler_{aspect}_fold{fold}.pkl'\n",
    "            if scaler_path.exists():\n",
    "                _stage(f\"[AUDIT] {aspect} Fold {fold+1}: Found existing scaler. Loading.\")\n",
    "                scaler_state = joblib.load(scaler_path)\n",
    "            else:\n",
    "                t0 = time.time()\n",
    "                scaler_state = _fit_scaler_from_indexed_rows(X, idx_tr, bs=2048, desc=f'{aspect} Fold {fold+1} Fit Scaler')\n",
    "                joblib.dump(scaler_state, scaler_path)\n",
    "                _stage(f\"[AUDIT] {aspect} Fold {fold+1}: Scaler fit in {time.time() - t0:.1f}s\")\n",
    "\n",
    "            mean = scaler_state['mean']\n",
    "            scale = scaler_state['scale']\n",
    "\n",
    "            # Performance: pre-scale X_test ONCE per fold (math-equivalent; avoids repeating same scaling per target-chunk).\n",
    "            test_src = X_test_ram\n",
    "            test_prescaled = False\n",
    "            try:\n",
    "                _stage(f'[AUDITOR] {aspect} Fold {fold+1}: pre-scaling X_test for this fold...')\n",
    "                X_test_scaled = np.empty((int(X_test_ram.shape[0]), int(X_test_ram.shape[1])), dtype=np.float32)\n",
    "                for b0 in tqdm(\n",
    "                    range(0, int(X_test_ram.shape[0]), int(TEST_BS)),\n",
    "                    desc=f'{aspect} Fold {fold+1} pre-scale X_test',\n",
    "                    unit='batch',\n",
    "                    leave=False,\n",
    "                ):\n",
    "                    b1 = min(b0 + int(TEST_BS), int(X_test_ram.shape[0]))\n",
    "                    xb = np.asarray(X_test_ram[b0:b1], dtype=np.float32)\n",
    "                    xb = (xb - mean) / (scale + 1e-12)\n",
    "                    X_test_scaled[b0:b1, :] = xb\n",
    "                    del xb\n",
    "                test_src = X_test_scaled\n",
    "                test_prescaled = True\n",
    "                _stage(f'[AUDITOR] {aspect} Fold {fold+1}: X_test pre-scaled: shape={X_test_scaled.shape} dtype={X_test_scaled.dtype}')\n",
    "            except MemoryError:\n",
    "                test_src = X_test_ram\n",
    "                test_prescaled = False\n",
    "                _stage(f'[AUDITOR] {aspect} Fold {fold+1}: WARNING: X_test pre-scale skipped (MemoryError); scaling per batch.')\n",
    "            except Exception as e:\n",
    "                test_src = X_test_ram\n",
    "                test_prescaled = False\n",
    "                _stage(f'[AUDITOR] {aspect} Fold {fold+1}: WARNING: X_test pre-scale failed; scaling per batch: {e!r}')\n",
    "            _log_mem(f\"{aspect} fold {fold+1} post scaler\")\n",
    "            _log_gpu_mem(f\"{aspect} fold {fold+1} post scaler\")\n",
    "\n",
    "            # GPU selection: only if we can afford the full training fold on GPU.\n",
    "            use_gpu = bool(HAS_RAPIDS and _gpu_mem_okay(int(len(idx_tr)), int(X.shape[1])))\n",
    "            gpu_success = False\n",
    "\n",
    "            if use_gpu:\n",
    "                try:\n",
    "                    _stage(f\"[AUDITOR] {aspect} Fold {fold+1}: Using RAPIDS/cuML\")\n",
    "\n",
    "                    _stage(f\"[AUDIT] {aspect} Fold {fold+1}: PRE X_tr_gpu build\")\n",
    "                    _log_mem(f\"{aspect} pre X_tr_gpu\")\n",
    "                    _log_gpu_mem(f\"{aspect} pre X_tr_gpu\")\n",
    "\n",
    "                    # Build X_tr on GPU by streaming scaled batches (avoid full host materialisation).\n",
    "                    X_tr_gpu, mean_gpu, scale_gpu = _alloc_and_fill_gpu_from_indexed_rows(\n",
    "                        X,\n",
    "                        idx_tr,\n",
    "                        mean=mean,\n",
    "                        scale=scale,\n",
    "                        bs=1024,\n",
    "                        desc=f'{aspect} Fold {fold+1} ->GPU X_tr',\n",
    "                    )\n",
    "\n",
    "                    _stage(f\"[AUDIT] {aspect} Fold {fold+1}: POST X_tr_gpu build\")\n",
    "                    _log_mem(f\"{aspect} post X_tr_gpu\")\n",
    "                    _log_gpu_mem(f\"{aspect} post X_tr_gpu\")\n",
    "\n",
    "                    # Performance: keep full X_test on GPU for the entire fold when possible.\n",
    "                    # This removes repeated host->device transfers for every target-chunk.\n",
    "                    X_test_gpu = None\n",
    "                    try:\n",
    "                        _stage(f\"[AUDITOR] {aspect} Fold {fold+1}: attempting to keep X_test on GPU (fold-scoped)\")\n",
    "                        _log_gpu_mem(f\"{aspect} pre X_test_gpu\")\n",
    "                        n_te = int(test_src.shape[0])\n",
    "                        n_cols = int(test_src.shape[1])\n",
    "                        X_test_gpu = cp.empty((n_te, n_cols), dtype=cp.float32)\n",
    "                        for b0 in tqdm(\n",
    "                            range(0, n_te, int(TEST_BS)),\n",
    "                            desc=f'{aspect} Fold {fold+1} ->GPU X_test',\n",
    "                            unit='batch',\n",
    "                            leave=False,\n",
    "                        ):\n",
    "                            b1 = min(b0 + int(TEST_BS), n_te)\n",
    "                            xb = np.asarray(test_src[b0:b1], dtype=np.float32)\n",
    "                            xg = cp.asarray(xb)\n",
    "                            if not test_prescaled:\n",
    "                                xg = (xg - mean_gpu) / (scale_gpu + 1e-12)\n",
    "                            X_test_gpu[b0:b1, :] = xg\n",
    "                            del xb, xg\n",
    "                        _stage(f\"[AUDITOR] {aspect} Fold {fold+1}: X_test_gpu ready: shape={tuple(X_test_gpu.shape)} dtype={X_test_gpu.dtype}\")\n",
    "                        _log_gpu_mem(f\"{aspect} post X_test_gpu\")\n",
    "                    except Exception as e:\n",
    "                        X_test_gpu = None\n",
    "                        _stage(f\"[AUDITOR] {aspect} Fold {fold+1}: WARNING: X_test not kept on GPU; streaming per chunk. Reason: {e!r}\")\n",
    "                        _log_gpu_mem(f\"{aspect} after X_test_gpu fail\")\n",
    "\n",
    "                    n_chunks = int((n_targets + TARGET_CHUNK - 1) // TARGET_CHUNK)\n",
    "                    chunk_total_s = []\n",
    "                    chunk_fit_s = []\n",
    "                    chunk_val_s = []\n",
    "                    chunk_test_s = []\n",
    "\n",
    "                    pbar = tqdm(\n",
    "                        range(0, n_targets, TARGET_CHUNK),\n",
    "                        total=n_chunks,\n",
    "                        desc=f'{aspect} Fold {fold+1} target chunks',\n",
    "                        unit='chunk',\n",
    "                        leave=False,\n",
    "                    )\n",
    "\n",
    "                    for start in pbar:\n",
    "                        t0_chunk = time.time()\n",
    "                        end = min(start + TARGET_CHUNK, n_targets)\n",
    "\n",
    "                        if start == 0:\n",
    "                            _stage(f\"[AUDIT] {aspect} Fold {fold+1} chunk0 PRE Y_tr_chunk\")\n",
    "                            _log_mem(f\"{aspect} chunk0 pre Y\")\n",
    "                            _log_gpu_mem(f\"{aspect} chunk0 pre Y\")\n",
    "\n",
    "                        cols = aspect_indices[start:end]\n",
    "                        y_host = np.asarray(Y_full[np.ix_(idx_tr, cols)], dtype=np.float32)\n",
    "                        Y_tr_chunk = cp.asarray(y_host)\n",
    "\n",
    "                        if start == 0:\n",
    "                            _stage(f\"[AUDIT] {aspect} Fold {fold+1} chunk0 PRE fit\")\n",
    "                            _log_mem(f\"{aspect} chunk0 pre fit\")\n",
    "                            _log_gpu_mem(f\"{aspect} chunk0 pre fit\")\n",
    "\n",
    "                        clf_chunk = cuOVR(cuLogReg(solver='qn', penalty='l2', C=1.0, max_iter=2000, tol=1e-3))\n",
    "                        chunk_i = int(start // TARGET_CHUNK) + 1\n",
    "                        hb_stop, hb_thr = _start_heartbeat(f'{aspect} Fold {fold+1} chunk {chunk_i}/{n_chunks} cuML fit', every_s=60.0)\n",
    "                        t0 = time.time()\n",
    "                        try:\n",
    "                            clf_chunk.fit(X_tr_gpu, Y_tr_chunk)\n",
    "                        finally:\n",
    "                            hb_stop.set()\n",
    "                            try:\n",
    "                                hb_thr.join(timeout=2.0)\n",
    "                            except Exception:\n",
    "                                pass\n",
    "                        fit_s = time.time() - t0\n",
    "\n",
    "                        if start == 0:\n",
    "                            _stage(f\"[AUDIT] {aspect} Fold {fold+1} chunk0 POST fit\")\n",
    "                            _log_mem(f\"{aspect} chunk0 post fit\")\n",
    "                            _log_gpu_mem(f\"{aspect} chunk0 post fit\")\n",
    "\n",
    "                        # Validation probs (batched) -> write directly into oof memmap\n",
    "                        t0 = time.time()\n",
    "                        _predict_proba_gpu_batched(\n",
    "                            clf_chunk,\n",
    "                            val_src,\n",
    "                            val_rows,\n",
    "                            mean_gpu,\n",
    "                            scale_gpu,\n",
    "                            bs=VAL_BS,\n",
    "                            out_np=oof_pred,\n",
    "                            out_rows=idx_val,\n",
    "                            col_slice=slice(start, end),\n",
    "                            desc=f'{aspect} Fold {fold+1} val proba',\n",
    "                        )\n",
    "                        val_s = time.time() - t0\n",
    "\n",
    "                        # Test probs (batched) -> accumulate\n",
    "                        t0 = time.time()\n",
    "                        if X_test_gpu is not None:\n",
    "                            # Fast path: test features already on GPU (no repeated host->device copies).\n",
    "                            for b0 in range(0, int(test_src.shape[0]), TEST_BS):\n",
    "                                b1 = min(b0 + TEST_BS, int(test_src.shape[0]))\n",
    "                                xb_gpu = X_test_gpu[b0:b1, :]\n",
    "                                p_te = _predict_proba_like(clf_chunk, xb_gpu, expect_cols=int(end - start))\n",
    "                                test_pred[b0:b1, start:end] += (np.asarray(p_te, dtype=np.float32) / float(n_splits))\n",
    "                                del xb_gpu, p_te\n",
    "                        else:\n",
    "                            # Fallback: stream from host per batch (slower; keeps VRAM lower).\n",
    "                            for b0 in range(0, int(test_src.shape[0]), TEST_BS):\n",
    "                                b1 = min(b0 + TEST_BS, int(test_src.shape[0]))\n",
    "                                xb = np.asarray(test_src[b0:b1], dtype=np.float32)\n",
    "                                xb_gpu = cp.asarray(xb)\n",
    "                                if not test_prescaled:\n",
    "                                    xb_gpu = (xb_gpu - mean_gpu) / (scale_gpu + 1e-12)\n",
    "\n",
    "                                p_te = _predict_proba_like(clf_chunk, xb_gpu, expect_cols=int(end - start))\n",
    "                                test_pred[b0:b1, start:end] += (np.asarray(p_te, dtype=np.float32) / float(n_splits))\n",
    "                                del xb, xb_gpu, p_te\n",
    "                        test_s = time.time() - t0\n",
    "\n",
    "                        del y_host, Y_tr_chunk, clf_chunk\n",
    "                        try:\n",
    "                            cp.get_default_memory_pool().free_all_blocks()\n",
    "                            cp.get_default_pinned_memory_pool().free_all_blocks()\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                        gc.collect()\n",
    "                        total_s = time.time() - t0_chunk\n",
    "                        chunk_fit_s.append(float(fit_s))\n",
    "                        chunk_val_s.append(float(val_s))\n",
    "                        chunk_test_s.append(float(test_s))\n",
    "                        chunk_total_s.append(float(total_s))\n",
    "\n",
    "                        # ETA: update tqdm postfix every chunk (smoothed after >=2 chunks).\n",
    "                        chunk_i = int(start // TARGET_CHUNK) + 1\n",
    "                        if len(chunk_total_s) >= 2:\n",
    "                            recent = chunk_total_s[-min(5, len(chunk_total_s)):]\n",
    "                            # Drop the very first chunk from ETA if we have enough signal.\n",
    "                            if len(chunk_total_s) >= 3:\n",
    "                                recent = chunk_total_s[-min(5, len(chunk_total_s) - 1):]\n",
    "                            avg_s = float(np.mean(recent)) if recent else float(chunk_total_s[-1])\n",
    "                            eta_fold_s = avg_s * float(n_chunks - chunk_i)\n",
    "                            eta_txt = _fmt_seconds(eta_fold_s)\n",
    "                            rate_txt = _fmt_seconds(avg_s) + '/chunk'\n",
    "                        else:\n",
    "                            eta_txt = '?'\n",
    "                            rate_txt = '?'\n",
    "                        try:\n",
    "                            pbar.set_postfix_str(f'avg~{rate_txt} ETA~{eta_txt}')\n",
    "                        except Exception:\n",
    "                            pass\n",
    "\n",
    "                        # Progress: first few chunks are often slow (warmup/JIT).\n",
    "                        if chunk_i <= 3 or (chunk_i % 10 == 0) or (chunk_i == n_chunks):\n",
    "                            _stage(f\"[PROGRESS] {aspect} Fold {fold+1}: chunk {chunk_i}/{n_chunks} fit={_fmt_seconds(fit_s)} val={_fmt_seconds(val_s)} test={_fmt_seconds(test_s)} total={_fmt_seconds(total_s)} avg~{rate_txt} ETA~{eta_txt}\")\n",
    "\n",
    "                    try:\n",
    "                        del X_test_gpu\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                    del X_tr_gpu, mean_gpu, scale_gpu\n",
    "                    try:\n",
    "                        cp.get_default_memory_pool().free_all_blocks()\n",
    "                        cp.get_default_pinned_memory_pool().free_all_blocks()\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                    gc.collect()\n",
    "                    gpu_success = True\n",
    "\n",
    "                except Exception as e:\n",
    "                    _stage(f\"[CRITICAL] {aspect} Fold {fold+1}: GPU Training Failed (likely OOM): {e}\")\n",
    "                    _stage('[AUDITOR] Cleaning up GPU memory and falling back to CPU...')\n",
    "                    gpu_success = False\n",
    "                    if HAS_RAPIDS and cp is not None:\n",
    "                        try:\n",
    "                            cp.get_default_memory_pool().free_all_blocks()\n",
    "                            cp.get_default_pinned_memory_pool().free_all_blocks()\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                    gc.collect()\n",
    "\n",
    "            if (not use_gpu) or (not gpu_success):\n",
    "                _stage(f\"[AUDITOR] {aspect} Fold {fold+1}: Using CPU SGD (fallback)\")\n",
    "\n",
    "                # CPU fallback: build one scaled X_tr memmap, then fit/predict target chunks.\n",
    "                X_trs_path = tmp_dir / f'X_tr_scaled_{aspect}_fold{fold}.npy'\n",
    "                if not X_trs_path.exists():\n",
    "                    mm = np.lib.format.open_memmap(str(X_trs_path), mode='w+', dtype=np.float32, shape=(len(idx_tr), X.shape[1]))\n",
    "                    for i, j, xb in _iter_indexed_rows(X, idx_tr, bs=2048, desc=f'{aspect} Fold {fold+1} scale X_tr (CPU)'):\n",
    "                        xb = (xb - mean) / (scale + 1e-12)\n",
    "                        mm[i:j, :] = xb\n",
    "                    mm.flush()\n",
    "                    del mm\n",
    "                    gc.collect()\n",
    "\n",
    "                X_trs = np.load(X_trs_path, mmap_mode='r')\n",
    "\n",
    "                for start in tqdm(\n",
    "                    range(0, n_targets, TARGET_CHUNK),\n",
    "                    total=(n_targets + TARGET_CHUNK - 1) // TARGET_CHUNK,\n",
    "                    desc=f'{aspect} Fold {fold+1} CPU target chunks',\n",
    "                    unit='chunk',\n",
    "                    leave=False,\n",
    "                ):\n",
    "                    end = min(start + TARGET_CHUNK, n_targets)\n",
    "                    cols = aspect_indices[start:end]\n",
    "                    Y_tr_chunk = np.asarray(Y_full[np.ix_(idx_tr, cols)], dtype=np.float32)\n",
    "\n",
    "                    # NOTE: keep n_jobs=1 to avoid loky subprocesses importing torch (can crash on CUDA mismatches).\n",
    "                    clf_logreg = OneVsRestClassifier(\n",
    "                        SGDClassifier(loss='log_loss', penalty='l2', alpha=0.0001, max_iter=1, tol=None, n_jobs=1),\n",
    "                        n_jobs=1,\n",
    "                    )\n",
    "                    clf_logreg.fit(X_trs, Y_tr_chunk)\n",
    "\n",
    "                    # Validation preds (batched, scale on the fly)\n",
    "                    for i, j, xb in _iter_indexed_rows(val_src, val_rows, bs=VAL_BS, desc=f'{aspect} Fold {fold+1} val predict (CPU)'):\n",
    "                        xb = (xb - mean) / (scale + 1e-12)\n",
    "                        pb = _predict_proba_like(clf_logreg, xb, expect_cols=int(end - start)).astype(np.float32)\n",
    "                        oof_pred[idx_val[i:j], start:end] = pb\n",
    "\n",
    "                    # Test preds (batched)\n",
    "                    for b0 in tqdm(range(0, int(test_src.shape[0]), TEST_BS), desc=f'{aspect} Fold {fold+1} test predict (CPU)', unit='batch', leave=False):\n",
    "                        b1 = min(b0 + TEST_BS, int(test_src.shape[0]))\n",
    "                        xb = np.asarray(test_src[b0:b1], dtype=np.float32)\n",
    "                        if not test_prescaled:\n",
    "                            xb = (xb - mean) / (scale + 1e-12)\n",
    "                        pb = _predict_proba_like(clf_logreg, xb, expect_cols=int(end - start)).astype(np.float32)\n",
    "                        test_pred[b0:b1, start:end] += pb / float(n_splits)\n",
    "\n",
    "                    del Y_tr_chunk, clf_logreg\n",
    "                    gc.collect()\n",
    "\n",
    "                del X_trs\n",
    "                gc.collect()\n",
    "\n",
    "                # Cleanup temp scaled fold\n",
    "                try:\n",
    "                    os.remove(X_trs_path)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            # Fold diagnostics (sampled, capped columns to avoid huge allocations)\n",
    "            try:\n",
    "                sample_n = int(min(20000, len(idx_val)))\n",
    "                sample_k = int(min(2000, n_targets))\n",
    "                if sample_n > 0 and sample_k > 0:\n",
    "                    sample_probs = np.asarray(oof_pred[idx_val[:sample_n], :sample_k], dtype=np.float32)\n",
    "                    cols = aspect_indices[:sample_k]\n",
    "                    sample_true = np.asarray(Y_full[np.ix_(idx_val[:sample_n], cols)], dtype=np.float32)\n",
    "\n",
    "                    best_f1 = 0.0\n",
    "                    best_thr = 0.0\n",
    "                    for thr in np.linspace(0.01, 0.20, 20):\n",
    "                        vp = (sample_probs > thr).astype(np.int8)\n",
    "                        score = f1_score(sample_true, vp, average='micro')\n",
    "                        if score > best_f1:\n",
    "                            best_f1, best_thr = score, float(thr)\n",
    "\n",
    "                    _stage(f\"  >> {aspect} Fold {fold+1} (sample) micro-F1={best_f1:.4f} best_thr={best_thr:.2f} (k={sample_k})\")\n",
    "            except Exception as e:\n",
    "                _stage('  [WARNING] Diagnostics skipped: ' + repr(e))\n",
    "\n",
    "            oof_pred.flush()\n",
    "            test_pred.flush()\n",
    "            marker_path.touch()\n",
    "            _stage(f\"{aspect} Fold {fold+1} completed and flushed.\")\n",
    "            _stage(f\"[TIMER] {aspect} Fold {fold+1} wall: {time.time() - t0_fold:.1f}s\")\n",
    "            fold_wall_s.append(float(time.time() - t0_fold))\n",
    "            done_now = int(already_done + len(fold_wall_s))\n",
    "            left = max(0, int(n_splits - done_now))\n",
    "            recent = fold_wall_s[-min(3, len(fold_wall_s)):]\n",
    "            avg_fold = float(np.mean(recent)) if recent else float(fold_wall_s[-1])\n",
    "            try:\n",
    "                fold_iter.set_postfix_str(f'avg~{_fmt_seconds(avg_fold)}/fold ETA~{_fmt_seconds(avg_fold * left)}')\n",
    "            except Exception:\n",
    "                pass\n",
    "            _log_mem(f\"{aspect} fold {fold+1} end\")\n",
    "            _log_gpu_mem(f\"{aspect} fold {fold+1} end\")\n",
    "\n",
    "        oof_pred.flush()\n",
    "        test_pred.flush()\n",
    "        del oof_pred, test_pred\n",
    "        gc.collect()\n",
    "\n",
    "        oof_pred_logreg_by_aspect[aspect] = np.load(lr_oof_path, mmap_mode='r')\n",
    "        test_pred_logreg_by_aspect[aspect] = np.load(lr_test_path, mmap_mode='r')\n",
    "        _stage(f\"[AUDITOR] {aspect}: saved {lr_oof_path.name}, {lr_test_path.name}\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Backwards compatibility: combined outputs are only assembled once BP+MF+CC exist.\n",
    "    # -----------------------------\n",
    "    if set(['BP', 'MF', 'CC']).issubset(set(oof_pred_logreg_by_aspect.keys())):\n",
    "        lr_oof_full_path = PRED_DIR / 'oof_pred_logreg.npy'\n",
    "        lr_test_full_path = PRED_DIR / 'test_pred_logreg.npy'\n",
    "\n",
    "        mode_oof = 'r+' if lr_oof_full_path.exists() else 'w+'\n",
    "        mode_test = 'r+' if lr_test_full_path.exists() else 'w+'\n",
    "        oof_full = np.lib.format.open_memmap(str(lr_oof_full_path), mode=mode_oof, dtype=np.float32, shape=(X.shape[0], len(top_terms)))\n",
    "        te_full = np.lib.format.open_memmap(str(lr_test_full_path), mode=mode_test, dtype=np.float32, shape=(X_test.shape[0], len(top_terms)))\n",
    "        if mode_oof == 'w+':\n",
    "            oof_full[:] = 0.0\n",
    "        if mode_test == 'w+':\n",
    "            te_full[:] = 0.0\n",
    "\n",
    "        for asp in ['BP', 'MF', 'CC']:\n",
    "            idx = aspect_indices_map[asp]\n",
    "            oof_full[:, idx] = np.asarray(oof_pred_logreg_by_aspect[asp], dtype=np.float32)\n",
    "            te_full[:, idx] = np.asarray(test_pred_logreg_by_aspect[asp], dtype=np.float32)\n",
    "\n",
    "        oof_full.flush()\n",
    "        te_full.flush()\n",
    "        del oof_full, te_full\n",
    "        gc.collect()\n",
    "\n",
    "        oof_pred_logreg = np.load(lr_oof_full_path, mmap_mode='r')\n",
    "        test_pred_logreg = np.load(lr_test_full_path, mmap_mode='r')\n",
    "        _stage(f\"[AUDITOR] Combined preds saved: {lr_oof_full_path.name}, {lr_test_full_path.name}\")\n",
    "    else:\n",
    "        oof_pred_logreg = None\n",
    "        test_pred_logreg = None\n",
    "\n",
    "    # Final checkpoint push (per-aspect; combined only if present)\n",
    "    if 'STORE' in globals() and STORE is not None:\n",
    "        required = [str(top_terms_path.as_posix())]\n",
    "        for asp in oof_pred_logreg_by_aspect.keys():\n",
    "            required += [\n",
    "                str((PRED_DIR / f'oof_pred_logreg_{asp}.npy').as_posix()),\n",
    "                str((PRED_DIR / f'test_pred_logreg_{asp}.npy').as_posix()),\n",
    "                str((PRED_DIR / f'top_terms_{asp}.json').as_posix()),\n",
    "            ]\n",
    "        if oof_pred_logreg is not None and test_pred_logreg is not None:\n",
    "            required += [\n",
    "                str((PRED_DIR / 'oof_pred_logreg.npy').as_posix()),\n",
    "                str((PRED_DIR / 'test_pred_logreg.npy').as_posix()),\n",
    "            ]\n",
    "\n",
    "        try:\n",
    "            STORE.maybe_push(\n",
    "                stage='stage_07a_level1_logreg_aspect_split',\n",
    "                required_paths=required,\n",
    "                note='Level-1 Logistic Regression predictions (OOF + test), split by GO aspect (BP/MF/CC).',\n",
    "            )\n",
    "        except Exception as e:\n",
    "            _stage(f\"[WARN] STORE push failed: {e}\")\n",
    "\n",
    "    _stage(f\"[TIMER] LogReg total wall: {time.time() - t0_all:.1f}s\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
