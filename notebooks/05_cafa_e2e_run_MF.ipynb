{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faae05b1",
   "metadata": {},
   "source": [
    "# 05 CAFA E2E — Run LogReg for MF (wrapper)\n",
    "\n",
    "Self-contained runner: executes `notebooks/05_cafa_e2e.ipynb` with `TARGET_ASPECT=MF`, stopping before the DNN cell.\n",
    "\n",
    "Key behaviour:\n",
    "- No subprocess calls\n",
    "- Logging-first diagnostics (search paths, stop-marker, failures)\n",
    "- Produces per-aspect artefacts: `oof_pred_logreg_MF.npy`, `test_pred_logreg_MF.npy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b933e2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 01 - Setup (NO REPO)\n",
    "import os\n",
    "import sys\n",
    "import ctypes\n",
    "from pathlib import Path\n",
    "\n",
    "# CUDA loader fix (PyTorch/RAPIDS coexistence): preload venv nvjitlink so we don't pick /usr/local/cuda/lib64\n",
    "try:\n",
    "    _venv_root = Path(sys.executable).resolve().parent.parent\n",
    "    _nvjit_dir = (\n",
    "        _venv_root\n",
    "        / \"lib\"\n",
    "        / f\"python{sys.version_info.major}.{sys.version_info.minor}\"\n",
    "        / \"site-packages\"\n",
    "        / \"nvidia\"\n",
    "        / \"nvjitlink\"\n",
    "        / \"lib\"\n",
    "    )\n",
    "    _nvjit_so = _nvjit_dir / \"libnvJitLink.so.12\"\n",
    "    if _nvjit_so.exists():\n",
    "        ctypes.CDLL(str(_nvjit_so), mode=ctypes.RTLD_GLOBAL)\n",
    "        os.environ[\"LD_LIBRARY_PATH\"] = f\"{_nvjit_dir}:{os.environ.get('LD_LIBRARY_PATH','')}\"\n",
    "        print(f\"[ENV] Preloaded nvjitlink: {_nvjit_so}\")\n",
    "except Exception as _e:\n",
    "    print(f\"[ENV] nvjitlink preload skipped: {_e}\")\n",
    "\n",
    "# Always run from a simple writable location; never cd into a repo.\n",
    "if os.path.exists('/content'):\n",
    "    os.chdir('/content')\n",
    "RUNTIME_ROOT = Path.cwd()\n",
    "DATA_ROOT = (RUNTIME_ROOT / 'cafa6_data')\n",
    "DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "TRAIN_LEVEL1 = True\n",
    "print(f'CWD: {Path.cwd()}')\n",
    "print(f'DATA_ROOT: {DATA_ROOT.resolve()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add96f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 13a - Setup & Data Loading (Phase 2 canonical)\n",
    "# =============================================\n",
    "# 4. PHASE 2: LEVEL-1 MODELS (DIVERSE ENSEMBLE)\n",
    "# =============================================\n",
    "# Target selection source-of-truth: Colab_04b_first_submission_no_ankh.ipynb (aspect-split Top-K)\n",
    "\n",
    "\n",
    "if TRAIN_LEVEL1:\n",
    "    import gc\n",
    "    import json\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import psutil\n",
    "\n",
    "    # AUDITOR: Hardware Check\n",
    "    try:\n",
    "        import torch\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"[AUDITOR] GPU Detected: {torch.cuda.get_device_name(0)}\")\n",
    "            print(\n",
    "                f\"[AUDITOR] VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\"\n",
    "            )\n",
    "        else:\n",
    "            print(\"[AUDITOR] WARNING: No GPU detected.\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    def log_mem(tag: str = \"\") -> None:\n",
    "        try:\n",
    "            mem = psutil.virtual_memory()\n",
    "            print(\n",
    "                f\"[MEM] {tag:<30} | Used: {mem.used/1e9:.2f}GB / {mem.total/1e9:.2f}GB ({mem.percent}%)\"\n",
    "            )\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # WORK_ROOT recovery (safety)\n",
    "    # Prefer canonical dataset root (cafa6_data/) and validate by presence of parsed artefacts.\n",
    "    if \"WORK_ROOT\" not in locals() and \"WORK_ROOT\" not in globals():\n",
    "        candidates = [\n",
    "            Path(\"/content/cafa6_data\"),\n",
    "            Path(\"/content/work\"),\n",
    "            Path(\"/kaggle/working/work\"),\n",
    "            Path.cwd() / \"cafa6_data\",\n",
    "            Path.cwd() / \"artefacts_local\" / \"work\",\n",
    "        ]\n",
    "\n",
    "        WORK_ROOT = None\n",
    "        for c in candidates:\n",
    "            if (c / \"parsed\" / \"train_terms.parquet\").exists():\n",
    "                WORK_ROOT = c\n",
    "                break\n",
    "\n",
    "        if WORK_ROOT is None:\n",
    "            for c in candidates:\n",
    "                if c.exists():\n",
    "                    WORK_ROOT = c\n",
    "                    break\n",
    "\n",
    "        if WORK_ROOT is None:\n",
    "            WORK_ROOT = Path.cwd() / \"cafa6_data\"\n",
    "\n",
    "        print(f\"WORK_ROOT recovered: {WORK_ROOT}\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Load targets + ids\n",
    "    # -----------------------------\n",
    "    print(\"Loading targets...\")\n",
    "    train_terms = pd.read_parquet(WORK_ROOT / \"parsed\" / \"train_terms.parquet\")\n",
    "    train_ids = pd.read_feather(WORK_ROOT / \"parsed\" / \"train_seq.feather\")[\"id\"].astype(str)\n",
    "    test_ids = pd.read_feather(WORK_ROOT / \"parsed\" / \"test_seq.feather\")[\"id\"].astype(str)\n",
    "\n",
    "    # FIX: Clean IDs in train_ids to match EntryID format\n",
    "    print(\"Applying ID cleaning fix...\")\n",
    "    train_ids_clean = train_ids.str.extract(r\"\\|(.*?)\\|\")[0]\n",
    "    train_ids_clean = train_ids_clean.fillna(train_ids)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Target Matrix Construction (Champion Strategy: 13,500 Terms)\n",
    "    # 10,000 BP + 2,000 MF + 1,500 CC\n",
    "    # -----------------------------\n",
    "    print(\"Selecting Top-K terms per aspect (Champion Strategy)...\")\n",
    "\n",
    "    try:\n",
    "        import obonet\n",
    "\n",
    "        # Robust OBO Path Search\n",
    "        possible_paths = [\n",
    "            WORK_ROOT / \"go-basic.obo\",\n",
    "            WORK_ROOT / \"Train\" / \"go-basic.obo\",\n",
    "            WORK_ROOT.parent / \"go-basic.obo\",\n",
    "            Path(\"go-basic.obo\"),\n",
    "            Path(\"Train/go-basic.obo\"),\n",
    "            Path(\"../Train/go-basic.obo\"),\n",
    "            Path(\"/content/cafa6_data/Train/go-basic.obo\"),\n",
    "        ]\n",
    "\n",
    "        obo_path = None\n",
    "        for p in possible_paths:\n",
    "            if p.exists():\n",
    "                obo_path = p\n",
    "                break\n",
    "\n",
    "        if obo_path is None:\n",
    "            raise FileNotFoundError(\n",
    "                f\"CRITICAL: go-basic.obo not found. Searched: {[str(p) for p in possible_paths]}\"\n",
    "            )\n",
    "\n",
    "        global PATH_GO_OBO\n",
    "        PATH_GO_OBO = obo_path\n",
    "        print(f\"Global PATH_GO_OBO set to: {PATH_GO_OBO}\")\n",
    "\n",
    "        print(f\"Loading OBO from {obo_path}...\")\n",
    "        graph = obonet.read_obo(obo_path)\n",
    "        term_to_ns = {\n",
    "            node: data.get(\"namespace\", \"unknown\") for node, data in graph.nodes(data=True)\n",
    "        }\n",
    "\n",
    "        # Keep compatibility with downstream code that expects go_namespaces\n",
    "        go_namespaces = term_to_ns\n",
    "\n",
    "        ns_map = {\n",
    "            \"biological_process\": \"BP\",\n",
    "            \"molecular_function\": \"MF\",\n",
    "            \"cellular_component\": \"CC\",\n",
    "        }\n",
    "\n",
    "        # Normalise any existing aspect column (some artefacts store full namespace strings)\n",
    "        aspect_aliases = {\n",
    "            \"biological_process\": \"BP\",\n",
    "            \"molecular_function\": \"MF\",\n",
    "            \"cellular_component\": \"CC\",\n",
    "            \"BP\": \"BP\",\n",
    "            \"MF\": \"MF\",\n",
    "            \"CC\": \"CC\",\n",
    "        }\n",
    "        if \"aspect\" in train_terms.columns:\n",
    "            train_terms[\"aspect\"] = train_terms[\"aspect\"].map(\n",
    "                lambda a: aspect_aliases.get(str(a), \"UNK\")\n",
    "            )\n",
    "        else:\n",
    "            train_terms[\"aspect\"] = train_terms[\"term\"].map(\n",
    "                lambda t: ns_map.get(term_to_ns.get(t), \"UNK\")\n",
    "            )\n",
    "\n",
    "    except ImportError as e:\n",
    "        raise RuntimeError(\"obonet not installed. Please install it.\") from e\n",
    "\n",
    "    # Canonical aspect split (04b)\n",
    "    term_counts = train_terms.groupby([\"aspect\", \"term\"]).size().reset_index(name=\"count\")\n",
    "    targets_bp = (\n",
    "        term_counts[term_counts[\"aspect\"] == \"BP\"].nlargest(10000, \"count\")[\"term\"].tolist()\n",
    "    )\n",
    "    targets_mf = (\n",
    "        term_counts[term_counts[\"aspect\"] == \"MF\"].nlargest(2000, \"count\")[\"term\"].tolist()\n",
    "    )\n",
    "    targets_cc = (\n",
    "        term_counts[term_counts[\"aspect\"] == \"CC\"].nlargest(1500, \"count\")[\"term\"].tolist()\n",
    "    )\n",
    "\n",
    "    # Guardrail: avoid silently switching target strategy due to aspect encoding mismatch\n",
    "    ALLOW_GLOBAL_FALLBACK = False\n",
    "    if len(targets_bp) == 0 and len(targets_mf) == 0 and len(targets_cc) == 0:\n",
    "        aspect_vc = train_terms[\"aspect\"].value_counts().to_dict() if \"aspect\" in train_terms.columns else {}\n",
    "        msg = (\n",
    "            \"No BP/MF/CC aspect split found after normalisation. \"\n",
    "            f\"aspect_vc={aspect_vc}. This would fall back to global Top-13,500; \"\n",
    "            \"set ALLOW_GLOBAL_FALLBACK=True to override.\"\n",
    "        )\n",
    "        if ALLOW_GLOBAL_FALLBACK:\n",
    "            print(\"  [WARNING] \" + msg)\n",
    "            top_terms = train_terms[\"term\"].value_counts().head(13500).index.tolist()\n",
    "        else:\n",
    "            raise RuntimeError(msg)\n",
    "    else:\n",
    "        # Stable, deterministic ordering: BP then MF then CC with de-dup preserving order\n",
    "        top_terms = []\n",
    "        seen = set()\n",
    "        for t in (targets_bp + targets_mf + targets_cc):\n",
    "            if t not in seen:\n",
    "                top_terms.append(t)\n",
    "                seen.add(t)\n",
    "        print(f\"  Selected: {len(targets_bp)} BP + {len(targets_mf)} MF + {len(targets_cc)} CC\")\n",
    "\n",
    "    # Persist label contract for downstream stages\n",
    "    top_terms_path = WORK_ROOT / \"features\" / \"top_terms_13500.json\"\n",
    "    top_terms_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if top_terms_path.exists():\n",
    "        try:\n",
    "            with open(top_terms_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                top_terms_disk = json.load(f)\n",
    "            if isinstance(top_terms_disk, list) and len(top_terms_disk) > 0:\n",
    "                top_terms = [str(x) for x in top_terms_disk]\n",
    "                print(f\"Loaded existing top_terms_13500.json (n={len(top_terms)})\")\n",
    "        except Exception as e:\n",
    "            print(f\"[WARNING] Failed to load existing top_terms_13500.json: {e}\")\n",
    "    else:\n",
    "        with open(top_terms_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(list(top_terms), f)\n",
    "        print(\"Saved: top_terms_13500.json\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Stable target contract (audited: 1,585 terms)\n",
    "    # Definition: GO terms with >= 50 positives AND valid namespace (BP/MF/CC)\n",
    "    # Stored separately from top_terms_13500.json (do not mix contracts).\n",
    "    # -----------------------------\n",
    "    stable_terms_path = WORK_ROOT / \"features\" / \"stable_terms_1585.json\"\n",
    "    stable_meta_path = WORK_ROOT / \"features\" / \"stable_terms_1585_meta.json\"\n",
    "    noise_floor = 50\n",
    "\n",
    "    if stable_terms_path.exists():\n",
    "        try:\n",
    "            stable_terms = json.loads(stable_terms_path.read_text(encoding=\"utf-8\"))\n",
    "            stable_terms = [str(t) for t in stable_terms]\n",
    "            print(f\"Loaded existing stable_terms_1585.json (n={len(stable_terms)})\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to load {stable_terms_path}: {e}\")\n",
    "    else:\n",
    "        # Compute from Phase-1 truth (train_terms.parquet) and OBO namespace mapping already loaded above.\n",
    "        stable_bp = (\n",
    "            term_counts[(term_counts[\"aspect\"] == \"BP\") & (term_counts[\"count\"] >= noise_floor)]\n",
    "            .sort_values([\"count\", \"term\"], ascending=[False, True])[\"term\"]\n",
    "            .astype(str)\n",
    "            .tolist()\n",
    "        )\n",
    "        stable_mf = (\n",
    "            term_counts[(term_counts[\"aspect\"] == \"MF\") & (term_counts[\"count\"] >= noise_floor)]\n",
    "            .sort_values([\"count\", \"term\"], ascending=[False, True])[\"term\"]\n",
    "            .astype(str)\n",
    "            .tolist()\n",
    "        )\n",
    "        stable_cc = (\n",
    "            term_counts[(term_counts[\"aspect\"] == \"CC\") & (term_counts[\"count\"] >= noise_floor)]\n",
    "            .sort_values([\"count\", \"term\"], ascending=[False, True])[\"term\"]\n",
    "            .astype(str)\n",
    "            .tolist()\n",
    "        )\n",
    "        stable_terms = stable_bp + stable_mf + stable_cc\n",
    "        stable_terms_path.write_text(json.dumps(stable_terms), encoding=\"utf-8\")\n",
    "        stable_meta_path.write_text(\n",
    "            json.dumps(\n",
    "                {\n",
    "                    \"noise_floor\": noise_floor,\n",
    "                    \"counts\": {\"BP\": len(stable_bp), \"MF\": len(stable_mf), \"CC\": len(stable_cc)},\n",
    "                    \"total\": len(stable_terms),\n",
    "                },\n",
    "                indent=2,\n",
    "            ),\n",
    "            encoding=\"utf-8\",\n",
    "        )\n",
    "        print(f\"Saved: stable_terms_1585.json (n={len(stable_terms)})\")\n",
    "\n",
    "    if len(stable_terms) != 1585:\n",
    "        raise RuntimeError(f\"Stable term contract mismatch: expected 1585, got {len(stable_terms)}\")\n",
    "\n",
    "    top_term_to_idx = {t: i for i, t in enumerate(top_terms)}\n",
    "    missing_stable = [t for t in stable_terms if t not in top_term_to_idx]\n",
    "    if missing_stable:\n",
    "        raise RuntimeError(\n",
    "            \"Stable terms contain items not present in top_terms_13500.json. \"\n",
    "            f\"Missing={len(missing_stable)} (example: {missing_stable[:10]})\"\n",
    "        )\n",
    "\n",
    "    stable_idx = np.asarray([top_term_to_idx[t] for t in stable_terms], dtype=np.int64)\n",
    "    print(f\"Stable targets ready: n={int(stable_idx.shape[0])} (expected 1585)\")\n",
    "\n",
    "    train_terms_top = train_terms[train_terms[\"term\"].isin(top_terms)]\n",
    "    Y_df = train_terms_top.pivot_table(index=\"EntryID\", columns=\"term\", aggfunc=\"size\", fill_value=0)\n",
    "    Y_df = Y_df.reindex(train_ids_clean, fill_value=0)\n",
    "    Y = Y_df.values.astype(np.float32)\n",
    "    print(f\"Targets: Y={Y.shape}\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Feature loading helper (Memory Optimised)\n",
    "    # -----------------------------\n",
    "    FEAT_DIR = WORK_ROOT / \"features\"\n",
    "\n",
    "    def load_features_dict(split: str = \"both\"):\n",
    "        log_mem(f\"Start load_features_dict({split})\")\n",
    "        print(f\"Loading multimodal features (mode={split})...\")\n",
    "\n",
    "        ft_train = {}\n",
    "        ft_test = {}\n",
    "\n",
    "        def _load_pair(stem: str):\n",
    "            tr = FEAT_DIR / f\"train_embeds_{stem}.npy\"\n",
    "            te = FEAT_DIR / f\"test_embeds_{stem}.npy\"\n",
    "            return tr, te\n",
    "\n",
    "        # All modalities are mandatory.\n",
    "        stems = [\n",
    "            (\"t5\", \"t5\"),\n",
    "            (\"esm2\", \"esm2_650m\"),\n",
    "            (\"esm2_3b\", \"esm2_3b\"),\n",
    "            (\"ankh\", \"ankh\"),\n",
    "            (\"text\", \"text\"),\n",
    "        ]\n",
    "\n",
    "        for stem, key in stems:\n",
    "            tr_path, te_path = _load_pair(stem)\n",
    "            if not (tr_path.exists() and te_path.exists()):\n",
    "                raise FileNotFoundError(f\"Missing mandatory embeddings for {stem}: {tr_path} or {te_path}\")\n",
    "\n",
    "            if split in [\"both\", \"train\"]:\n",
    "                ft_train[key] = np.load(tr_path, mmap_mode=\"r\")\n",
    "            if split in [\"both\", \"test\"]:\n",
    "                ft_test[key] = np.load(te_path, mmap_mode=\"r\")\n",
    "\n",
    "        taxa_train_path = WORK_ROOT / \"parsed\" / \"train_taxa.feather\"\n",
    "        taxa_test_path = WORK_ROOT / \"parsed\" / \"test_taxa.feather\"\n",
    "\n",
    "        if not (taxa_train_path.exists() and taxa_test_path.exists()):\n",
    "            raise FileNotFoundError(f\"Missing mandatory taxa features: {taxa_train_path} or {taxa_test_path}\")\n",
    "\n",
    "        from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "        tax_tr = pd.read_feather(taxa_train_path).astype({\"id\": str})\n",
    "        tax_te = pd.read_feather(taxa_test_path).astype({\"id\": str})\n",
    "        enc = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False, dtype=np.float32)\n",
    "        enc.fit(pd.concat([tax_tr[[\"taxon_id\"]], tax_te[[\"taxon_id\"]]], axis=0))\n",
    "\n",
    "        if split in [\"both\", \"train\"]:\n",
    "            tax_tr = tax_tr.set_index(\"id\").reindex(train_ids, fill_value=0).reset_index()\n",
    "            ft_train[\"taxa\"] = enc.transform(tax_tr[[\"taxon_id\"]]).astype(np.float32)\n",
    "        if split in [\"both\", \"test\"]:\n",
    "            tax_te = tax_te.set_index(\"id\").reindex(test_ids, fill_value=0).reset_index()\n",
    "            ft_test[\"taxa\"] = enc.transform(tax_te[[\"taxon_id\"]]).astype(np.float32)\n",
    "\n",
    "        log_mem(f\"End load_features_dict({split})\")\n",
    "        if split == \"train\":\n",
    "            return ft_train\n",
    "        if split == \"test\":\n",
    "            return ft_test\n",
    "        return ft_train, ft_test\n",
    "\n",
    "    # Materialise feature dicts (mmap arrays where possible)\n",
    "    features_train, features_test = load_features_dict(split=\"both\")\n",
    "\n",
    "    # Flat concatenation order for classical models (LR/GBDT)\n",
    "    FLAT_KEYS = [k for k in [\"t5\", \"esm2_650m\", \"esm2_3b\", \"ankh\", \"text\", \"taxa\"] if k in features_train]\n",
    "    if \"ankh\" not in FLAT_KEYS:\n",
    "        raise RuntimeError(\"Ankh is mandatory but was not loaded into features_train.\")\n",
    "    print(f\"Flat X keys={FLAT_KEYS}\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Disk-backed X / X_test (for RAM-safe downstream cells)\n",
    "    # -----------------------------\n",
    "    X_train_path = FEAT_DIR / \"X_train_mmap.npy\"\n",
    "    X_test_path = FEAT_DIR / \"X_test_mmap.npy\"\n",
    "\n",
    "    def _build_X_memmaps(chunk_size: int = 10000) -> None:\n",
    "        dims = {k: int(features_train[k].shape[1]) for k in FLAT_KEYS}\n",
    "        total_dim = int(sum(dims.values()))\n",
    "        n_tr = int(len(train_ids))\n",
    "        n_te = int(len(test_ids))\n",
    "\n",
    "        print(f\"Building X memmaps: train=({n_tr}, {total_dim}) test=({n_te}, {total_dim})\")\n",
    "        X_mm = np.lib.format.open_memmap(\n",
    "            str(X_train_path), mode=\"w+\", dtype=np.float32, shape=(n_tr, total_dim)\n",
    "        )\n",
    "        Xte_mm = np.lib.format.open_memmap(\n",
    "            str(X_test_path), mode=\"w+\", dtype=np.float32, shape=(n_te, total_dim)\n",
    "        )\n",
    "\n",
    "        col = 0\n",
    "        for k in FLAT_KEYS:\n",
    "            d = dims[k]\n",
    "            print(f\"  Streaming {k} into cols {col}:{col + d}\")\n",
    "            for i in range(0, n_tr, chunk_size):\n",
    "                j = min(i + chunk_size, n_tr)\n",
    "                X_mm[i:j, col : col + d] = np.asarray(features_train[k][i:j], dtype=np.float32)\n",
    "            for i in range(0, n_te, chunk_size):\n",
    "                j = min(i + chunk_size, n_te)\n",
    "                Xte_mm[i:j, col : col + d] = np.asarray(features_test[k][i:j], dtype=np.float32)\n",
    "            col += d\n",
    "\n",
    "        X_mm.flush()\n",
    "        Xte_mm.flush()\n",
    "\n",
    "    if X_train_path.exists() and X_test_path.exists():\n",
    "        print(\"X memmaps already exist; skipping build.\")\n",
    "    else:\n",
    "        _build_X_memmaps(chunk_size=5000)\n",
    "\n",
    "    X = np.load(X_train_path, mmap_mode=\"r\")\n",
    "    X_test = np.load(X_test_path, mmap_mode=\"r\")\n",
    "\n",
    "    log_mem(\"Phase 2 setup done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3240d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 13c - Level 1: Logistic Regression — Long tail 13,500 (Aspect Split BP/MF/CC)\n",
    "# ==============================================================================\n",
    "# Rank 1 Optimization: Asynchronous GPU Pipelining + Automated Artifact Sync\n",
    "# ==============================================================================\n",
    "if not TRAIN_LEVEL1:\n",
    "    print('Skipping LogReg (TRAIN_LEVEL1=False).')\n",
    "else:\n",
    "    import os, sys, time, threading, gc, warnings, psutil, json\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import joblib\n",
    "    from pathlib import Path\n",
    "    from tqdm.auto import tqdm\n",
    "    from sklearn.model_selection import KFold\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from cuml.linear_model import LogisticRegression as cuLogReg\n",
    "    from cuml.multiclass import OneVsRestClassifier as cuOVR\n",
    "    import cupy as cp\n",
    "    import obonet\n",
    "\n",
    "    def _stage(msg):\n",
    "        print(msg); sys.stdout.flush()\n",
    "\n",
    "    # --- WORK_ROOT Recovery (Standalone Support) ---\n",
    "    if 'WORK_ROOT' not in locals() and 'WORK_ROOT' not in globals():\n",
    "        WORK_ROOT = Path.cwd() / 'cafa6_data'\n",
    "        print(f\"WORK_ROOT recovered: {WORK_ROOT}\")\n",
    "\n",
    "    # --- OBO / Aspect Logic (Standalone Support) ---\n",
    "    # We need to know which aspect (BP/MF/CC) each term belongs to.\n",
    "    try:\n",
    "        candidates = [\n",
    "            Path(WORK_ROOT) / \"go-basic.obo\",\n",
    "            Path(WORK_ROOT) / \"Train\" / \"go-basic.obo\",\n",
    "            Path(WORK_ROOT).parent / \"go-basic.obo\",\n",
    "            Path(\"go-basic.obo\"),\n",
    "            Path(\"Train/go-basic.obo\"),\n",
    "            Path(\"../Train/go-basic.obo\"),\n",
    "            Path(\"cafa6_data/go-basic.obo\"),\n",
    "            Path(\"../cafa6_data/go-basic.obo\"),\n",
    "            Path(\"/content/cafa6_data/Train/go-basic.obo\"),\n",
    "        ]\n",
    "        obo_path = next((p for p in candidates if p.exists()), None)\n",
    "        if obo_path:\n",
    "            print(f\"Loading OBO from {obo_path}...\")\n",
    "            graph = obonet.read_obo(obo_path)\n",
    "            _term_to_ns = {id_: data.get('namespace', 'unknown') for id_, data in graph.nodes(data=True)}\n",
    "            _ns_alias = {'biological_process': 'BP', 'molecular_function': 'MF', 'cellular_component': 'CC'}\n",
    "            def _aspect_of_term(t):\n",
    "                return _ns_alias.get(_term_to_ns.get(t), 'UNK')\n",
    "        else:\n",
    "            print(f\"WARNING: go-basic.obo not found in {[str(p) for p in candidates]}. Aspect splitting will fail (all UNK).\")\n",
    "            def _aspect_of_term(t): return 'UNK'\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load OBO: {e}\")\n",
    "        def _aspect_of_term(t): return 'UNK'\n",
    "\n",
    "    # --- HELPER: Fast-Path Predict Proba (Manual GEMM) [Diagnostic Fix] ---\n",
    "    def safe_predict_proba_gpu(clf, x_gpu):\n",
    "        W, b = None, None\n",
    "        # Exhaustive weights search to prevent AttributeErrors [Turn 8]\n",
    "        if hasattr(clf, 'multiclass_estimator'):\n",
    "            m_est = clf.multiclass_estimator\n",
    "            if hasattr(m_est, 'coef_'):\n",
    "                W, b = cp.asarray(m_est.coef_, dtype=cp.float32), cp.asarray(m_est.intercept_, dtype=cp.float32)\n",
    "            elif hasattr(m_est, 'estimators_'):\n",
    "                # Handle case where estimators might be on CPU or GPU\n",
    "                try:\n",
    "                    ws = []\n",
    "                    bs = []\n",
    "                    for e in m_est.estimators_:\n",
    "                        w = e.coef_\n",
    "                        b_val = e.intercept_\n",
    "                        if not isinstance(w, cp.ndarray): w = cp.asarray(w)\n",
    "                        if not isinstance(b_val, cp.ndarray): b_val = cp.asarray(b_val)\n",
    "                        ws.append(w)\n",
    "                        bs.append(b_val)\n",
    "                    W = cp.vstack(ws)\n",
    "                    b = cp.hstack(bs)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "        if W is None and hasattr(clf, 'coef_'):\n",
    "            W, b = cp.asarray(clf.coef_, dtype=cp.float32), cp.asarray(clf.intercept_, dtype=cp.float32)\n",
    "        \n",
    "        # Native A100 GEMM (Fast Path) [Source 07]\n",
    "        if W is not None and b is not None:\n",
    "            scores = cp.dot(x_gpu, W.T if W.ndim > 1 else W) + b\n",
    "            return 1.0 / (1.0 + cp.exp(-cp.clip(scores, -50.0, 50.0)))\n",
    "        \n",
    "        # Fallback\n",
    "        return clf.predict_proba(x_gpu)\n",
    "\n",
    "    # --- DATA LOADING ---\n",
    "    FEAT_DIR = Path(WORK_ROOT) / 'features'\n",
    "    PRED_DIR = FEAT_DIR / 'level1_preds'\n",
    "    PRED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    top_terms_path = FEAT_DIR / 'top_terms_13500.json'\n",
    "    if not top_terms_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing {top_terms_path}. Run Cell 13a first.\")\n",
    "    top_terms = json.loads(top_terms_path.read_text())\n",
    "\n",
    "    # --- [FROM SOURCE 53-54] LOAD IA WEIGHTS ---\n",
    "    weights_full = None\n",
    "    try:\n",
    "        ia_path = next((p for p in [Path(WORK_ROOT)/'IA.tsv', FEAT_DIR/'IA.tsv'] if p.exists()), None)\n",
    "        if ia_path:\n",
    "            ia_df = pd.read_csv(ia_path, sep='\\t', header=None, names=['term', 'ia'])\n",
    "            ia_map = dict(zip(ia_df['term'].astype(str), ia_df['ia'].astype(np.float32)))\n",
    "            # Map weights to our master top_terms list\n",
    "            weights_full = np.asarray([ia_map.get(t, 1.0) for t in top_terms], dtype=np.float32)\n",
    "            _stage(f\"[RANK 1] Loaded IA weights from {ia_path}\")\n",
    "        else:\n",
    "             _stage(f\"[WARN] IA.tsv not found. Using default weights (1.0).\")\n",
    "             weights_full = np.ones(len(top_terms), dtype=np.float32)\n",
    "    except Exception as e:\n",
    "        _stage(f\"[CRITICAL] IA weights required for Rank 1 F1 boost: {e}\")\n",
    "        weights_full = np.ones(len(top_terms), dtype=np.float32)\n",
    "    \n",
    "    # X Loading (Memmap)\n",
    "    x_path = FEAT_DIR / 'X_train_mmap.npy'\n",
    "    xt_path = FEAT_DIR / 'X_test_mmap.npy'\n",
    "    if not x_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing {x_path}. Run Cell 13a first to build memmaps.\")\n",
    "    \n",
    "    X = np.load(x_path, mmap_mode='r')\n",
    "    X_test_ram = np.ascontiguousarray(np.load(xt_path, mmap_mode='r'), dtype=np.float32)\n",
    "    \n",
    "    # Y Loading (Target Matrix)\n",
    "    y_path = FEAT_DIR / 'Y_target_13500.npy'\n",
    "    if y_path.exists():\n",
    "        print(f\"Loading targets from {y_path}...\")\n",
    "        Y_full = np.load(y_path, mmap_mode='r')\n",
    "    else:\n",
    "        print(f\"Building targets {y_path} (first run)...\")\n",
    "        # Reconstruct Y if missing (Standalone robustness)\n",
    "        train_terms = pd.read_parquet(WORK_ROOT / 'parsed' / 'train_terms.parquet')\n",
    "        train_ids = pd.read_feather(WORK_ROOT / 'parsed' / 'train_seq.feather')['id'].astype(str)\n",
    "        # Clean IDs\n",
    "        train_ids_clean = train_ids.str.extract(r\"\\|(.*?)\\|\")[0].fillna(train_ids)\n",
    "        \n",
    "        train_terms_top = train_terms[train_terms[\"term\"].isin(top_terms)]\n",
    "        Y_df = train_terms_top.pivot_table(index=\"EntryID\", columns=\"term\", aggfunc=\"size\", fill_value=0)\n",
    "        Y_df = Y_df.reindex(train_ids_clean, fill_value=0)\n",
    "        Y_full_ram = Y_df.values.astype(np.float32)\n",
    "        \n",
    "        np.save(y_path, Y_full_ram)\n",
    "        Y_full = np.load(y_path, mmap_mode='r')\n",
    "        print(f\"Saved {y_path}\")\n",
    "\n",
    "    # Runtime Knobs [Source 62]\n",
    "    TARGET_CHUNK = 125 # Reduced from 250 to prevent cuBLAS handle exhaustion\n",
    "    VAL_BS, TEST_BS, n_splits = 4096, 8192, 5\n",
    "    aspects = [(os.environ.get('TARGET_ASPECT') or 'MF').upper()]\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    oof_pred_logreg_by_aspect = {}\n",
    "    aspect_indices_map = {}\n",
    "\n",
    "    for aspect in aspects:\n",
    "        _stage(f\"\\n=== LogReg Aspect: {aspect} ===\")\n",
    "        aspect_indices = np.array([i for i, t in enumerate(top_terms) if _aspect_of_term(t) == aspect])\n",
    "        if len(aspect_indices) == 0:\n",
    "            print(f\"No terms found for aspect {aspect}. Check OBO loading.\")\n",
    "            continue\n",
    "            \n",
    "        aspect_indices_map[aspect] = aspect_indices\n",
    "        n_targets = len(aspect_indices)\n",
    "        print(f\"Training {n_targets} targets for {aspect}...\")\n",
    "\n",
    "        lr_oof_path = PRED_DIR / f'oof_pred_logreg_{aspect}.npy'\n",
    "        lr_test_path = PRED_DIR / f'test_pred_logreg_{aspect}.npy'\n",
    "        \n",
    "        # Initialize memmaps\n",
    "        if not lr_oof_path.exists():\n",
    "            np.save(lr_oof_path, np.zeros((X.shape[0], n_targets), dtype=np.float32))\n",
    "        if not lr_test_path.exists():\n",
    "            np.save(lr_test_path, np.zeros((X_test_ram.shape[0], n_targets), dtype=np.float32))\n",
    "            \n",
    "        oof_pred = np.lib.format.open_memmap(str(lr_oof_path), mode='r+', dtype=np.float32, shape=(X.shape[0], n_targets))\n",
    "        test_pred = np.lib.format.open_memmap(str(lr_test_path), mode='r+', dtype=np.float32, shape=(X_test_ram.shape[0], n_targets))\n",
    "\n",
    "        for fold, (idx_tr, idx_val) in enumerate(kf.split(np.arange(X.shape[0]))):\n",
    "            _stage(f\"{aspect} Fold {fold+1}/{n_splits}\")\n",
    "            scaler = StandardScaler()\n",
    "            X_tr_gpu = cp.asarray(scaler.fit_transform(X[idx_tr]), dtype=cp.float32)\n",
    "            X_val_gpu = cp.asarray(scaler.transform(X[idx_val]), dtype=cp.float32)\n",
    "            X_test_gpu = cp.asarray(scaler.transform(X_test_ram), dtype=cp.float32)\n",
    "\n",
    "            for start in tqdm(range(0, n_targets, TARGET_CHUNK), desc=f\"Chunks\"):\n",
    "                end = min(start + TARGET_CHUNK, n_targets)\n",
    "                cols = aspect_indices[start:end]\n",
    "                chunk_width = end - start\n",
    "                \n",
    "                # [HEARTBEAT START]\n",
    "                print(f\"    [Start] {aspect} Fold {fold+1} | Terms {start}-{end} | {time.strftime('%H:%M:%S')}\")\n",
    "                sys.stdout.flush()\n",
    "\n",
    "                # --- [RANK 1 REQUIREMENT] IA-WEIGHTED CLASS WEIGHTS [Source 139, 475, 488] ---\n",
    "                # Slicing IA weights for the current chunk of GO terms\n",
    "                weights_chunk = weights_full[cols]\n",
    "                \n",
    "                # cuML requires a dictionary mapping class indices to weights for class_weight.\n",
    "                # For rare term prioritization, we pass the IA value as the weight for the positive class.\n",
    "                # Note: In Multi-label OVR, this prioritizes the '1' label for high-IA terms.\n",
    "                cw_dict = {i: float(weights_chunk[i]) for i in range(chunk_width)}\n",
    "\n",
    "                # Initialize cuLogReg with IA-weighting for Rank 1 precision [Source 136, 488]\n",
    "                clf_chunk = cuOVR(cuLogReg(\n",
    "                    solver='qn', \n",
    "                    max_iter=1000, \n",
    "                    tol=1e-2, \n",
    "                    class_weight=cw_dict # <--- IA-WEIGHTING INTEGRATED HERE\n",
    "                ))\n",
    "                \n",
    "                Y_tr_chunk = cp.asarray(Y_full[np.ix_(idx_tr, cols)])\n",
    "                \n",
    "                # --- FIT TIMING ---\n",
    "                t_fit_start = time.time()\n",
    "                clf_chunk.fit(X_tr_gpu, Y_tr_chunk)\n",
    "                t_fit_end = time.time()\n",
    "\n",
    "                # --- [PHASE 3 READINESS] GENERATING OOF FEATURES [Source 137, 491] ---\n",
    "                # These OOF probabilities replace raw embeddings as primary features for GCNs.\n",
    "                val_gpu_buffer = cp.zeros((len(idx_val), chunk_width), dtype=cp.float32)\n",
    "                for b0 in range(0, len(idx_val), VAL_BS):\n",
    "                    b1 = min(b0 + VAL_BS, len(idx_val))\n",
    "                    val_gpu_buffer[b0:b1, :] = safe_predict_proba_gpu(clf_chunk, X_val_gpu[b0:b1])\n",
    "                \n",
    "                # Store directly into memmap for GCN Stacker consumption\n",
    "                oof_pred[idx_val, start:end] = val_gpu_buffer.get() \n",
    "\n",
    "                # Generate Test Features for GCN Inference\n",
    "                test_gpu_buffer = cp.zeros((X_test_ram.shape[0], chunk_width), dtype=cp.float32)\n",
    "                for b0 in range(0, X_test_ram.shape[0], TEST_BS):\n",
    "                    b1 = min(b0 + TEST_BS, X_test_ram.shape[0])\n",
    "                    test_gpu_buffer[b0:b1, :] = safe_predict_proba_gpu(clf_chunk, X_test_gpu[b0:b1])\n",
    "                \n",
    "                test_pred[:, start:end] += (test_gpu_buffer.get() / n_splits)\n",
    "                t_inf_end = time.time()\n",
    "\n",
    "                # --- FIX: Aggressive GPU cleanup to prevent cuBLAS handle exhaustion ---\n",
    "                del clf_chunk, Y_tr_chunk, val_gpu_buffer, test_gpu_buffer\n",
    "                cp.get_default_memory_pool().free_all_blocks()\n",
    "                cp.get_default_pinned_memory_pool().free_all_blocks()\n",
    "                \n",
    "                # [HEARTBEAT & METRICS]\n",
    "                dur_fit = t_fit_end - t_fit_start\n",
    "                dur_inf = t_inf_end - t_fit_end\n",
    "                mem_used_gb = cp.get_default_memory_pool().used_bytes() / 1e9\n",
    "                print(f\"    [Step] {aspect} Fold {fold+1} | Terms {start:>5}-{end:<5} | \"\n",
    "                      f\"Fit: {dur_fit:>5.1f}s | Inf: {dur_inf:>5.1f}s | \"\n",
    "                      f\"Mem: {mem_used_gb:.1f}GB | {time.strftime('%H:%M:%S')}\")\n",
    "                sys.stdout.flush()\n",
    "\n",
    "        oof_pred.flush(); test_pred.flush()\n",
    "        oof_pred_logreg_by_aspect[aspect] = np.load(lr_oof_path, mmap_mode='r')\n",
    "\n",
    "    # --- RESTORED: STAGE MAYBE PUSH [Source 103-105] ---\n",
    "    if 'STORE' in globals() and STORE is not None:\n",
    "        _stage(\"[STORE] Executing Stage Maybe push for Level 1 LogReg...\")\n",
    "        required = [str(top_terms_path.as_posix())]\n",
    "        for asp in oof_pred_logreg_by_aspect.keys():\n",
    "            required += [\n",
    "                str((PRED_DIR / f'oof_pred_logreg_{asp}.npy').as_posix()),\n",
    "                str((PRED_DIR / f'test_pred_logreg_{asp}.npy').as_posix()),\n",
    "                str((PRED_DIR / f'top_terms_{asp}.json').as_posix()),\n",
    "            ]\n",
    "        try:\n",
    "            STORE.maybe_push(\n",
    "                stage='stage_07a_level1_logreg_aspect_split',\n",
    "                required_paths=required,\n",
    "                note='Rank 1 Optimized LogReg Predictions (A100 Fast Path).'\n",
    "            )\n",
    "        except Exception as e:\n",
    "            _stage(f\"[WARN] Store push failed: {e}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
