{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24a5cff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch tqdm obonet biopython transformers scikit-learn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "578fc248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: c:\\Users\\Olale\\Documents\\Codebase\\Science\\cafa-6-protein-function-prediction\n",
      "Src path: c:\\Users\\Olale\\Documents\\Codebase\\Science\\cafa-6-protein-function-prediction\\src\n",
      "Src exists: True\n",
      "Added to path: c:\\Users\\Olale\\Documents\\Codebase\\Science\\cafa-6-protein-function-prediction\\src\n",
      "\n",
      "Python path:\n",
      "  c:\\Users\\Olale\\Documents\\Codebase\\Science\\cafa-6-protein-function-prediction\\src\n",
      "  c:\\Users\\Olale\\Documents\\Codebase\\Science\\src\n",
      "  c:\\Users\\Olale\\Documents\\Codebase\\Science\\src\n",
      "  c:\\Users\\Olale\\Documents\\Codebase\\Science\\src\n",
      "  c:\\Users\\Olale\\Documents\\Codebase\\Science\\src\n"
     ]
    }
   ],
   "source": [
    "# Test path setup\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "src_path = Path.cwd() / 'src'\n",
    "print(f\"Current directory: {Path.cwd()}\")\n",
    "print(f\"Src path: {src_path}\")\n",
    "print(f\"Src exists: {src_path.exists()}\")\n",
    "\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "    print(f\"Added to path: {src_path}\")\n",
    "\n",
    "print(f\"\\nPython path:\")\n",
    "for p in sys.path[:5]:\n",
    "    print(f\"  {p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9fdc2b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'data.loaders'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Test importing our modules with corrected path\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mloaders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OntologyLoader\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mâœ… Successfully imported OntologyLoader\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'data.loaders'"
     ]
    }
   ],
   "source": [
    "# Test importing our modules with corrected path\n",
    "from data.loaders import OntologyLoader\n",
    "print(\"âœ… Successfully imported OntologyLoader\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c08ff628",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'data.loaders'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m sys.path.insert(\u001b[32m0\u001b[39m, \u001b[38;5;28mstr\u001b[39m(Path.cwd().parent / \u001b[33m'\u001b[39m\u001b[33msrc\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Our modules\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mloaders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OntologyLoader, LabelLoader, SequenceLoader\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfinetune_dataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FineTuneDataset\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mesm_classifier\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ESMForSequenceClassification\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'data.loaders'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd().parent / 'src'))\n",
    "\n",
    "# Our modules\n",
    "from data.loaders import OntologyLoader, LabelLoader, SequenceLoader\n",
    "from data.finetune_dataset import FineTuneDataset\n",
    "from models.esm_classifier import ESMForSequenceClassification\n",
    "from inference.propagation import propagate_predictions, get_propagated_terms\n",
    "\n",
    "print(\"âœ… Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc7efb8",
   "metadata": {},
   "source": [
    "## 1. Load GO Ontology\n",
    "\n",
    "The ontology is a directed graph where edges point from specific terms to general ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1438bc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path.cwd().parent\n",
    "\n",
    "print(\"Loading GO ontology...\")\n",
    "ontology = OntologyLoader(base_dir / \"Train\" / \"go-basic.obo\")\n",
    "\n",
    "print(f\"Total GO terms: {len(ontology.graph)}\")\n",
    "print(f\"\\nGraph type: {type(ontology.graph)}\")\n",
    "print(f\"Nodes: {ontology.graph.number_of_nodes()}\")\n",
    "print(f\"Edges: {ontology.graph.number_of_edges()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bab92cd",
   "metadata": {},
   "source": [
    "## 2. Test Propagation with Example Terms\n",
    "\n",
    "Let's check: if we predict \"nuclease activity\", what ancestors should we add?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf80ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: nuclease activity\n",
    "nuclease = 'GO:0004518'\n",
    "\n",
    "# Get all ancestors\n",
    "ancestors = ontology.get_ancestors(nuclease)\n",
    "\n",
    "print(f\"Term: {nuclease}\")\n",
    "if nuclease in ontology.graph:\n",
    "    print(f\"Name: {ontology.graph.nodes[nuclease].get('name', 'N/A')}\")\n",
    "    print(f\"Namespace: {ontology.get_namespace(nuclease)}\")\n",
    "\n",
    "print(f\"\\nNumber of ancestors: {len(ancestors)}\")\n",
    "print(f\"\\nFirst 10 ancestors:\")\n",
    "for anc in list(ancestors)[:10]:\n",
    "    name = ontology.graph.nodes[anc].get('name', 'N/A')\n",
    "    print(f\"  {anc}: {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6df470",
   "metadata": {},
   "source": [
    "## 3. Test Array Propagation\n",
    "\n",
    "Simulate model predictions and see how propagation changes probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20cdf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy predictions for 5 terms\n",
    "test_terms = [\n",
    "    'GO:0003674',  # molecular_function (root)\n",
    "    'GO:0016787',  # hydrolase activity\n",
    "    'GO:0004518',  # nuclease activity (child of hydrolase)\n",
    "    'GO:0008150',  # biological_process (root)\n",
    "    'GO:0006281',  # DNA repair\n",
    "]\n",
    "\n",
    "# Simulate: high confidence only for nuclease (0.9)\n",
    "preds_before = np.array([[0.05, 0.10, 0.90, 0.03, 0.15]])\n",
    "\n",
    "print(\"Before propagation:\")\n",
    "for term, prob in zip(test_terms, preds_before[0]):\n",
    "    name = ontology.graph.nodes[term].get('name', 'N/A')\n",
    "    print(f\"  {term} ({name}): {prob:.3f}\")\n",
    "\n",
    "# Apply propagation\n",
    "preds_after = propagate_predictions(preds_before, test_terms, ontology, strategy='max')\n",
    "\n",
    "print(\"\\nAfter propagation:\")\n",
    "for term, prob in zip(test_terms, preds_after[0]):\n",
    "    name = ontology.graph.nodes[term].get('name', 'N/A')\n",
    "    change = 'âœ…' if prob > preds_before[0][test_terms.index(term)] else ''\n",
    "    print(f\"  {term} ({name}): {prob:.3f} {change}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Hydrolase boosted: {preds_before[0][1]:.3f} â†’ {preds_after[0][1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c337b05",
   "metadata": {},
   "source": [
    "## 4. Load Training Data\n",
    "\n",
    "We need the validation set to measure F1 improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29be10a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading sequences...\")\n",
    "seq_loader = SequenceLoader(base_dir / \"Train\" / \"train_sequences.fasta\")\n",
    "\n",
    "print(\"Loading labels...\")\n",
    "label_loader = LabelLoader(base_dir / \"Train\" / \"train_terms.tsv\")\n",
    "\n",
    "print(\"Loading IA weights...\")\n",
    "ia_df = pd.read_csv(base_dir / \"IA.tsv\", sep='\\t')\n",
    "ia_weights = dict(zip(ia_df['term'], ia_df['IA']))\n",
    "\n",
    "print(f\"\\nTotal proteins: {len(seq_loader.sequences)}\")\n",
    "print(f\"Total annotations: {len(label_loader.df)}\")\n",
    "print(f\"IA weights available: {len(ia_weights)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecb7f64",
   "metadata": {},
   "source": [
    "## 5. Load Fine-Tuned Model\n",
    "\n",
    "Our best model: ESM-2 8M with Asymmetric Loss (F1 = 0.2331)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29968bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = base_dir / \"models\" / \"esm_finetuned\" / \"best_model\"\n",
    "\n",
    "print(f\"Loading model from {model_path}...\")\n",
    "\n",
    "# Check if model exists\n",
    "if not model_path.exists():\n",
    "    print(\"âŒ Model not found. Please train model first using src/training/finetune_esm.py\")\n",
    "else:\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Device: {device}\")\n",
    "    \n",
    "    # Load model\n",
    "    model = ESMForSequenceClassification.from_pretrained(str(model_path))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"âœ… Model loaded successfully\")\n",
    "    print(f\"Output dimension: {model.classifier.out_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35fa633",
   "metadata": {},
   "source": [
    "## 6. Create Validation Dataset\n",
    "\n",
    "Use same split as training (last 20% of data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c705e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating dataset...\")\n",
    "full_dataset = FineTuneDataset(\n",
    "    sequence_loader=seq_loader,\n",
    "    label_loader=label_loader,\n",
    "    max_length=512,\n",
    "    top_k_terms=5000\n",
    ")\n",
    "\n",
    "# Split (80/20)\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "val_dataset = Subset(full_dataset, range(train_size, len(full_dataset)))\n",
    "\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Top terms vocabulary: {len(full_dataset.top_terms)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd8b296",
   "metadata": {},
   "source": [
    "## 7. Evaluation Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c38e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_f1_with_threshold(y_true, y_pred, threshold, ia_weights_dict, term_list):\n",
    "    \"\"\"\n",
    "    Compute weighted F1 score.\n",
    "    \n",
    "    Args:\n",
    "        y_true: (N, K) binary labels\n",
    "        y_pred: (N, K) probabilities\n",
    "        threshold: float, prediction threshold\n",
    "        ia_weights_dict: dict mapping GO terms to IA weights\n",
    "        term_list: list of GO term IDs\n",
    "    \"\"\"\n",
    "    # Threshold predictions\n",
    "    y_pred_binary = (y_pred >= threshold).astype(int)\n",
    "    \n",
    "    # Get IA weights for our vocabulary\n",
    "    weights = np.array([ia_weights_dict.get(term, 1.0) for term in term_list])\n",
    "    \n",
    "    # Compute per-sample F1\n",
    "    f1_scores = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    \n",
    "    for i in range(len(y_true)):\n",
    "        true_pos = (y_true[i] == 1) & (y_pred_binary[i] == 1)\n",
    "        pred_pos = (y_pred_binary[i] == 1)\n",
    "        actual_pos = (y_true[i] == 1)\n",
    "        \n",
    "        # Weighted counts\n",
    "        tp = (true_pos * weights).sum()\n",
    "        fp = ((pred_pos & ~true_pos) * weights).sum()\n",
    "        fn = ((actual_pos & ~true_pos) * weights).sum()\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        f1_scores.append(f1)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "    \n",
    "    return {\n",
    "        'f1': np.mean(f1_scores),\n",
    "        'precision': np.mean(precisions),\n",
    "        'recall': np.mean(recalls)\n",
    "    }\n",
    "\n",
    "print(\"âœ… Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7be0b03",
   "metadata": {},
   "source": [
    "## 8. Generate Predictions on Validation Set\n",
    "\n",
    "Get model predictions for all validation samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4db69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create dataloader\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "print(\"Generating predictions...\")\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader, desc=\"Inference\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].cpu().numpy()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Convert to probabilities\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()\n",
    "        \n",
    "        all_preds.append(probs)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "# Concatenate\n",
    "all_preds = np.vstack(all_preds)\n",
    "all_labels = np.vstack(all_labels)\n",
    "\n",
    "print(f\"\\nPredictions shape: {all_preds.shape}\")\n",
    "print(f\"Labels shape: {all_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215c6afb",
   "metadata": {},
   "source": [
    "## 9. Evaluate WITHOUT Propagation\n",
    "\n",
    "Baseline: our current best performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f46660e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating WITHOUT propagation...\")\n",
    "print(\"Testing thresholds...\\n\")\n",
    "\n",
    "thresholds = [0.01, 0.05, 0.10, 0.15, 0.20, 0.30, 0.40, 0.50]\n",
    "results_baseline = []\n",
    "\n",
    "for thr in thresholds:\n",
    "    metrics = compute_f1_with_threshold(\n",
    "        all_labels, \n",
    "        all_preds, \n",
    "        thr, \n",
    "        ia_weights, \n",
    "        full_dataset.top_terms\n",
    "    )\n",
    "    results_baseline.append({\n",
    "        'threshold': thr,\n",
    "        'f1': metrics['f1'],\n",
    "        'precision': metrics['precision'],\n",
    "        'recall': metrics['recall']\n",
    "    })\n",
    "    print(f\"Thr={thr:.2f}: F1={metrics['f1']:.4f}, P={metrics['precision']:.4f}, R={metrics['recall']:.4f}\")\n",
    "\n",
    "# Find best\n",
    "best_baseline = max(results_baseline, key=lambda x: x['f1'])\n",
    "print(f\"\\nðŸ† Best WITHOUT propagation:\")\n",
    "print(f\"   F1 = {best_baseline['f1']:.4f} at threshold {best_baseline['threshold']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfad074",
   "metadata": {},
   "source": [
    "## 10. Apply Propagation\n",
    "\n",
    "Add ancestor terms to predictions using GO hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefeadf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Applying label propagation...\")\n",
    "\n",
    "# Convert predictions to torch for propagation function\n",
    "all_preds_torch = torch.from_numpy(all_preds)\n",
    "\n",
    "# Propagate (this adds ancestor probabilities)\n",
    "all_preds_propagated = propagate_predictions(\n",
    "    all_preds_torch,\n",
    "    full_dataset.top_terms,\n",
    "    ontology,\n",
    "    strategy='max'\n",
    ")\n",
    "\n",
    "# Convert back to numpy\n",
    "all_preds_propagated = all_preds_propagated.numpy()\n",
    "\n",
    "print(f\"âœ… Propagation complete\")\n",
    "print(f\"Shape unchanged: {all_preds_propagated.shape}\")\n",
    "\n",
    "# Check how many probabilities increased\n",
    "increased = (all_preds_propagated > all_preds).sum()\n",
    "total = all_preds.size\n",
    "print(f\"Probabilities boosted: {increased:,} / {total:,} ({100*increased/total:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d1d94a",
   "metadata": {},
   "source": [
    "## 11. Evaluate WITH Propagation\n",
    "\n",
    "Key question: did F1 improve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac610b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating WITH propagation...\")\n",
    "print(\"Testing thresholds...\\n\")\n",
    "\n",
    "results_propagated = []\n",
    "\n",
    "for thr in thresholds:\n",
    "    metrics = compute_f1_with_threshold(\n",
    "        all_labels, \n",
    "        all_preds_propagated, \n",
    "        thr, \n",
    "        ia_weights, \n",
    "        full_dataset.top_terms\n",
    "    )\n",
    "    results_propagated.append({\n",
    "        'threshold': thr,\n",
    "        'f1': metrics['f1'],\n",
    "        'precision': metrics['precision'],\n",
    "        'recall': metrics['recall']\n",
    "    })\n",
    "    print(f\"Thr={thr:.2f}: F1={metrics['f1']:.4f}, P={metrics['precision']:.4f}, R={metrics['recall']:.4f}\")\n",
    "\n",
    "# Find best\n",
    "best_propagated = max(results_propagated, key=lambda x: x['f1'])\n",
    "print(f\"\\nðŸ† Best WITH propagation:\")\n",
    "print(f\"   F1 = {best_propagated['f1']:.4f} at threshold {best_propagated['threshold']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3f7028",
   "metadata": {},
   "source": [
    "## 12. Compare Results\n",
    "\n",
    "Did we hit our target (F1 â‰¥ 0.25)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678a33dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"ðŸ“Š COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nBaseline (no propagation):\")\n",
    "print(f\"  F1:        {best_baseline['f1']:.4f}\")\n",
    "print(f\"  Precision: {best_baseline['precision']:.4f}\")\n",
    "print(f\"  Recall:    {best_baseline['recall']:.4f}\")\n",
    "print(f\"  Threshold: {best_baseline['threshold']}\")\n",
    "\n",
    "print(f\"\\nWith Propagation:\")\n",
    "print(f\"  F1:        {best_propagated['f1']:.4f}\")\n",
    "print(f\"  Precision: {best_propagated['precision']:.4f}\")\n",
    "print(f\"  Recall:    {best_propagated['recall']:.4f}\")\n",
    "print(f\"  Threshold: {best_propagated['threshold']}\")\n",
    "\n",
    "improvement = best_propagated['f1'] - best_baseline['f1']\n",
    "pct_improvement = 100 * improvement / best_baseline['f1']\n",
    "\n",
    "print(f\"\\n{'ðŸŽ‰' if improvement > 0 else 'âš ï¸'} Improvement:\")\n",
    "print(f\"  Î” F1:      {improvement:+.4f} ({pct_improvement:+.2f}%)\")\n",
    "\n",
    "if best_propagated['f1'] >= 0.25:\n",
    "    print(f\"\\nâœ… TARGET REACHED! F1 â‰¥ 0.25\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸ Target not reached (goal: 0.25, got: {best_propagated['f1']:.4f})\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82419f62",
   "metadata": {},
   "source": [
    "## 13. Visualize Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97280e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot F1 vs threshold\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: F1 curves\n",
    "ax1 = axes[0]\n",
    "baseline_f1s = [r['f1'] for r in results_baseline]\n",
    "propagated_f1s = [r['f1'] for r in results_propagated]\n",
    "\n",
    "ax1.plot(thresholds, baseline_f1s, 'o-', label='Without Propagation', linewidth=2)\n",
    "ax1.plot(thresholds, propagated_f1s, 's-', label='With Propagation', linewidth=2)\n",
    "ax1.axhline(y=0.25, color='red', linestyle='--', alpha=0.5, label='Target (0.25)')\n",
    "ax1.set_xlabel('Threshold', fontsize=12)\n",
    "ax1.set_ylabel('F1 Score', fontsize=12)\n",
    "ax1.set_title('F1 Score vs Threshold', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Right: Precision-Recall trade-off\n",
    "ax2 = axes[1]\n",
    "baseline_prec = [r['precision'] for r in results_baseline]\n",
    "baseline_rec = [r['recall'] for r in results_baseline]\n",
    "propagated_prec = [r['precision'] for r in results_propagated]\n",
    "propagated_rec = [r['recall'] for r in results_propagated]\n",
    "\n",
    "ax2.plot(baseline_rec, baseline_prec, 'o-', label='Without Propagation', linewidth=2)\n",
    "ax2.plot(propagated_rec, propagated_prec, 's-', label='With Propagation', linewidth=2)\n",
    "ax2.set_xlabel('Recall', fontsize=12)\n",
    "ax2.set_ylabel('Precision', fontsize=12)\n",
    "ax2.set_title('Precision-Recall Curve', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ“ˆ Plots generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907ee923",
   "metadata": {},
   "source": [
    "## 14. Detailed Example: How Propagation Works\n",
    "\n",
    "Pick a random sample and show term-by-term changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eacbb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a sample with decent number of predictions\n",
    "sample_idx = 100\n",
    "\n",
    "print(f\"Sample #{sample_idx}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get predictions\n",
    "preds_before = all_preds[sample_idx]\n",
    "preds_after = all_preds_propagated[sample_idx]\n",
    "true_labels = all_labels[sample_idx]\n",
    "\n",
    "# Find terms where probability increased\n",
    "increased_mask = preds_after > preds_before\n",
    "increased_indices = np.where(increased_mask)[0]\n",
    "\n",
    "print(f\"\\nTerms with boosted probability: {len(increased_indices)}\\n\")\n",
    "\n",
    "# Show top 10 boosted terms\n",
    "boosts = preds_after[increased_indices] - preds_before[increased_indices]\n",
    "top_boost_idx = increased_indices[np.argsort(boosts)[-10:][::-1]]\n",
    "\n",
    "for idx in top_boost_idx:\n",
    "    term = full_dataset.top_terms[idx]\n",
    "    name = ontology.graph.nodes[term].get('name', 'N/A') if term in ontology.graph else 'N/A'\n",
    "    before = preds_before[idx]\n",
    "    after = preds_after[idx]\n",
    "    true_val = true_labels[idx]\n",
    "    \n",
    "    true_str = 'âœ…' if true_val == 1 else ''\n",
    "    print(f\"{term} ({name[:40]}...)\")\n",
    "    print(f\"  Before: {before:.4f} â†’ After: {after:.4f} (+{after-before:.4f}) {true_str}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81bc0c6",
   "metadata": {},
   "source": [
    "## 15. Summary & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4502247e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"ðŸ“‹ SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nâœ… What we did:\")\n",
    "print(\"  1. Implemented label propagation using GO hierarchy\")\n",
    "print(\"  2. Evaluated on validation set with/without propagation\")\n",
    "print(\"  3. Measured F1 improvement\")\n",
    "\n",
    "print(\"\\nðŸ“Š Results:\")\n",
    "print(f\"  Baseline F1:     {best_baseline['f1']:.4f}\")\n",
    "print(f\"  Propagated F1:   {best_propagated['f1']:.4f}\")\n",
    "print(f\"  Improvement:     {improvement:+.4f} ({pct_improvement:+.2f}%)\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Next actions (from PLAN.md):\")\n",
    "print(\"  1. âœ… Label propagation (DONE)\")\n",
    "print(\"  2. â¬œ Per-aspect thresholds (MF/BP/CC separate)\")\n",
    "print(\"  3. â¬œ Simple ensemble (KNN + ESM)\")\n",
    "print(\"  4. â¬œ Larger backbone (ESM-2 35M)\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Key insight:\")\n",
    "if improvement > 0:\n",
    "    print(\"  Propagation helps! Ontology consistency = free performance.\")\n",
    "else:\n",
    "    print(\"  Propagation may need tuning (try different strategies).\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8034ff06",
   "metadata": {},
   "source": [
    "## 16. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993e1732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comparison to CSV\n",
    "results_df = pd.DataFrame({\n",
    "    'threshold': thresholds,\n",
    "    'f1_baseline': [r['f1'] for r in results_baseline],\n",
    "    'f1_propagated': [r['f1'] for r in results_propagated],\n",
    "    'precision_baseline': [r['precision'] for r in results_baseline],\n",
    "    'precision_propagated': [r['precision'] for r in results_propagated],\n",
    "    'recall_baseline': [r['recall'] for r in results_baseline],\n",
    "    'recall_propagated': [r['recall'] for r in results_propagated],\n",
    "})\n",
    "\n",
    "output_path = base_dir / \"results\" / \"propagation_comparison.csv\"\n",
    "output_path.parent.mkdir(exist_ok=True)\n",
    "results_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"âœ… Results saved to {output_path}\")\n",
    "print(\"\\nðŸ“Š Results preview:\")\n",
    "print(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
