{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9ba7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment configuration\n",
    "ENVIRONMENT = 'local'  # Change to 'kaggle' when running on Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4dbdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch transformers pandas numpy scikit-learn tqdm biopython -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb95cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from Bio import SeqIO\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"‚úÖ Imports successful | Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f175e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set base directory\n",
    "if ENVIRONMENT == 'kaggle':\n",
    "    base_dir = Path(\"/kaggle/input/cafa-6-dataset\")\n",
    "else:\n",
    "    base_dir = Path.cwd().parent\n",
    "\n",
    "print(f\"üìÅ Base directory: {base_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e1c9d7",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799496bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 8\n",
    "GRADIENT_ACCUMULATION = 4\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 10\n",
    "VOCAB_SIZE = 5000\n",
    "MIN_COUNT = 10\n",
    "PATIENCE = 3\n",
    "DROPOUT = 0.1\n",
    "\n",
    "# Model\n",
    "MODEL_NAME = \"facebook/esm2_t6_8M_UR50D\"\n",
    "\n",
    "# Paths\n",
    "SAVE_DIR = base_dir.parent / \"models\" / \"esm_finetuned\" if ENVIRONMENT == 'local' else Path(\"/kaggle/working/models/esm_finetuned\")\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION}\")\n",
    "print(f\"Vocabulary: Top {VOCAB_SIZE} GO terms\")\n",
    "print(f\"Save directory: {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cfb0e9",
   "metadata": {},
   "source": [
    "## 2. Load Data and Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b3c558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sequences\n",
    "print(\"Loading sequences...\")\n",
    "sequences = {}\n",
    "for record in SeqIO.parse(base_dir / \"Train\" / \"train_sequences.fasta\", \"fasta\"):\n",
    "    sequences[record.id] = str(record.seq)\n",
    "\n",
    "print(f\"Loaded {len(sequences)} sequences\")\n",
    "\n",
    "# Load annotations\n",
    "print(\"Loading annotations...\")\n",
    "train_terms = pd.read_csv(base_dir / \"Train\" / \"train_terms.tsv\", sep='\\t')\n",
    "print(f\"Total annotations: {len(train_terms)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1881e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary: top N most frequent GO terms\n",
    "print(\"Building vocabulary...\")\n",
    "term_counts = Counter(train_terms['term'])\n",
    "filtered_terms = {term: count for term, count in term_counts.items() if count >= MIN_COUNT}\n",
    "vocab_terms = [term for term, _ in sorted(filtered_terms.items(), key=lambda x: x[1], reverse=True)[:VOCAB_SIZE]]\n",
    "vocab = {term: idx for idx, term in enumerate(vocab_terms)}\n",
    "\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "print(f\"Coverage: {sum(filtered_terms[t] for t in vocab_terms) / len(train_terms):.2%} of annotations\")\n",
    "\n",
    "# Save vocabulary\n",
    "with open(SAVE_DIR / \"vocab.json\", \"w\") as f:\n",
    "    json.dump(vocab, f, indent=2)\n",
    "print(f\"‚úÖ Vocabulary saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41fd1cd",
   "metadata": {},
   "source": [
    "## 3. Create Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a9bcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, protein_ids, sequences_dict, annotations_df, vocab_dict, tokenizer, max_length=512):\n",
    "        self.protein_ids = protein_ids\n",
    "        self.sequences = sequences_dict\n",
    "        self.vocab = vocab_dict\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Build protein -> terms mapping\n",
    "        self.protein_terms = {}\n",
    "        for protein_id in protein_ids:\n",
    "            terms = annotations_df[annotations_df['EntryID'] == protein_id]['term'].tolist()\n",
    "            # Filter to vocabulary\n",
    "            terms_in_vocab = [t for t in terms if t in vocab_dict]\n",
    "            self.protein_terms[protein_id] = terms_in_vocab\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.protein_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        protein_id = self.protein_ids[idx]\n",
    "        sequence = self.sequences[protein_id]\n",
    "        \n",
    "        # Tokenize sequence\n",
    "        tokens = self.tokenizer(\n",
    "            sequence,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Create multi-hot label vector\n",
    "        labels = torch.zeros(len(self.vocab), dtype=torch.float32)\n",
    "        for term in self.protein_terms[protein_id]:\n",
    "            labels[self.vocab[term]] = 1.0\n",
    "        \n",
    "        return {\n",
    "            'input_ids': tokens['input_ids'].squeeze(0),\n",
    "            'attention_mask': tokens['attention_mask'].squeeze(0),\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ Dataset class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211dd813",
   "metadata": {},
   "source": [
    "## 4. Create Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6f15b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESMForGOPrediction(nn.Module):\n",
    "    def __init__(self, model_name, num_labels, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.esm = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(self.esm.config.hidden_size, num_labels)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get ESM-2 embeddings\n",
    "        outputs = self.esm(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Mean pooling\n",
    "        token_embeddings = outputs.last_hidden_state\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        pooled = sum_embeddings / sum_mask\n",
    "        \n",
    "        # Classification\n",
    "        pooled = self.dropout(pooled)\n",
    "        logits = self.classifier(pooled)\n",
    "        return logits\n",
    "\n",
    "print(\"‚úÖ Model class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc747711",
   "metadata": {},
   "source": [
    "## 5. Create Train/Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a73194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split proteins\n",
    "all_proteins = [p for p in train_terms['EntryID'].unique() if p in sequences]\n",
    "train_proteins, val_proteins = train_test_split(\n",
    "    all_proteins, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train proteins: {len(train_proteins)}\")\n",
    "print(f\"Val proteins: {len(val_proteins)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cfd6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Create datasets\n",
    "print(\"Creating datasets...\")\n",
    "train_dataset = ProteinDataset(train_proteins, sequences, train_terms, vocab, tokenizer)\n",
    "val_dataset = ProteinDataset(val_proteins, sequences, train_terms, vocab, tokenizer)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"‚úÖ Train batches: {len(train_loader)}\")\n",
    "print(f\"‚úÖ Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a199b8",
   "metadata": {},
   "source": [
    "## 6. Initialize Model and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70ddd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "print(\"Initializing model...\")\n",
    "model = ESMForGOPrediction(MODEL_NAME, len(vocab), dropout=DROPOUT).to(device)\n",
    "\n",
    "# Loss function (BCEWithLogitsLoss for multi-label)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Learning rate scheduler\n",
    "total_steps = len(train_loader) * NUM_EPOCHS // GRADIENT_ACCUMULATION\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.1 * total_steps),\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Model initialized | Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbcc91a",
   "metadata": {},
   "source": [
    "## 7. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b6942c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, criterion, device, threshold=0.5):\n",
    "    \"\"\"Evaluate model on validation set.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = (torch.sigmoid(logits) > threshold).cpu().numpy()\n",
    "            all_preds.append(preds)\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.vstack(all_preds)\n",
    "    all_labels = np.vstack(all_labels)\n",
    "    \n",
    "    # Compute F1 (sample-averaged)\n",
    "    f1_samples = []\n",
    "    for pred, label in zip(all_preds, all_labels):\n",
    "        if pred.sum() == 0:\n",
    "            f1_samples.append(0.0)\n",
    "        else:\n",
    "            f1_samples.append(f1_score(label, pred, average='binary', zero_division=0))\n",
    "    \n",
    "    return total_loss / len(dataloader), np.mean(f1_samples)\n",
    "\n",
    "print(\"‚úÖ Evaluation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346ae50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training...\\n\")\n",
    "\n",
    "best_f1 = 0\n",
    "patience_counter = 0\n",
    "history = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    progress = tqdm(train_loader, desc=\"Training\")\n",
    "    for batch_idx, batch in enumerate(progress):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss = loss / GRADIENT_ACCUMULATION\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights every GRADIENT_ACCUMULATION steps\n",
    "        if (batch_idx + 1) % GRADIENT_ACCUMULATION == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        train_loss += loss.item() * GRADIENT_ACCUMULATION\n",
    "        progress.set_postfix({'loss': train_loss / (batch_idx + 1)})\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    \n",
    "    # Validation\n",
    "    val_loss, val_f1 = evaluate_model(model, val_loader, criterion, device)\n",
    "    \n",
    "    print(f\"\\nTrain Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"Val F1: {val_f1:.4f}\")\n",
    "    \n",
    "    history.append({\n",
    "        'epoch': epoch + 1,\n",
    "        'train_loss': avg_train_loss,\n",
    "        'val_loss': val_loss,\n",
    "        'val_f1': val_f1\n",
    "    })\n",
    "    \n",
    "    # Save best model\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        patience_counter = 0\n",
    "        \n",
    "        best_model_dir = SAVE_DIR / \"best_model\"\n",
    "        best_model_dir.mkdir(exist_ok=True)\n",
    "        torch.save(model.state_dict(), best_model_dir / \"pytorch_model.bin\")\n",
    "        \n",
    "        # Save config\n",
    "        config = {\n",
    "            'model_name': MODEL_NAME,\n",
    "            'num_labels': len(vocab),\n",
    "            'dropout': DROPOUT,\n",
    "            'best_f1': float(best_f1)\n",
    "        }\n",
    "        with open(best_model_dir / \"config.json\", \"w\") as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "        \n",
    "        print(f\"üèÜ New best model saved! F1: {best_f1:.4f}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"No improvement. Patience: {patience_counter}/{PATIENCE}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= PATIENCE:\n",
    "        print(f\"\\n‚èπÔ∏è Early stopping triggered after {epoch+1} epochs\")\n",
    "        break\n",
    "\n",
    "print(f\"\\n‚úÖ Training complete! Best F1: {best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f793051e",
   "metadata": {},
   "source": [
    "## 8. Save Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05383f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save history\n",
    "history_df = pd.DataFrame(history)\n",
    "history_df.to_csv(SAVE_DIR / \"training_history.csv\", index=False)\n",
    "\n",
    "print(\"\\nüìä Training History:\")\n",
    "print(history_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\n‚úÖ Model and history saved to {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c30e2d",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**ESM-2 Fine-Tuned Classifier:**\n",
    "- Fine-tuned ESM-2 8M model with classification head\n",
    "- Trained on 5000 most common GO terms\n",
    "- Multi-label classification with BCE loss\n",
    "- Expected F1: ~0.23\n",
    "\n",
    "**Next:** 04_label_propagation.ipynb - Apply graph-based propagation to improve predictions"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
