{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6908b5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 13a - Setup & Data Loading (Phase 2 canonical)\n",
    "# =============================================\n",
    "# 4. PHASE 2: LEVEL-1 MODELS (DIVERSE ENSEMBLE)\n",
    "# =============================================\n",
    "# Target selection source-of-truth: Colab_04b_first_submission_no_ankh.ipynb (aspect-split Top-K)\n",
    "\n",
    "\n",
    "if TRAIN_LEVEL1:\n",
    "    import gc\n",
    "    import json\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import psutil\n",
    "\n",
    "    # AUDITOR: Hardware Check\n",
    "    try:\n",
    "        import torch\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"[AUDITOR] GPU Detected: {torch.cuda.get_device_name(0)}\")\n",
    "            print(\n",
    "                f\"[AUDITOR] VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\"\n",
    "            )\n",
    "        else:\n",
    "            print(\"[AUDITOR] WARNING: No GPU detected.\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    def log_mem(tag: str = \"\") -> None:\n",
    "        try:\n",
    "            mem = psutil.virtual_memory()\n",
    "            print(\n",
    "                f\"[MEM] {tag:<30} | Used: {mem.used/1e9:.2f}GB / {mem.total/1e9:.2f}GB ({mem.percent}%)\"\n",
    "            )\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # WORK_ROOT recovery (safety)\n",
    "    # Prefer canonical dataset root (cafa6_data/) and validate by presence of parsed artefacts.\n",
    "    if \"WORK_ROOT\" not in locals() and \"WORK_ROOT\" not in globals():\n",
    "        candidates = [\n",
    "            Path(\"/content/cafa6_data\"),\n",
    "            Path(\"/content/work\"),\n",
    "            Path(\"/kaggle/working/work\"),\n",
    "            Path.cwd() / \"cafa6_data\",\n",
    "            Path.cwd() / \"artefacts_local\" / \"work\",\n",
    "        ]\n",
    "\n",
    "        WORK_ROOT = None\n",
    "        for c in candidates:\n",
    "            if (c / \"parsed\" / \"train_terms.parquet\").exists():\n",
    "                WORK_ROOT = c\n",
    "                break\n",
    "\n",
    "        if WORK_ROOT is None:\n",
    "            for c in candidates:\n",
    "                if c.exists():\n",
    "                    WORK_ROOT = c\n",
    "                    break\n",
    "\n",
    "        if WORK_ROOT is None:\n",
    "            WORK_ROOT = Path.cwd() / \"cafa6_data\"\n",
    "\n",
    "        print(f\"WORK_ROOT recovered: {WORK_ROOT}\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Load targets + ids\n",
    "    # -----------------------------\n",
    "    print(\"Loading targets...\")\n",
    "    train_terms = pd.read_parquet(WORK_ROOT / \"parsed\" / \"train_terms.parquet\")\n",
    "    train_ids = pd.read_feather(WORK_ROOT / \"parsed\" / \"train_seq.feather\")[\"id\"].astype(str)\n",
    "    test_ids = pd.read_feather(WORK_ROOT / \"parsed\" / \"test_seq.feather\")[\"id\"].astype(str)\n",
    "\n",
    "    # FIX: Clean IDs in train_ids to match EntryID format\n",
    "    print(\"Applying ID cleaning fix...\")\n",
    "    train_ids_clean = train_ids.str.extract(r\"\\|(.*?)\\|\")[0]\n",
    "    train_ids_clean = train_ids_clean.fillna(train_ids)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Target Matrix Construction (Champion Strategy: 13,500 Terms)\n",
    "    # 10,000 BP + 2,000 MF + 1,500 CC\n",
    "    # -----------------------------\n",
    "    print(\"Selecting Top-K terms per aspect (Champion Strategy)...\")\n",
    "\n",
    "    try:\n",
    "        import obonet\n",
    "\n",
    "        # Robust OBO Path Search\n",
    "        possible_paths = [\n",
    "            WORK_ROOT / \"go-basic.obo\",\n",
    "            WORK_ROOT / \"Train\" / \"go-basic.obo\",\n",
    "            WORK_ROOT.parent / \"go-basic.obo\",\n",
    "            Path(\"go-basic.obo\"),\n",
    "            Path(\"Train/go-basic.obo\"),\n",
    "            Path(\"../Train/go-basic.obo\"),\n",
    "            Path(\"/content/cafa6_data/Train/go-basic.obo\"),\n",
    "        ]\n",
    "\n",
    "        obo_path = None\n",
    "        for p in possible_paths:\n",
    "            if p.exists():\n",
    "                obo_path = p\n",
    "                break\n",
    "\n",
    "        if obo_path is None:\n",
    "            raise FileNotFoundError(\n",
    "                f\"CRITICAL: go-basic.obo not found. Searched: {[str(p) for p in possible_paths]}\"\n",
    "            )\n",
    "\n",
    "        global PATH_GO_OBO\n",
    "        PATH_GO_OBO = obo_path\n",
    "        print(f\"Global PATH_GO_OBO set to: {PATH_GO_OBO}\")\n",
    "\n",
    "        print(f\"Loading OBO from {obo_path}...\")\n",
    "        graph = obonet.read_obo(obo_path)\n",
    "        term_to_ns = {\n",
    "            node: data.get(\"namespace\", \"unknown\") for node, data in graph.nodes(data=True)\n",
    "        }\n",
    "\n",
    "        # Keep compatibility with downstream code that expects go_namespaces\n",
    "        go_namespaces = term_to_ns\n",
    "\n",
    "        ns_map = {\n",
    "            \"biological_process\": \"BP\",\n",
    "            \"molecular_function\": \"MF\",\n",
    "            \"cellular_component\": \"CC\",\n",
    "        }\n",
    "\n",
    "        # Normalise any existing aspect column (some artefacts store full namespace strings)\n",
    "        aspect_aliases = {\n",
    "            \"biological_process\": \"BP\",\n",
    "            \"molecular_function\": \"MF\",\n",
    "            \"cellular_component\": \"CC\",\n",
    "            \"BP\": \"BP\",\n",
    "            \"MF\": \"MF\",\n",
    "            \"CC\": \"CC\",\n",
    "        }\n",
    "        if \"aspect\" in train_terms.columns:\n",
    "            train_terms[\"aspect\"] = train_terms[\"aspect\"].map(\n",
    "                lambda a: aspect_aliases.get(str(a), \"UNK\")\n",
    "            )\n",
    "        else:\n",
    "            train_terms[\"aspect\"] = train_terms[\"term\"].map(\n",
    "                lambda t: ns_map.get(term_to_ns.get(t), \"UNK\")\n",
    "            )\n",
    "\n",
    "    except ImportError as e:\n",
    "        raise RuntimeError(\"obonet not installed. Please install it.\") from e\n",
    "\n",
    "    # Canonical aspect split (04b)\n",
    "    term_counts = train_terms.groupby([\"aspect\", \"term\"]).size().reset_index(name=\"count\")\n",
    "    targets_bp = (\n",
    "        term_counts[term_counts[\"aspect\"] == \"BP\"].nlargest(10000, \"count\")[\"term\"].tolist()\n",
    "    )\n",
    "    targets_mf = (\n",
    "        term_counts[term_counts[\"aspect\"] == \"MF\"].nlargest(2000, \"count\")[\"term\"].tolist()\n",
    "    )\n",
    "    targets_cc = (\n",
    "        term_counts[term_counts[\"aspect\"] == \"CC\"].nlargest(1500, \"count\")[\"term\"].tolist()\n",
    "    )\n",
    "\n",
    "    # Guardrail: avoid silently switching target strategy due to aspect encoding mismatch\n",
    "    ALLOW_GLOBAL_FALLBACK = False\n",
    "    if len(targets_bp) == 0 and len(targets_mf) == 0 and len(targets_cc) == 0:\n",
    "        aspect_vc = train_terms[\"aspect\"].value_counts().to_dict() if \"aspect\" in train_terms.columns else {}\n",
    "        msg = (\n",
    "            \"No BP/MF/CC aspect split found after normalisation. \"\n",
    "            f\"aspect_vc={aspect_vc}. This would fall back to global Top-13,500; \"\n",
    "            \"set ALLOW_GLOBAL_FALLBACK=True to override.\"\n",
    "        )\n",
    "        if ALLOW_GLOBAL_FALLBACK:\n",
    "            print(\"  [WARNING] \" + msg)\n",
    "            top_terms = train_terms[\"term\"].value_counts().head(13500).index.tolist()\n",
    "        else:\n",
    "            raise RuntimeError(msg)\n",
    "    else:\n",
    "        # Stable, deterministic ordering: BP then MF then CC with de-dup preserving order\n",
    "        top_terms = []\n",
    "        seen = set()\n",
    "        for t in (targets_bp + targets_mf + targets_cc):\n",
    "            if t not in seen:\n",
    "                top_terms.append(t)\n",
    "                seen.add(t)\n",
    "        print(f\"  Selected: {len(targets_bp)} BP + {len(targets_mf)} MF + {len(targets_cc)} CC\")\n",
    "\n",
    "    # Persist label contract for downstream stages\n",
    "    top_terms_path = WORK_ROOT / \"features\" / \"top_terms_13500.json\"\n",
    "    top_terms_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if top_terms_path.exists():\n",
    "        try:\n",
    "            with open(top_terms_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                top_terms_disk = json.load(f)\n",
    "            if isinstance(top_terms_disk, list) and len(top_terms_disk) > 0:\n",
    "                top_terms = [str(x) for x in top_terms_disk]\n",
    "                print(f\"Loaded existing top_terms_13500.json (n={len(top_terms)})\")\n",
    "        except Exception as e:\n",
    "            print(f\"[WARNING] Failed to load existing top_terms_13500.json: {e}\")\n",
    "    else:\n",
    "        with open(top_terms_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(list(top_terms), f)\n",
    "        print(\"Saved: top_terms_13500.json\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Stable target contract (audited: 1,585 terms)\n",
    "    # Definition: GO terms with >= 50 positives AND valid namespace (BP/MF/CC)\n",
    "    # Stored separately from top_terms_13500.json (do not mix contracts).\n",
    "    # -----------------------------\n",
    "    stable_terms_path = WORK_ROOT / \"features\" / \"stable_terms_1585.json\"\n",
    "    stable_meta_path = WORK_ROOT / \"features\" / \"stable_terms_1585_meta.json\"\n",
    "    noise_floor = 50\n",
    "\n",
    "    if stable_terms_path.exists():\n",
    "        try:\n",
    "            stable_terms = json.loads(stable_terms_path.read_text(encoding=\"utf-8\"))\n",
    "            stable_terms = [str(t) for t in stable_terms]\n",
    "            print(f\"Loaded existing stable_terms_1585.json (n={len(stable_terms)})\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to load {stable_terms_path}: {e}\")\n",
    "    else:\n",
    "        # Compute from Phase-1 truth (train_terms.parquet) and OBO namespace mapping already loaded above.\n",
    "        stable_bp = (\n",
    "            term_counts[(term_counts[\"aspect\"] == \"BP\") & (term_counts[\"count\"] >= noise_floor)]\n",
    "            .sort_values([\"count\", \"term\"], ascending=[False, True])[\"term\"]\n",
    "            .astype(str)\n",
    "            .tolist()\n",
    "        )\n",
    "        stable_mf = (\n",
    "            term_counts[(term_counts[\"aspect\"] == \"MF\") & (term_counts[\"count\"] >= noise_floor)]\n",
    "            .sort_values([\"count\", \"term\"], ascending=[False, True])[\"term\"]\n",
    "            .astype(str)\n",
    "            .tolist()\n",
    "        )\n",
    "        stable_cc = (\n",
    "            term_counts[(term_counts[\"aspect\"] == \"CC\") & (term_counts[\"count\"] >= noise_floor)]\n",
    "            .sort_values([\"count\", \"term\"], ascending=[False, True])[\"term\"]\n",
    "            .astype(str)\n",
    "            .tolist()\n",
    "        )\n",
    "        stable_terms = stable_bp + stable_mf + stable_cc\n",
    "        stable_terms_path.write_text(json.dumps(stable_terms), encoding=\"utf-8\")\n",
    "        stable_meta_path.write_text(\n",
    "            json.dumps(\n",
    "                {\n",
    "                    \"noise_floor\": noise_floor,\n",
    "                    \"counts\": {\"BP\": len(stable_bp), \"MF\": len(stable_mf), \"CC\": len(stable_cc)},\n",
    "                    \"total\": len(stable_terms),\n",
    "                },\n",
    "                indent=2,\n",
    "            ),\n",
    "            encoding=\"utf-8\",\n",
    "        )\n",
    "        print(f\"Saved: stable_terms_1585.json (n={len(stable_terms)})\")\n",
    "\n",
    "    if len(stable_terms) != 1585:\n",
    "        raise RuntimeError(f\"Stable term contract mismatch: expected 1585, got {len(stable_terms)}\")\n",
    "\n",
    "    top_term_to_idx = {t: i for i, t in enumerate(top_terms)}\n",
    "    missing_stable = [t for t in stable_terms if t not in top_term_to_idx]\n",
    "    if missing_stable:\n",
    "        raise RuntimeError(\n",
    "            \"Stable terms contain items not present in top_terms_13500.json. \"\n",
    "            f\"Missing={len(missing_stable)} (example: {missing_stable[:10]})\"\n",
    "        )\n",
    "\n",
    "    stable_idx = np.asarray([top_term_to_idx[t] for t in stable_terms], dtype=np.int64)\n",
    "    print(f\"Stable targets ready: n={int(stable_idx.shape[0])} (expected 1585)\")\n",
    "\n",
    "    train_terms_top = train_terms[train_terms[\"term\"].isin(top_terms)]\n",
    "    Y_df = train_terms_top.pivot_table(index=\"EntryID\", columns=\"term\", aggfunc=\"size\", fill_value=0)\n",
    "    Y_df = Y_df.reindex(train_ids_clean, fill_value=0)\n",
    "    Y = Y_df.values.astype(np.float32)\n",
    "    print(f\"Targets: Y={Y.shape}\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Feature loading helper (Memory Optimised)\n",
    "    # -----------------------------\n",
    "    FEAT_DIR = WORK_ROOT / \"features\"\n",
    "\n",
    "    def load_features_dict(split: str = \"both\"):\n",
    "        log_mem(f\"Start load_features_dict({split})\")\n",
    "        print(f\"Loading multimodal features (mode={split})...\")\n",
    "\n",
    "        ft_train = {}\n",
    "        ft_test = {}\n",
    "\n",
    "        def _load_pair(stem: str):\n",
    "            tr = FEAT_DIR / f\"train_embeds_{stem}.npy\"\n",
    "            te = FEAT_DIR / f\"test_embeds_{stem}.npy\"\n",
    "            return tr, te\n",
    "\n",
    "        # All modalities are mandatory.\n",
    "        stems = [\n",
    "            (\"t5\", \"t5\"),\n",
    "            (\"esm2\", \"esm2_650m\"),\n",
    "            (\"esm2_3b\", \"esm2_3b\"),\n",
    "            (\"ankh\", \"ankh\"),\n",
    "            (\"text\", \"text\"),\n",
    "        ]\n",
    "\n",
    "        for stem, key in stems:\n",
    "            tr_path, te_path = _load_pair(stem)\n",
    "            if not (tr_path.exists() and te_path.exists()):\n",
    "                raise FileNotFoundError(f\"Missing mandatory embeddings for {stem}: {tr_path} or {te_path}\")\n",
    "\n",
    "            if split in [\"both\", \"train\"]:\n",
    "                ft_train[key] = np.load(tr_path, mmap_mode=\"r\")\n",
    "            if split in [\"both\", \"test\"]:\n",
    "                ft_test[key] = np.load(te_path, mmap_mode=\"r\")\n",
    "\n",
    "        taxa_train_path = WORK_ROOT / \"parsed\" / \"train_taxa.feather\"\n",
    "        taxa_test_path = WORK_ROOT / \"parsed\" / \"test_taxa.feather\"\n",
    "\n",
    "        if not (taxa_train_path.exists() and taxa_test_path.exists()):\n",
    "            raise FileNotFoundError(f\"Missing mandatory taxa features: {taxa_train_path} or {taxa_test_path}\")\n",
    "\n",
    "        from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "        tax_tr = pd.read_feather(taxa_train_path).astype({\"id\": str})\n",
    "        tax_te = pd.read_feather(taxa_test_path).astype({\"id\": str})\n",
    "        enc = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False, dtype=np.float32)\n",
    "        enc.fit(pd.concat([tax_tr[[\"taxon_id\"]], tax_te[[\"taxon_id\"]]], axis=0))\n",
    "\n",
    "        if split in [\"both\", \"train\"]:\n",
    "            tax_tr = tax_tr.set_index(\"id\").reindex(train_ids, fill_value=0).reset_index()\n",
    "            ft_train[\"taxa\"] = enc.transform(tax_tr[[\"taxon_id\"]]).astype(np.float32)\n",
    "        if split in [\"both\", \"test\"]:\n",
    "            tax_te = tax_te.set_index(\"id\").reindex(test_ids, fill_value=0).reset_index()\n",
    "            ft_test[\"taxa\"] = enc.transform(tax_te[[\"taxon_id\"]]).astype(np.float32)\n",
    "\n",
    "        log_mem(f\"End load_features_dict({split})\")\n",
    "        if split == \"train\":\n",
    "            return ft_train\n",
    "        if split == \"test\":\n",
    "            return ft_test\n",
    "        return ft_train, ft_test\n",
    "\n",
    "    # Materialise feature dicts (mmap arrays where possible)\n",
    "    features_train, features_test = load_features_dict(split=\"both\")\n",
    "\n",
    "    # Flat concatenation order for classical models (LR/GBDT)\n",
    "    FLAT_KEYS = [k for k in [\"t5\", \"esm2_650m\", \"esm2_3b\", \"ankh\", \"text\", \"taxa\"] if k in features_train]\n",
    "    if \"ankh\" not in FLAT_KEYS:\n",
    "        raise RuntimeError(\"Ankh is mandatory but was not loaded into features_train.\")\n",
    "    print(f\"Flat X keys={FLAT_KEYS}\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Disk-backed X / X_test (for RAM-safe downstream cells)\n",
    "    # -----------------------------\n",
    "    X_train_path = FEAT_DIR / \"X_train_mmap.npy\"\n",
    "    X_test_path = FEAT_DIR / \"X_test_mmap.npy\"\n",
    "\n",
    "    def _build_X_memmaps(chunk_size: int = 10000) -> None:\n",
    "        dims = {k: int(features_train[k].shape[1]) for k in FLAT_KEYS}\n",
    "        total_dim = int(sum(dims.values()))\n",
    "        n_tr = int(len(train_ids))\n",
    "        n_te = int(len(test_ids))\n",
    "\n",
    "        print(f\"Building X memmaps: train=({n_tr}, {total_dim}) test=({n_te}, {total_dim})\")\n",
    "        X_mm = np.lib.format.open_memmap(\n",
    "            str(X_train_path), mode=\"w+\", dtype=np.float32, shape=(n_tr, total_dim)\n",
    "        )\n",
    "        Xte_mm = np.lib.format.open_memmap(\n",
    "            str(X_test_path), mode=\"w+\", dtype=np.float32, shape=(n_te, total_dim)\n",
    "        )\n",
    "\n",
    "        col = 0\n",
    "        for k in FLAT_KEYS:\n",
    "            d = dims[k]\n",
    "            print(f\"  Streaming {k} into cols {col}:{col + d}\")\n",
    "            for i in range(0, n_tr, chunk_size):\n",
    "                j = min(i + chunk_size, n_tr)\n",
    "                X_mm[i:j, col : col + d] = np.asarray(features_train[k][i:j], dtype=np.float32)\n",
    "            for i in range(0, n_te, chunk_size):\n",
    "                j = min(i + chunk_size, n_te)\n",
    "                Xte_mm[i:j, col : col + d] = np.asarray(features_test[k][i:j], dtype=np.float32)\n",
    "            col += d\n",
    "\n",
    "        X_mm.flush()\n",
    "        Xte_mm.flush()\n",
    "\n",
    "    if X_train_path.exists() and X_test_path.exists():\n",
    "        print(\"X memmaps already exist; skipping build.\")\n",
    "    else:\n",
    "        _build_X_memmaps(chunk_size=5000)\n",
    "\n",
    "    X = np.load(X_train_path, mmap_mode=\"r\")\n",
    "    X_test = np.load(X_test_path, mmap_mode=\"r\")\n",
    "\n",
    "    log_mem(\"Phase 2 setup done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99aa48b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 13E - KNN (GPU-accelerated, IA-weighted) + checkpoint push\n",
    "# ===================================================================\n",
    "# RANK-1 Winning Strategy Implementation:\n",
    "#   1. cuML GPU acceleration (10-20× speedup on A100)\n",
    "#   2. L2 normalization → Euclidean = Cosine (Manual GEMM Fast Path)\n",
    "#   3. IA-weighted neighbor voting (prioritises rare/high-value terms)\n",
    "#   4. Aspect-specific calibration (BP: 0.25, MF: 0.35, CC: 0.35)\n",
    "#   5. Finite-value quality gates (prevents NaN corruption of GCN stacker)\n",
    "\n",
    "if not TRAIN_LEVEL1:\n",
    "    print('Skipping KNN (TRAIN_LEVEL1=False).')\n",
    "else:\n",
    "    import json\n",
    "    import time\n",
    "    import numpy as np\n",
    "    from pathlib import Path\n",
    "    from sklearn.model_selection import KFold\n",
    "\n",
    "    # Telemetry (best-effort)\n",
    "    try:\n",
    "        import psutil\n",
    "    except Exception:\n",
    "        psutil = None\n",
    "\n",
    "    try:\n",
    "        import cupy as cp\n",
    "    except Exception:\n",
    "        cp = None\n",
    "\n",
    "    def _fmt_s(seconds: float) -> str:\n",
    "        seconds = float(max(0.0, seconds))\n",
    "        if seconds < 60:\n",
    "            return f'{seconds:.1f}s'\n",
    "        minutes = seconds / 60.0\n",
    "        if minutes < 60:\n",
    "            return f'{minutes:.1f}min'\n",
    "        hours = minutes / 60.0\n",
    "        return f'{hours:.2f}h'\n",
    "\n",
    "    def _sys_parts() -> list[str]:\n",
    "        parts: list[str] = []\n",
    "        if psutil is not None:\n",
    "            try:\n",
    "                p = psutil.Process()\n",
    "                rss_gb = p.memory_info().rss / (1024**3)\n",
    "                avail_gb = psutil.virtual_memory().available / (1024**3)\n",
    "                parts.append(f'rss={rss_gb:.1f}GiB')\n",
    "                parts.append(f'ram_avail={avail_gb:.1f}GiB')\n",
    "            except Exception:\n",
    "                pass\n",
    "        if cp is not None:\n",
    "            try:\n",
    "                free_b, total_b = cp.cuda.runtime.memGetInfo()\n",
    "                used_gb = (total_b - free_b) / (1024**3)\n",
    "                parts.append(f'vram_used={used_gb:.1f}GiB')\n",
    "                parts.append(f'vram_total={total_b/(1024**3):.1f}GiB')\n",
    "                pool = cp.get_default_memory_pool()\n",
    "                parts.append(f'cp_pool_total={pool.total_bytes()/(1024**3):.1f}GiB')\n",
    "                parts.append(f'cp_pool_used={pool.used_bytes()/(1024**3):.1f}GiB')\n",
    "            except Exception:\n",
    "                pass\n",
    "        return parts\n",
    "\n",
    "    def _hb(prefix: str, **kv):\n",
    "        parts = [prefix]\n",
    "        for k, v in kv.items():\n",
    "            parts.append(f'{k}={v}')\n",
    "        sys_p = _sys_parts()\n",
    "        if sys_p:\n",
    "            parts.extend(sys_p)\n",
    "        print(' '.join(parts))\n",
    "\n",
    "    overall_t0 = time.time()\n",
    "\n",
    "    # RANK-1: cuML migration with runtime detection\n",
    "    try:\n",
    "        from cuml.neighbors import NearestNeighbors as cuNearestNeighbors\n",
    "\n",
    "        USE_CUML = True\n",
    "        print('[KNN] Using cuML NearestNeighbors (GPU-accelerated)')\n",
    "    except ImportError:\n",
    "        from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "        USE_CUML = False\n",
    "        print('[WARN] cuML not available; using sklearn NearestNeighbors (CPU, slower). Install cuML for Rank-1 performance.')\n",
    "\n",
    "    WORK_ROOT = Path(WORK_ROOT)\n",
    "    FEAT_DIR = WORK_ROOT / 'features'\n",
    "    PRED_DIR = FEAT_DIR / 'level1_preds'\n",
    "    PRED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # RANK-1: Load aspect-specific thresholds\n",
    "    thr_path = FEAT_DIR / 'aspect_thresholds.json'\n",
    "    if not thr_path.exists():\n",
    "        print(\n",
    "            f'[WARN] aspect_thresholds.json not found at {thr_path}. Run Cell 13F first for per-aspect thresholds (proven +3.3% F1 boost).'\n",
    "        )\n",
    "        ASPECT_THRESHOLDS = {'ALL': 0.3, 'BP': 0.25, 'MF': 0.35, 'CC': 0.35, 'UNK': 0.3}\n",
    "    else:\n",
    "        ASPECT_THRESHOLDS = json.loads(thr_path.read_text(encoding='utf-8'))\n",
    "        print(f'[KNN] Loaded aspect thresholds: {ASPECT_THRESHOLDS}')\n",
    "\n",
    "    knn_oof_path = PRED_DIR / 'oof_pred_knn.npy'\n",
    "    knn_test_path = PRED_DIR / 'test_pred_knn.npy'\n",
    "\n",
    "    # Backwards-compatible copies (some downstream code loads from WORK_ROOT/features)\n",
    "    knn_oof_compat = WORK_ROOT / 'features' / 'oof_pred_knn.npy'\n",
    "    knn_test_compat = WORK_ROOT / 'features' / 'test_pred_knn.npy'\n",
    "\n",
    "    if knn_oof_path.exists() and knn_test_path.exists():\n",
    "        oof_pred_knn = np.load(knn_oof_path)\n",
    "        test_pred_knn = np.load(knn_test_path)\n",
    "        oof_max_sim = None\n",
    "        print('Loaded existing KNN preds:', knn_oof_path.name, knn_test_path.name)\n",
    "    else:\n",
    "        if 'features_train' not in globals() or 'features_test' not in globals():\n",
    "            raise RuntimeError('Missing `features_train`/`features_test`. Run the Phase 2 feature load cell first.')\n",
    "        if 'Y' not in globals():\n",
    "            raise RuntimeError('Missing Y. Run Cell 13a first (targets).')\n",
    "        if 'esm2_3b' not in features_train:\n",
    "            raise FileNotFoundError(\n",
    "                \"Missing required modality 'esm2_3b' in features_train. Ensure features/train_embeds_esm2_3b.npy exists.\"\n",
    "            )\n",
    "\n",
    "        # ---- Load IA weights for weighted neighbour voting ----\n",
    "        top_terms_path = FEAT_DIR / 'top_terms_13500.json'\n",
    "        if not top_terms_path.exists():\n",
    "            raise FileNotFoundError(f'Missing {top_terms_path}. Run Cell 13a first.')\n",
    "        top_terms = [str(t) for t in json.loads(top_terms_path.read_text(encoding='utf-8'))]\n",
    "\n",
    "        ia_path = WORK_ROOT / 'IA.tsv'\n",
    "        if not ia_path.exists():\n",
    "            raise FileNotFoundError(f'Missing IA.tsv at {ia_path}')\n",
    "\n",
    "        import pandas as pd\n",
    "\n",
    "        ia_df = pd.read_csv(ia_path, sep='\\t')\n",
    "        term_col = 'term' if 'term' in ia_df.columns else ('#term' if '#term' in ia_df.columns else ia_df.columns[0])\n",
    "        ia_col = 'ia' if 'ia' in ia_df.columns else (ia_df.columns[1] if len(ia_df.columns) > 1 else ia_df.columns[0])\n",
    "        ia_map = dict(zip(ia_df[term_col].astype(str).values, ia_df[ia_col].astype(np.float32).values))\n",
    "        weights_full = np.asarray([ia_map.get(t, np.float32(1.0)) for t in top_terms], dtype=np.float32)\n",
    "        print(\n",
    "            f'[KNN] IA weights ready: shape={weights_full.shape} min={float(weights_full.min()):.4f} max={float(weights_full.max()):.4f}'\n",
    "        )\n",
    "\n",
    "        # ---- Prepare embeddings ----\n",
    "        X_knn = features_train['esm2_3b'].astype(np.float32)\n",
    "        X_knn_test = features_test['esm2_3b'].astype(np.float32)\n",
    "        Y_knn = Y.astype(np.float32)\n",
    "\n",
    "        # RANK-1: L2 pre-normalisation (transforms cosine→dot-product for Manual GEMM Fast Path)\n",
    "        def _l2_norm(X):\n",
    "            norms = np.linalg.norm(X, axis=1, keepdims=True)\n",
    "            norms = np.maximum(norms, 1e-12)\n",
    "            return (X / norms).astype(np.float32)\n",
    "\n",
    "        t_norm = time.time()\n",
    "        X_knn = _l2_norm(X_knn)\n",
    "        X_knn_test = _l2_norm(X_knn_test)\n",
    "        print(f'[KNN] L2-normalised embeddings: train={X_knn.shape} test={X_knn_test.shape} (took {_fmt_s(time.time()-t_norm)})')\n",
    "\n",
    "        # ---- KNN parameters ----\n",
    "        KNN_K = int(globals().get('KNN_K', 50))\n",
    "        KNN_BATCH = int(globals().get('KNN_BATCH', 4096))  # Memory pipelining for A100\n",
    "        n_splits = 5\n",
    "\n",
    "        print(f'[KNN] Config: k={KNN_K} batch={KNN_BATCH} folds={n_splits} backend={\"cuML\" if USE_CUML else \"sklearn\"}')\n",
    "\n",
    "        # ---- Initialise accumulators ----\n",
    "        train_n = int(X_knn.shape[0])\n",
    "        test_n = int(X_knn_test.shape[0])\n",
    "        out_dim = int(Y_knn.shape[1])\n",
    "\n",
    "        oof_pred_knn = np.zeros((train_n, out_dim), dtype=np.float32)\n",
    "        test_pred_knn = np.zeros((test_n, out_dim), dtype=np.float32)\n",
    "        oof_max_sim = np.zeros((train_n,), dtype=np.float32)\n",
    "\n",
    "        # Broadcast IA weights: (1, 1, L)\n",
    "        w_ia_broadcast = weights_full[np.newaxis, np.newaxis, :]\n",
    "\n",
    "        # ---- 5-Fold Cross-Validation ----\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        fold_t0 = time.time()\n",
    "        folds_done = 0\n",
    "\n",
    "        for fold, (tr_idx, va_idx) in enumerate(kf.split(X_knn), start=1):\n",
    "            fold_start = time.time()\n",
    "            print(f'\\n[KNN] ═══ Fold {fold}/{n_splits} ═══')\n",
    "            print(f'  Train samples: {len(tr_idx)}, Val samples: {len(va_idx)}')\n",
    "            _hb('  [KNN][HB]', phase='start_fold')\n",
    "\n",
    "            # Fit KNN on training fold\n",
    "            if USE_CUML:\n",
    "                knn = cuNearestNeighbors(n_neighbors=KNN_K, metric='euclidean')\n",
    "            else:\n",
    "                knn = NearestNeighbors(n_neighbors=KNN_K, metric='cosine', n_jobs=-1)\n",
    "\n",
    "            t_fit = time.time()\n",
    "            knn.fit(X_knn[tr_idx])\n",
    "            print(f'  [KNN] Fit complete in {_fmt_s(time.time()-t_fit)}')\n",
    "\n",
    "            # Compute neighbours for validation fold\n",
    "            t_kn = time.time()\n",
    "            dists_va, neigh_va = knn.kneighbors(X_knn[va_idx], return_distance=True)\n",
    "            print(f'  [KNN] kneighbors(val) complete in {_fmt_s(time.time()-t_kn)}')\n",
    "\n",
    "            # Convert distances to similarities\n",
    "            # sklearn cosine: distance = 1 - similarity\n",
    "            # cuML euclidean on L2-normalised vectors: we still use (1 - dist) proxy as previously\n",
    "            sims_va = np.clip((1.0 - dists_va).astype(np.float32), 0.0, 1.0)\n",
    "\n",
    "            # Global neighbour indices (map fold-local to global)\n",
    "            neigh_global = tr_idx[neigh_va]\n",
    "\n",
    "            # Batched aggregation (memory pipelining)\n",
    "            agg_t0 = time.time()\n",
    "            n_val = int(len(va_idx))\n",
    "            n_batches = int((n_val + KNN_BATCH - 1) // KNN_BATCH)\n",
    "            for b, i in enumerate(range(0, n_val, KNN_BATCH), start=1):\n",
    "                j = min(i + KNN_BATCH, n_val)\n",
    "\n",
    "                neigh_b = neigh_global[i:j]  # (B, K)\n",
    "                sims_b = sims_va[i:j]        # (B, K)\n",
    "\n",
    "                # Fetch neighbour labels\n",
    "                Y_nei = Y_knn[neigh_b]  # (B, K, L)\n",
    "\n",
    "                weighted_votes = (sims_b[:, :, np.newaxis] * Y_nei * w_ia_broadcast).sum(axis=1)\n",
    "                denom = np.maximum(sims_b.sum(axis=1, keepdims=True), 1e-8)\n",
    "                scores = (weighted_votes / denom).astype(np.float32)\n",
    "\n",
    "                oof_pred_knn[va_idx[i:j]] = scores\n",
    "\n",
    "                # Heartbeat (OOF aggregation)\n",
    "                if b == 1 or b == n_batches or (b % 10 == 0):\n",
    "                    elapsed = time.time() - agg_t0\n",
    "                    avg = elapsed / b\n",
    "                    eta = avg * (n_batches - b)\n",
    "                    _hb(\n",
    "                        '  [KNN][HB]',\n",
    "                        phase='oof_agg',\n",
    "                        batch=f'{b}/{n_batches}',\n",
    "                        pct=f'{(100.0*b/n_batches):.0f}%',\n",
    "                        elapsed=_fmt_s(elapsed),\n",
    "                        eta=_fmt_s(eta),\n",
    "                    )\n",
    "\n",
    "            # Track max similarity for diagnostics\n",
    "            oof_max_sim[va_idx] = sims_va.max(axis=1)\n",
    "            print(f'  [KNN] Fold {fold} OOF complete in {_fmt_s(time.time()-fold_start)}')\n",
    "\n",
    "            folds_done += 1\n",
    "            fold_elapsed = time.time() - fold_t0\n",
    "            avg_fold = fold_elapsed / folds_done\n",
    "            eta_folds = avg_fold * (n_splits - folds_done)\n",
    "            print(f'  [KNN] Fold progress: {folds_done}/{n_splits} | ETA(folds)={_fmt_s(eta_folds)} (avg_fold={_fmt_s(avg_fold)})')\n",
    "\n",
    "        # Quality gate - Check OOF predictions\n",
    "        if not np.isfinite(oof_pred_knn).all():\n",
    "            nan_count = int((~np.isfinite(oof_pred_knn)).sum())\n",
    "            print(f'[WARN] KNN OOF contains {nan_count} NaN/Inf values; clipping to valid range [0, 1]')\n",
    "            oof_pred_knn = np.clip(np.nan_to_num(oof_pred_knn, nan=0.0, posinf=1.0, neginf=0.0), 0.0, 1.0)\n",
    "\n",
    "        # ---- Test Predictions ----\n",
    "        print('\\n[KNN] Computing test predictions...')\n",
    "        _hb('[KNN][HB]', phase='test_start', test_n=test_n)\n",
    "\n",
    "        if USE_CUML:\n",
    "            knn_final = cuNearestNeighbors(n_neighbors=KNN_K, metric='euclidean')\n",
    "        else:\n",
    "            knn_final = NearestNeighbors(n_neighbors=KNN_K, metric='cosine', n_jobs=-1)\n",
    "\n",
    "        t_fit_final = time.time()\n",
    "        knn_final.fit(X_knn)\n",
    "        print(f'[KNN] Fit (full train) complete in {_fmt_s(time.time()-t_fit_final)}')\n",
    "\n",
    "        t_kn_te = time.time()\n",
    "        dists_te, neigh_te = knn_final.kneighbors(X_knn_test, return_distance=True)\n",
    "        print(f'[KNN] kneighbors(test) complete in {_fmt_s(time.time()-t_kn_te)}')\n",
    "\n",
    "        sims_te = np.clip((1.0 - dists_te).astype(np.float32), 0.0, 1.0)\n",
    "\n",
    "        te_agg_t0 = time.time()\n",
    "        n_batches_te = int((test_n + KNN_BATCH - 1) // KNN_BATCH)\n",
    "        for b, i in enumerate(range(0, test_n, KNN_BATCH), start=1):\n",
    "            j = min(i + KNN_BATCH, test_n)\n",
    "\n",
    "            neigh_b = neigh_te[i:j]\n",
    "            sims_b = sims_te[i:j]\n",
    "\n",
    "            Y_nei = Y_knn[neigh_b]\n",
    "            weighted_votes = (sims_b[:, :, np.newaxis] * Y_nei * w_ia_broadcast).sum(axis=1)\n",
    "            denom = np.maximum(sims_b.sum(axis=1, keepdims=True), 1e-8)\n",
    "            scores = (weighted_votes / denom).astype(np.float32)\n",
    "\n",
    "            test_pred_knn[i:j] = scores\n",
    "\n",
    "            # Heartbeat (test aggregation)\n",
    "            if b == 1 or b == n_batches_te or (b % 10 == 0):\n",
    "                elapsed = time.time() - te_agg_t0\n",
    "                avg = elapsed / b\n",
    "                eta = avg * (n_batches_te - b)\n",
    "                _hb(\n",
    "                    '[KNN][HB]',\n",
    "                    phase='test_agg',\n",
    "                    batch=f'{b}/{n_batches_te}',\n",
    "                    pct=f'{(100.0*b/n_batches_te):.0f}%',\n",
    "                    elapsed=_fmt_s(elapsed),\n",
    "                    eta=_fmt_s(eta),\n",
    "                )\n",
    "\n",
    "        # Quality gate - Check test predictions\n",
    "        if not np.isfinite(test_pred_knn).all():\n",
    "            nan_count = int((~np.isfinite(test_pred_knn)).sum())\n",
    "            print(f'[WARN] KNN test contains {nan_count} NaN/Inf values; clipping to valid range [0, 1]')\n",
    "            test_pred_knn = np.clip(np.nan_to_num(test_pred_knn, nan=0.0, posinf=1.0, neginf=0.0), 0.0, 1.0)\n",
    "\n",
    "        # Save predictions\n",
    "        t_save = time.time()\n",
    "        np.save(knn_oof_path, oof_pred_knn)\n",
    "        np.save(knn_test_path, test_pred_knn)\n",
    "        np.save(knn_oof_compat, oof_pred_knn)\n",
    "        np.save(knn_test_compat, test_pred_knn)\n",
    "        print(f'[KNN] Saved predictions in {_fmt_s(time.time()-t_save)}')\n",
    "        print('Saved:', knn_oof_path)\n",
    "        print('Saved:', knn_test_path)\n",
    "        print('Saved (compat):', knn_oof_compat)\n",
    "        print('Saved (compat):', knn_test_compat)\n",
    "\n",
    "        print(f'\\n[KNN] ═══ COMPLETE ═══ total_time={_fmt_s(time.time()-overall_t0)}')\n",
    "\n",
    "    # Checkpoint push (always)\n",
    "    STORE.maybe_push(\n",
    "        stage='stage_07d_level1_knn',\n",
    "        required_paths=[\n",
    "            str((WORK_ROOT / 'features' / 'top_terms_13500.json').as_posix()),\n",
    "            str(knn_oof_path.as_posix()),\n",
    "            str(knn_test_path.as_posix()),\n",
    "        ],\n",
    "        note='Level-1 KNN (GPU-accelerated, IA-weighted) predictions using ESM2-3B embeddings (OOF + test).',\n",
    "    )\n",
    "\n",
    "    # Diagnostics: similarity distribution + IA-F1 vs threshold\n",
    "    try:\n",
    "        import os\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "        if oof_max_sim is not None:\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            plt.hist(oof_max_sim, bins=50, alpha=0.7)\n",
    "            plt.title('KNN OOF diagnostic: max cosine similarity to neighbours (per protein)')\n",
    "            plt.xlabel('max similarity')\n",
    "            plt.ylabel('count')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.show()\n",
    "\n",
    "        if 'ia_weighted_f1' in globals():\n",
    "            DIAG_N = int(os.getenv('CAFA_DIAG_N', '20000'))\n",
    "\n",
    "            def _sub(y_true: np.ndarray, y_score: np.ndarray):\n",
    "                n = int(y_true.shape[0])\n",
    "                m = min(n, int(DIAG_N))\n",
    "                if m <= 0:\n",
    "                    return y_true[:0], y_score[:0]\n",
    "                idx = np.linspace(0, n - 1, num=m, dtype=np.int64)\n",
    "                return y_true[idx], y_score[idx]\n",
    "\n",
    "            y_t, y_s = _sub(Y, oof_pred_knn)\n",
    "\n",
    "            thrs = np.linspace(0.05, 0.60, 23)\n",
    "            curves = {k: [] for k in ['ALL', 'MF', 'BP', 'CC']}\n",
    "\n",
    "            for thr in thrs:\n",
    "                s = ia_weighted_f1(y_t, y_s, thr=float(thr))\n",
    "                for k in curves.keys():\n",
    "                    curves[k].append(s[k])\n",
    "\n",
    "            plt.figure(figsize=(10, 3))\n",
    "            for k in ['ALL', 'MF', 'BP', 'CC']:\n",
    "                plt.plot(thrs, curves[k], label=k)\n",
    "            plt.title('KNN OOF: IA-weighted F1 vs threshold (sampled)')\n",
    "            plt.xlabel('threshold')\n",
    "            plt.ylabel('IA-F1')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print('KNN diagnostics skipped:', repr(e))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
