# CAFA-6 End-to-End Pipeline Sketch

## ðŸ—‚ï¸ Data Flow Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          RAW INPUT DATA (Train/)                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                             â”‚                             â”‚
        â–¼                             â–¼                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ train_        â”‚          â”‚ train_terms.tsv  â”‚          â”‚ go-basic.obo     â”‚
â”‚ sequences.    â”‚          â”‚                  â”‚          â”‚                  â”‚
â”‚ fasta         â”‚          â”‚ EntryID | term   â”‚          â”‚ [Term]           â”‚
â”‚               â”‚          â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚          â”‚ id: GO:0008150   â”‚
â”‚ >T100...001   â”‚          â”‚ T10..1  GO:0..1  â”‚          â”‚ name: bio proc   â”‚
â”‚ MKKLAVAA...   â”‚          â”‚ T10..1  GO:0..2  â”‚          â”‚ namespace: BP    â”‚
â”‚ >T100...002   â”‚          â”‚ T10..2  GO:0..3  â”‚          â”‚ is_a: GO:0..X    â”‚
â”‚ ATGGCCTA...   â”‚          â”‚ ...              â”‚          â”‚ ...              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                             â”‚                             â”‚
        â”‚                             â”‚                             â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚   PREPROCESSING LAYER       â”‚
                        â”‚                             â”‚
                        â”‚ â€¢ Parse FASTA sequences     â”‚
                        â”‚ â€¢ Map GO terms â†’ aspects    â”‚
                        â”‚   (Fâ†’MF, Pâ†’BP, Câ†’CC)        â”‚
                        â”‚ â€¢ Build GO graph structure  â”‚
                        â”‚ â€¢ Train/Val split (80/20)   â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚                                           â”‚
                â–¼                                           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  TRAINING DATA        â”‚                 â”‚  VALIDATION DATA      â”‚
    â”‚                       â”‚                 â”‚                       â”‚
    â”‚  â€¢ 113K proteins      â”‚                 â”‚  â€¢ 29K proteins       â”‚
    â”‚  â€¢ 490K annotations   â”‚                 â”‚  â€¢ 122K annotations   â”‚
    â”‚  â€¢ 26K unique GO termsâ”‚                 â”‚  â€¢ Unseen proteins    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ”„ Four Parallel Modelling Pipelines

### **Pipeline 1: Frequency Baseline** (`01_baseline_frequency.ipynb`)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        FREQUENCY BASELINE                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    TRAINING DATA (train_terms.tsv)
            â”‚
            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Count Term Frequencies       â”‚
    â”‚                               â”‚
    â”‚  GO:0008150 â†’ 2,319 times     â”‚
    â”‚  GO:0003824 â†’ 33,713 times    â”‚
    â”‚  GO:0005575 â†’ 13,283 times    â”‚
    â”‚  ...                          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Select Top 10,000 Terms      â”‚
    â”‚                               â”‚
    â”‚  â€¢ MF: 17.5% (1,750 terms)    â”‚
    â”‚  â€¢ BP: 57.5% (5,750 terms)    â”‚
    â”‚  â€¢ CC: 25.0% (2,500 terms)    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  PREDICTION STRATEGY          â”‚
    â”‚                               â”‚
    â”‚  For ANY protein:             â”‚
    â”‚  Predict same 10K terms       â”‚
    â”‚  with their frequencies       â”‚
    â”‚  as probability scores        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  OUTPUT                       â”‚
    â”‚                               â”‚
    â”‚  142K proteins Ã— 10K terms    â”‚
    â”‚  = 1.4 million predictions    â”‚
    â”‚  (chunked processing)         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
    Per-aspect F1 evaluation
    (expected: MF high, BP low, CC medium)
```

**Key Insight:** Ignores sequence content â€” purely statistical baseline

---

### **Pipeline 2: K-Nearest Neighbors** (`02_baseline_knn.ipynb`)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         KNN BASELINE                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    TRAINING DATA                    VALIDATION DATA
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Sequences    â”‚                â”‚ Query proteinâ”‚
    â”‚ + GO labels  â”‚                â”‚ MKKLAVAA...  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                               â”‚
            â–¼                               â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚     SEQUENCE SIMILARITY COMPUTATION           â”‚
    â”‚                                               â”‚
    â”‚     â€¢ BLAST alignment / k-mer overlap         â”‚
    â”‚     â€¢ Find K=5 most similar proteins          â”‚
    â”‚     â€¢ Compute similarity scores (0-1)         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚     AGGREGATE NEIGHBOR LABELS                 â”‚
    â”‚                                               â”‚
    â”‚     Neighbor 1 (sim=0.95): GO:001, GO:002    â”‚
    â”‚     Neighbor 2 (sim=0.89): GO:001, GO:003    â”‚
    â”‚     Neighbor 3 (sim=0.82): GO:002, GO:004    â”‚
    â”‚     Neighbor 4 (sim=0.78): GO:001            â”‚
    â”‚     Neighbor 5 (sim=0.71): GO:003, GO:005    â”‚
    â”‚                                               â”‚
    â”‚     Weighted vote:                            â”‚
    â”‚     GO:001 â†’ (0.95+0.89+0.78)/3 = 0.873      â”‚
    â”‚     GO:002 â†’ (0.95+0.82)/2 = 0.885           â”‚
    â”‚     GO:003 â†’ (0.89+0.71)/2 = 0.800           â”‚
    â”‚     ...                                       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚     OUTPUT: Sequence-aware predictions        â”‚
    â”‚                                               â”‚
    â”‚     â€¢ BP F1 expected to IMPROVE significantly â”‚
    â”‚     â€¢ Rare BP terms can be predicted via      â”‚
    â”‚       similar sequences                       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
    Per-aspect F1 evaluation
```

**Key Insight:** Leverages sequence similarity â€” should fix BP problem

---

### **Pipeline 3: ESM-2 Fine-Tuned** (`03_model_esm_finetuned.ipynb`)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ESM-2 FINE-TUNED MODEL                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    PROTEIN SEQUENCE                 PRE-TRAINED MODEL
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ MKKLAVAA...  â”‚                â”‚  ESM-2 8M params    â”‚
    â”‚              â”‚                â”‚  (Facebook/Meta AI) â”‚
    â”‚              â”‚                â”‚                     â”‚
    â”‚              â”‚                â”‚  Trained on 250M    â”‚
    â”‚              â”‚                â”‚  protein sequences  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                               â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   TOKEN EMBEDDING         â”‚
                â”‚                           â”‚
                â”‚   M â†’ [0.23, -0.15, ...]  â”‚
                â”‚   K â†’ [0.41, 0.09, ...]   â”‚
                â”‚   K â†’ [0.41, 0.09, ...]   â”‚
                â”‚   ...                     â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   TRANSFORMER LAYERS      â”‚
                â”‚                           â”‚
                â”‚   â€¢ Self-attention (6x)   â”‚
                â”‚   â€¢ Learn sequence contextâ”‚
                â”‚   â€¢ Output: 320-dim vectorâ”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   CLASSIFICATION HEAD     â”‚
                â”‚   (trainable)             â”‚
                â”‚                           â”‚
                â”‚   Linear: 320 â†’ 26,125    â”‚
                â”‚   (one output per GO term)â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   SIGMOID ACTIVATION      â”‚
                â”‚                           â”‚
                â”‚   GO:0001 â†’ 0.92 âœ“        â”‚
                â”‚   GO:0002 â†’ 0.05          â”‚
                â”‚   GO:0003 â†’ 0.78 âœ“        â”‚
                â”‚   ...                     â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  TRAINING PROCESS                     â”‚
        â”‚                                       â”‚
        â”‚  Loss: Binary Cross-Entropy           â”‚
        â”‚  Optimizer: AdamW                     â”‚
        â”‚  Learning Rate: 1e-4                  â”‚
        â”‚  Batch Size: 8                        â”‚
        â”‚  Epochs: 10                           â”‚
        â”‚                                       â”‚
        â”‚  Each epoch:                          â”‚
        â”‚  â€¢ Forward pass (predict)             â”‚
        â”‚  â€¢ Compute loss vs true labels        â”‚
        â”‚  â€¢ Backprop gradients                 â”‚
        â”‚  â€¢ Update classification head weights â”‚
        â”‚  â€¢ Validate on held-out set           â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
        Per-aspect F1 evaluation during training
```

**Key Insight:** Deep learning captures sequence patterns frequency/KNN miss

---

### **Pipeline 4: Label Propagation** (`04_label_propagation.ipynb`)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      LABEL PROPAGATION                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    BASE MODEL PREDICTIONS          GO ONTOLOGY GRAPH
    (from Pipeline 3)               (from go-basic.obo)
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ GO:0006355 â†’ 0.8 â”‚            â”‚                         â”‚
    â”‚ (specific term)  â”‚            â”‚   GO:0008150 (root)     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚      â”‚                  â”‚
            â”‚                       â”‚      â”œâ”€ GO:0065007      â”‚
            â”‚                       â”‚      â”‚    â”‚             â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€ GO:0006355   â”‚
                                    â”‚           (leaf)        â”‚
                                    â”‚                         â”‚
                                    â”‚  is_a relationships     â”‚
                                    â”‚  form directed graph    â”‚
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                            â”‚
                                            â–¼
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚  PROPAGATION ALGORITHM        â”‚
                            â”‚                               â”‚
                            â”‚  IF predict GO:0006355 (0.8)  â”‚
                            â”‚  THEN also predict:           â”‚
                            â”‚                               â”‚
                            â”‚  â€¢ GO:0065007 (parent) â†’ 0.8  â”‚
                            â”‚  â€¢ GO:0008150 (root)   â†’ 0.8  â”‚
                            â”‚                               â”‚
                            â”‚  Rule: ancestors inherit      â”‚
                            â”‚  max score of descendants     â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                            â”‚
                                            â–¼
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚  ENHANCED PREDICTIONS         â”‚
                            â”‚                               â”‚
                            â”‚  Original: 150 terms          â”‚
                            â”‚  After prop: 210 terms        â”‚
                            â”‚                               â”‚
                            â”‚  â€¢ Ensures biological validityâ”‚
                            â”‚  â€¢ Fixes "orphan" predictions â”‚
                            â”‚  â€¢ Expected: +0.02-0.04 F1    â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                            â”‚
                                            â–¼
                            Per-aspect F1 evaluation
```

**Key Insight:** Enforces GO hierarchy constraints â€” free performance boost

---

## ðŸ“Š Evaluation Framework (All Pipelines)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   PER-ASPECT CAFA METRIC                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    MODEL PREDICTIONS                VALIDATION LABELS
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ GO:0001 â†’ 0.92  â”‚              â”‚ GO:0001 âœ“       â”‚
    â”‚ GO:0002 â†’ 0.05  â”‚              â”‚ GO:0003 âœ“       â”‚
    â”‚ GO:0003 â†’ 0.78  â”‚              â”‚ GO:0005 âœ“       â”‚
    â”‚ GO:0004 â†’ 0.15  â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚ GO:0005 â†’ 0.88  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  STEP 1: Map terms to aspects               â”‚
    â”‚                                             â”‚
    â”‚  GO:0001 (F) â†’ MF                          â”‚
    â”‚  GO:0002 (P) â†’ BP                          â”‚
    â”‚  GO:0003 (F) â†’ MF                          â”‚
    â”‚  GO:0004 (C) â†’ CC                          â”‚
    â”‚  GO:0005 (P) â†’ BP                          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  STEP 2: Split by aspect                    â”‚
    â”‚                                             â”‚
    â”‚  MF predictions: [0.92, 0.78]              â”‚
    â”‚  BP predictions: [0.05, 0.88]              â”‚
    â”‚  CC predictions: [0.15]                    â”‚
    â”‚                                             â”‚
    â”‚  MF labels: [GO:0001, GO:0003]             â”‚
    â”‚  BP labels: [GO:0005]                      â”‚
    â”‚  CC labels: []                             â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  STEP 3: Apply threshold (e.g., 0.5)        â”‚
    â”‚                                             â”‚
    â”‚  MF: [0.92âœ“, 0.78âœ“] â†’ predict both         â”‚
    â”‚  BP: [0.05âœ—, 0.88âœ“] â†’ predict GO:0005 only â”‚
    â”‚  CC: [0.15âœ—] â†’ predict nothing             â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  STEP 4: Compute IA-weighted F1 per aspect  â”‚
    â”‚                                             â”‚
    â”‚  Precision_MF = TP / (TP + FP)             â”‚
    â”‚  Recall_MF = TP / (TP + FN)                â”‚
    â”‚  F1_MF = 2 Ã— (P Ã— R) / (P + R)             â”‚
    â”‚  (weighted by IA scores)                    â”‚
    â”‚                                             â”‚
    â”‚  Repeat for BP, CC                         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  STEP 5: Average across aspects             â”‚
    â”‚                                             â”‚
    â”‚  F1_overall = (F1_MF + F1_BP + F1_CC) / 3  â”‚
    â”‚                                             â”‚
    â”‚  Example:                                   â”‚
    â”‚  F1_MF = 0.42                              â”‚
    â”‚  F1_BP = 0.15                              â”‚
    â”‚  F1_CC = 0.38                              â”‚
    â”‚  Overall = 0.317                           â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸŽ¯ Final Submission Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        TEST SET PREDICTION                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    TEST DATA (Test/)
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ testsuperset.fasta   â”‚
    â”‚                      â”‚
    â”‚ >T200...001          â”‚
    â”‚ MGGKLAAAA...         â”‚
    â”‚ >T200...002          â”‚
    â”‚ ATAGGCCTA...         â”‚
    â”‚ ...                  â”‚
    â”‚ (142,000 proteins)   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  BEST MODEL (Pipeline 3 or 4)        â”‚
    â”‚                                      â”‚
    â”‚  â€¢ Trained ESM-2 + propagation       â”‚
    â”‚  â€¢ Optimal threshold per aspect      â”‚
    â”‚  â€¢ Generate predictions              â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  FORMAT SUBMISSION                   â”‚
    â”‚                                      â”‚
    â”‚  EntryID    term      score          â”‚
    â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€        â”‚
    â”‚  T200..1    GO:0001   0.92           â”‚
    â”‚  T200..1    GO:0003   0.78           â”‚
    â”‚  T200..2    GO:0005   0.88           â”‚
    â”‚  ...                                 â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
    sample_submission.tsv
    (upload to Kaggle)
```

---

## ðŸ”— Data Dependencies Map - **THE LAYMAN'S GUIDE**

### **Think of it like a Restaurant Database System:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  YOUR MISSION: Build a system that looks at a customer (protein)         â”‚
â”‚  and predicts what dishes they'll order (GO functions)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### **1. train_sequences.fasta** = **CUSTOMER LIST** ðŸ‘¥

```
What it looks like:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
>T100900000001
MKKLAVAATVMSLLIACSASSAAKENVTNFKTEQSTPQAAA
>T100900000002
ATGGCCTATATCGGTGCCAAGGACGGCGACTACAAAGACGATGAC

What it ACTUALLY means:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Customer #1 (ID: T100900000001)
  Description: "Tall, brown hair, wears glasses, has tattoo"
  (In reality: protein's amino acid sequence = its physical structure)

Customer #2 (ID: T100900000002)  
  Description: "Short, blonde, athletic build"
  (In reality: another protein's unique sequence)

ðŸ§‘â€ðŸ³ Analogy: 
   Each line of random letters (MKKLAVAA...) is like describing a person's appearance.
   Just like you can recognize someone by "tall + glasses + tattoo", 
   the model recognizes protein function from "MKKL..." sequence pattern.

ðŸ”¬ Reality:
   - 113,000 proteins (customers) in training set
   - Each sequence is 100-5000 letters long
   - Letters = amino acids (building blocks of proteins)
   - The SEQUENCE determines what the protein DOES
```

---

### **2. train_terms.tsv** = **ORDER HISTORY** ðŸ•ðŸ”ðŸœ

```
What it looks like:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
T100900000001   GO:0008150
T100900000001   GO:0003824
T100900000001   GO:0005737
T100900000002   GO:0016020
T100900000002   GO:0005575

What it ACTUALLY means:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Customer #1 ordered:
  â€¢ GO:0008150 (biological_process) = "Meal category: Main course"
  â€¢ GO:0003824 (catalytic_activity) = "Specific dish: Pepperoni Pizza"
  â€¢ GO:0005737 (cytoplasm) = "Location: Dine-in"

Customer #2 ordered:
  â€¢ GO:0016020 (membrane) = "Location: Takeaway"
  â€¢ GO:0005575 (cellular_component) = "Meal category: Dessert"

ðŸ§‘â€ðŸ³ Analogy:
   Your job: Look at Customer #1's appearance (sequence MKKLAVAA...)
   and predict they'll order pizza + main course + dine-in.
   
   This file tells you "Customer #1 DID order these things in the past"
   â†’ Training data to learn patterns

ðŸ”¬ Reality:
   - 490,000 rows (order records)
   - Each protein has 3-50 GO terms (functions)
   - GO terms = biological jobs like "cuts DNA", "makes energy", "lives in nucleus"
   - THIS IS WHAT YOU'RE TRYING TO PREDICT for new proteins!
```

---

### **3. go-basic.obo** = **MENU HIERARCHY** ðŸ“‹

```
What it looks like:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
[Term]
id: GO:0006355
name: regulation of transcription, DNA-templated
namespace: biological_process
is_a: GO:0065007 ! regulation of biological process
is_a: GO:0051252 ! regulation of RNA metabolic process

What it ACTUALLY means:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Dish: "Margherita Pizza" (GO:0006355)
  â†³ is_a: "Pizza" (GO:0065007)
     â†³ is_a: "Main Course" (GO:0051252)
        â†³ is_a: "Food" (root category)

ðŸ§‘â€ðŸ³ Analogy:
   Restaurant menu has hierarchy:
   
   FOOD (root)
   â”œâ”€â”€ Main Course
   â”‚   â”œâ”€â”€ Pizza
   â”‚   â”‚   â”œâ”€â”€ Margherita Pizza â† specific
   â”‚   â”‚   â””â”€â”€ Pepperoni Pizza  â† specific
   â”‚   â””â”€â”€ Pasta
   â””â”€â”€ Dessert
       â””â”€â”€ Ice Cream

   Rules:
   â€¢ If someone orders "Margherita Pizza" â†’ they ALSO ordered "Pizza", "Main Course", "Food"
   â€¢ Can't order "Margherita" without it being a "Pizza"
   â€¢ Parent categories are IMPLIED by child orders

ðŸ”¬ Reality:
   - 47,000 GO terms in ontology
   - 3 main branches: Molecular Function (MF), Biological Process (BP), Cellular Component (CC)
   - "is_a" relationships form a tree structure
   - Used in Pipeline 4 (label propagation) to add missing parent terms
   
Example:
  Model predicts: GO:0006355 (transcription)
  But forgets: GO:0065007 (biological regulation) â† its parent
  
  go-basic.obo says "transcription is_a biological regulation"
  â†’ Propagation adds the parent automatically
```

---

### **4. IA.tsv** = **DISH RARITY SCORES** â­ðŸ’Ž

```
What it looks like:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
GO:0008150    1.000
GO:0003824    2.145
GO:0006355    7.851
GO:0043167    9.825

What it ACTUALLY means:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
GO:0008150 (biological_process root) â†’ IA = 1.0
  = "Food" category â†’ EVERYONE orders this â†’ boring, no credit

GO:0003824 (catalytic activity) â†’ IA = 2.1
  = "Pizza" â†’ 70% of customers order this â†’ common, little credit

GO:0006355 (transcription) â†’ IA = 7.9
  = "Truffle Risotto" â†’ Only 5% order this â†’ rare, HIGH CREDIT!

GO:0043167 (ion binding) â†’ IA = 9.8
  = "Molecular Gastronomy Foam" â†’ Only 0.5% order â†’ VERY RARE, HUGE CREDIT!

ðŸ§‘â€ðŸ³ Analogy:
   You're a waiter trying to predict orders.
   
   Scenario A: You predict "Customer will order food"
      â†’ Correct! But everyone orders food. Score: 1/10 (useless prediction)
   
   Scenario B: You predict "Customer will order Truffle Risotto"
      â†’ Correct! Very few order this. Score: 8/10 (impressive!)
   
   Scenario C: You predict "Customer will order Molecular Foam"
      â†’ Correct! Almost nobody orders this. Score: 10/10 (master waiter!)

ðŸ”¬ Reality:
   - IA = -logâ‚‚(frequency)
   - Common terms (appear 50% of time) â†’ IA â‰ˆ 1.0
   - Rare terms (appear 0.1% of time) â†’ IA â‰ˆ 10.0
   - Used to WEIGHT the F1 score
   - Predicting rare = more valuable than predicting common
   - Prevents model from cheating by only predicting "biological_process" for everyone
```

---

### **HOW THEY ALL CONNECT - THE COMPLETE STORY:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RESTAURANT PREDICTION SYSTEM                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STEP 1: Load Customer Database
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
train_sequences.fasta:
  Customer T100...001: "Tall, glasses, tattoo" (protein sequence MKKLAVAA...)
  Customer T100...002: "Short, blonde, athletic" (protein sequence ATGGCC...)
  
  â†’ These are your TRAINING CUSTOMERS


STEP 2: Load Order History
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
train_terms.tsv:
  Customer T100...001 previously ordered:
    â€¢ GO:0003824 (Pepperoni Pizza)
    â€¢ GO:0005737 (Dine-in)
    
  Customer T100...002 previously ordered:
    â€¢ GO:0016020 (Takeaway)
    â€¢ GO:0005575 (Dessert)
    
  â†’ This is your TRAINING DATA (what they actually ordered)


STEP 3: Study Menu Structure
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
go-basic.obo:
  Pizza is_a Main Course
  Main Course is_a Food
  Pepperoni Pizza is_a Pizza
  
  â†’ This is the MENU HIERARCHY (how dishes relate)


STEP 4: Load Rarity Scores
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
IA.tsv:
  "Food" â†’ IA = 1.0 (everyone orders, no credit)
  "Pizza" â†’ IA = 2.1 (common, little credit)
  "Truffle Risotto" â†’ IA = 7.9 (rare, big credit)
  "Molecular Foam" â†’ IA = 9.8 (very rare, huge credit)
  
  â†’ This is the SCORING SYSTEM (how much credit for correct predictions)


STEP 5: TRAIN THE MODEL
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
"Look at customer appearance (sequence) + past orders (train_terms)
 Learn patterns like:
 â€¢ Tall customers with tattoos â†’ usually order Pizza
 â€¢ Athletic customers â†’ usually order Salad
 â€¢ Customers wearing suits â†’ usually order Wine"


STEP 6: PREDICT NEW CUSTOMER
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
New customer walks in: "Tall, glasses, tattoo" (test protein sequence)

Model thinks:
  "Hmm, this looks like Customer T100...001 from training"
  "T100...001 ordered Pizza + Dine-in"
  "I predict this new customer will order Pizza + Dine-in"
  
Model outputs:
  GO:0003824 (Pizza) â†’ confidence 0.92
  GO:0005737 (Dine-in) â†’ confidence 0.78


STEP 7: PROPAGATE HIERARCHY
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
go-basic.obo says:
  "Pizza is_a Main Course is_a Food"
  
Propagation adds:
  GO:0003824 (Pizza) â†’ 0.92  â† model predicted
  GO:xxxxxxx (Main Course) â†’ 0.92  â† added by propagation
  GO:0008150 (Food) â†’ 0.92  â† added by propagation


STEP 8: SCORE WITH IA WEIGHTS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
True labels: Customer ordered Pizza + Truffle Risotto

Model predicted: Pizza + Dine-in

Standard F1:
  TP = 1 (Pizza correct)
  FP = 1 (Dine-in wrong)
  FN = 1 (missed Truffle Risotto)
  F1 = 0.50

IA-weighted F1:
  TP_weight = IA(Pizza) = 2.1
  FP_weight = IA(Dine-in) = 3.0
  FN_weight = IA(Truffle Risotto) = 7.9
  
  Precision = 2.1 / (2.1 + 3.0) = 0.41
  Recall = 2.1 / (2.1 + 7.9) = 0.21
  F1 = 0.28  â† LOWER because missed rare dish (Truffle Risotto)
  
  â†’ Penalty for missing rare items!
```

---

### **WHY THE DATA LOOKS RANDOM:**

```
â“ "Why does train_sequences.fasta look like gibberish?"
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MKKLAVAATVMSLLIACSASSAAKENVTNFKTEQSTPQAAA...

Answer: It's NOT random! It's a LANGUAGE.
  
  M = Methionine (amino acid)
  K = Lysine (amino acid)
  K = Lysine (amino acid)
  L = Leucine (amino acid)
  A = Alanine (amino acid)
  V = Valine (amino acid)
  ...
  
  Just like "HELLO" means something in English,
  "MKKLAVAA" means something in Protein Language.
  
  The model learns:
    "MKKL" at start â†’ signal peptide â†’ protein goes to membrane
    "KDEL" at end â†’ ER retention signal â†’ protein stays in ER
    "CxxC" pattern â†’ zinc finger domain â†’ DNA binding protein
  
  Same way you recognize words â†’ model recognizes sequence motifs!


â“ "Why GO:0008150 instead of normal names?"
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Answer: GO IDs are UNIQUE and STANDARDIZED.
  
  "biological process" could mean different things
  GO:0008150 ALWAYS means same thing worldwide
  
  Like:
    ISBN numbers for books (GO IDs)
    vs
    "Harry Potter" (common name - which book? which edition?)


â“ "How do proteins (sequences) link to GO terms?"
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Answer: train_terms.tsv is the BRIDGE!

  train_sequences.fasta says: T100...001 = MKKLAVAA...
  train_terms.tsv says: T100...001 has GO:0003824
  
  So: MKKLAVAA... â†’ GO:0003824
  
  Model learns: "This sequence pattern â†’ catalytic activity"
  
  Like: Customer appearance â†’ past orders
        Tall + glasses â†’ ordered Pizza last time
        MKKLAVAA... â†’ has catalytic activity


â“ "Why do we need IA.tsv?"
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Answer: Without it, model would CHEAT!
  
  Model could predict "biological_process" (GO:0008150) for EVERYONE
  â†’ 100% correct (all proteins do SOME biological process)
  â†’ But totally useless (doesn't tell us WHAT process)
  
  IA.tsv says: GO:0008150 = 1.0 (no credit)
                GO:0006355 = 7.9 (big credit)
  
  Forces model to be SPECIFIC, not just correct but vague.
  
  Like: Waiter predicting "customer will order food" vs "customer will order Truffle Risotto"
        Both might be correct, but second is USEFUL!
```

---

### **QUICK REFERENCE - FILE CONNECTIONS:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        FILE RELATIONSHIPS                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Train/
â”œâ”€â”€ train_sequences.fasta â”€â”€â”¬â”€â”€â†’ WHO (protein identities)
â”‚   "Customer appearance"   â”‚    
â”‚                           â”‚
â”œâ”€â”€ train_terms.tsv â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â†’ WHAT (protein functions)
â”‚   "Order history"         â”‚    Links WHO to WHAT
â”‚                           â”‚    
â”œâ”€â”€ go-basic.obo â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â†’ HOW (functions relate)
â”‚   "Menu hierarchy"        â”‚    Links WHAT to WHAT
â”‚                           â”‚    
â””â”€â”€ train_taxonomy.tsv      â””â”€â”€â†’ WHERE (species info - optional)
    "Customer demographics"      

Competition Files/
â”œâ”€â”€ IA.tsv â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ VALUE (scoring system)
â”‚   "Dish rarity scores"         How VALUABLE each WHAT is
â”‚                           
â””â”€â”€ sample_submission.tsv â”€â”€â”€â”€â”€â”€â†’ FORMAT (output template)
    "Order receipt format"
```

---

## ðŸ“ˆ Expected Performance Trajectory

```
Model Pipeline          MF F1    BP F1    CC F1    Overall F1   Notes
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Frequency            0.28     0.00     0.22     0.167        Baseline
   (ignores sequence)                                            BP fails

2. KNN                  0.35     0.18     0.30     0.277        +0.11
   (sequence-aware)                                              BP fixed!

3. ESM-2 Fine-tuned     0.42     0.25     0.38     0.350        +0.07
   (deep learning)                                               SOTA

4. + Label Propagation  0.44     0.27     0.40     0.370        +0.02
   (enforce hierarchy)                                           Free boost
```

---

## ðŸ§  Key Conceptual Links

### Why BP Fails in Frequency Baseline:
```
BP terms: 16,858 unique â†’ spread thin â†’ individually RARE
MF terms: 6,616 unique  â†’ concentrated â†’ individually FREQUENT

Frequency baseline predicts SAME terms for ALL proteins
â†’ Only picks most frequent terms
â†’ Most frequent terms are MF/CC (GO:0003824, GO:0005575)
â†’ BP terms like GO:0006355 (2,319 occurrences) get filtered out
â†’ BP F1 = 0.00
```

### Why KNN Fixes BP:
```
KNN looks at SIMILAR sequences
â†’ Similar proteins often have similar BP functions
â†’ Can predict rare BP terms via neighbors
â†’ Example: DNA-binding protein â†’ neighbors likely have DNA-related BP terms
â†’ BP F1 jumps from 0.00 â†’ 0.18
```

### Why ESM-2 Does Better:
```
Transformer learns patterns like:
â€¢ "KDEL motif at C-terminus" â†’ ER retention (GO:0006621)
â€¢ "Zinc finger domains" â†’ transcription regulation (GO:0006355)
â€¢ "Transmembrane helices" â†’ membrane localization (GO:0016020)

Can predict BP terms WITHOUT finding similar training examples
â†’ Generalizes to unseen sequence patterns
â†’ BP F1 jumps to 0.25
```

### Why Propagation Helps:
```
Model predicts: GO:0006355 (transcription, DNA-templated)
But forgets parent: GO:0065007 (biological regulation)

Propagation enforces:
IF child predicted â†’ THEN ancestors should be predicted too
â†’ Adds ~40-60 ancestor terms per protein
â†’ Fixes "incomplete" predictions
â†’ +0.02 F1 boost (free lunch!)
```

---

## ðŸŽ¯ Information Accretion (IA) Weights - Deep Dive

### What is IA?

**Information Accretion** measures how **specific** a GO term is in the ontology hierarchy.

```
IA Score Logic:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
High IA (9-10)  â†’ Very SPECIFIC term (leaf nodes, rare annotations)
Medium IA (4-8) â†’ Moderately specific term (mid-level)
Low IA (1-2)    â†’ Very GENERAL term (root nodes, common annotations)
```

### Why IA Matters in CAFA Evaluation

```
WITHOUT IA weighting:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Predicting GO:0008150 (biological_process - root)     â†’ Easy, uninformative
Predicting GO:0006355 (DNA-templated transcription)   â†’ Hard, informative

Both count equally in F1 â†’ Model just predicts easy root terms â†’ Useless!

WITH IA weighting:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Predicting GO:0008150 (IA=1.00)  â†’ Low reward
Predicting GO:0006355 (IA=7.85)  â†’ High reward (7.85Ã— more valuable!)

F1 calculation weights by IA â†’ Model incentivized to predict SPECIFIC terms
```

### IA Calculation Formula

```
IA(term) = -logâ‚‚(P(term))

Where:
P(term) = frequency of term in training annotations / total annotations

Example:
â”€â”€â”€â”€â”€â”€â”€â”€
GO:0003674 (molecular_function root):
  â€¢ Appears in 128K annotations out of 490K total
  â€¢ P = 128K/490K = 0.261
  â€¢ IA = -logâ‚‚(0.261) = 1.94  â† LOW (very common)

GO:0043167 (ion binding):
  â€¢ Appears in 450 annotations out of 490K total
  â€¢ P = 450/490K = 0.00092
  â€¢ IA = -logâ‚‚(0.00092) = 10.08  â† HIGH (very rare)
```

### How IA Integrates into Evaluation

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               PRECISION CALCULATION WITH IA                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Predictions:        True Labels:         IA Weights:
GO:0001 (0.9) âœ“     GO:0001 âœ“           GO:0001 â†’ 8.5
GO:0002 (0.8) âœ—     GO:0003 âœ“           GO:0002 â†’ 6.2
GO:0003 (0.7) âœ“                         GO:0003 â†’ 9.1

Standard Precision = 2/3 = 0.667

IA-weighted Precision:
  TP_weight = IA(GO:0001) + IA(GO:0003) = 8.5 + 9.1 = 17.6
  FP_weight = IA(GO:0002) = 6.2
  
  Precision_IA = 17.6 / (17.6 + 6.2) = 0.739

â†’ Correctly predicting rare terms (GO:0003, IA=9.1) increases precision more
â†’ False positives on rare terms hurt more than on common terms
```

### IA Distribution Across Aspects

```
Load IA.tsv and analyze:

                Min IA    Median IA    Max IA    Interpretation
              â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Molecular     1.00      4.23         9.85      MF has many common
Function                                       terms (enzymes)

Biological    1.00      5.67         10.12     BP has most specific
Process                                        terms (rare pathways)

Cellular      1.00      3.98         8.94      CC terms moderately
Component                                      specific (organelles)

â†’ BP has highest IA scores â†’ Predicting BP correctly is most valuable
â†’ Explains why BP F1 low in frequency baseline hurts overall score so much
```

### Practical Example in Code

```python
# Load IA weights
ia_weights = pd.read_csv('IA.tsv', sep='\t', header=None, names=['term', 'IA'])
ia_dict = dict(zip(ia_weights['term'], ia_weights['IA']))

# During evaluation
def compute_weighted_f1(y_true, y_pred, terms, ia_dict):
    """
    y_true: [0, 1, 0, 1, 0]  (ground truth labels)
    y_pred: [1, 1, 0, 0, 1]  (model predictions)
    terms: ['GO:0001', 'GO:0002', 'GO:0003', 'GO:0004', 'GO:0005']
    """
    
    tp_weight = sum(ia_dict[term] for term, true, pred in zip(terms, y_true, y_pred)
                    if true == 1 and pred == 1)  # Correct positives
    
    fp_weight = sum(ia_dict[term] for term, true, pred in zip(terms, y_true, y_pred)
                    if true == 0 and pred == 1)  # False positives
    
    fn_weight = sum(ia_dict[term] for term, true, pred in zip(terms, y_true, y_pred)
                    if true == 1 and pred == 0)  # Missed positives
    
    precision = tp_weight / (tp_weight + fp_weight) if (tp_weight + fp_weight) > 0 else 0
    recall = tp_weight / (tp_weight + fn_weight) if (tp_weight + fn_weight) > 0 else 0
    
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    
    return f1
```

### Strategic Implications

```
1. ROOT TERMS ARE WORTHLESS
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   GO:0008150, GO:0003674, GO:0005575 all have IA â‰ˆ 1.0
   â†’ Predicting these gives almost NO credit
   â†’ Always propagate to MORE SPECIFIC children

2. RARE BP TERMS ARE GOLD
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   BP terms like GO:0006355 (transcription) have IA > 7.0
   â†’ Getting these right massively boosts F1
   â†’ This is why frequency baseline fails â€” misses rare BP terms

3. THRESHOLD TUNING MATTERS
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â€¢ Too high â†’ Miss rare high-IA terms â†’ Low recall â†’ Low F1
   â€¢ Too low â†’ Predict common low-IA terms â†’ Low precision â†’ Low F1
   â€¢ Optimal threshold balances high-IA true positives vs false positives

4. ASPECT-SPECIFIC THRESHOLDS
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   MF: threshold 0.5 (common terms, need high confidence)
   BP: threshold 0.1 (rare terms, accept lower confidence)
   CC: threshold 0.3 (moderate)
   
   â†’ Can tune per-aspect for +0.02-0.05 F1 improvement
```

### Visualization of IA Impact

```
Scenario: Model predicts 10 terms, 5 are correct

Case 1: Predicts 5 COMMON terms correctly (IA avg = 2.0)
        TP_weight = 5 Ã— 2.0 = 10.0
        F1 â‰ˆ 0.25

Case 2: Predicts 5 RARE terms correctly (IA avg = 8.0)
        TP_weight = 5 Ã— 8.0 = 40.0
        F1 â‰ˆ 0.68

â†’ Same NUMBER of correct predictions, but 2.7Ã— BETTER F1 score!
â†’ CAFA rewards biological insight (rare terms) over naive prediction (common terms)
```

---

## ðŸ’¾ Intermediate File Outputs

```
Pipeline 1 (Frequency):
â””â”€â”€ predictions_temp.parquet â”€â†’ Chunked predictions (1.4M rows)

Pipeline 2 (KNN):
â””â”€â”€ similarity_matrix.npy â”€â”€â”€â”€â”€â†’ Pairwise sequence similarities

Pipeline 3 (ESM-2):
â”œâ”€â”€ model_checkpoint_best.pt â”€â”€â†’ Trained model weights
â””â”€â”€ training_history.json â”€â”€â”€â”€â”€â†’ Loss/F1 curves per epoch

Pipeline 4 (Propagation):
â”œâ”€â”€ go_graph.pkl â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Parsed GO ontology graph
â””â”€â”€ propagated_predictions.tsv â”€â†’ Enhanced predictions

Final:
â””â”€â”€ submission.tsv â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Kaggle upload file
```

---

## ðŸŽ“ Learning Progression

```
START: "I have protein sequences + GO labels. What now?"
   â†“
Step 1: Frequency baseline
   â†’ Learn: aspect distribution, term frequencies, evaluation metric
   â†’ Outcome: Understand why naive approach fails for BP
   â†“
Step 2: KNN baseline
   â†’ Learn: sequence similarity matters, k-mer matching, weighted voting
   â†’ Outcome: See BP performance improve dramatically
   â†“
Step 3: ESM-2 fine-tuning
   â†’ Learn: transfer learning, transformers, embeddings
   â†’ Outcome: Beat handcrafted features with deep learning
   â†“
Step 4: Label propagation
   â†’ Learn: GO hierarchy, graph algorithms, biological constraints
   â†’ Outcome: Enforce domain knowledge for free gains
   â†“
END: Competitive CAFA-6 submission with interpretable pipeline
```

